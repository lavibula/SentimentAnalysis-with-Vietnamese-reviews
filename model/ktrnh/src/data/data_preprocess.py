import re
import unicodedata
from pyvi import ViTokenizer

class TextPreprocessor:
    def __init__(self):
        self.acronyms = {
            '√¥ k√™i':'ok','okie':'ok','o k√™':'ok',
            'okey':'ok','√¥k√™':'ok','oki':'ok','oke': 'ok','okay':'ok','ok√™':'ok',
            'tks': 'c√°m ∆°n','thks': 'c√°m ∆°n','thanks': 'c√°m ∆°n','ths': 'c√°m ∆°n','thank': 'c√°m ∆°n',
            '‚≠ê':'star','*':'star','üåü':'star',
            'kg': 'kh√¥ng','not': 'kh√¥ng', 'kg': 'kh√¥ng','"k': 'kh√¥ng','kh':'kh√¥ng','k√¥':'kh√¥ng','hok':'kh√¥ng','kp': 'kh√¥ng ph·∫£i','k√¥': 'kh√¥ng','"ko': 'kh√¥ng', 'ko': 'kh√¥ng', 'k': 'kh√¥ng','khong': 'kh√¥ng', 'hok': 'kh√¥ng',
            'cute': 'd·ªÖ th∆∞∆°ng','vs': 'v·ªõi','wa':'qu√°','w√°': 'qu√°','j': 'g√¨','‚Äú':'',
            'sz': 'c·ª°','size': 'c·ª°', 'ƒëx': 'ƒë∆∞·ª£c','dk': 'ƒë∆∞·ª£c','dc': 'ƒë∆∞·ª£c','ƒëk': 'ƒë∆∞·ª£c',
            'ƒëc': 'ƒë∆∞·ª£c','authentic': 'chu·∫©n ch√≠nh h√£ng','aut': 'chu·∫©n ch√≠nh h√£ng', 'auth': 'chu·∫©n ch√≠nh h√£ng','store': 'c·ª≠a h√†ng',
            'shop': 'c·ª≠a h√†ng','sp': 's·∫£n ph·∫©m','gud': 't·ªët','god': 't·ªët','wel done':'t·ªët','good': 't·ªët','g√∫t': 't·ªët',
            's·∫•u': 'x·∫•u','gut': 't·ªët', 'tot': 't·ªët', 'nice': 't·ªët','perfect':'r·∫•t t·ªët','bt': 'b√¨nh th∆∞·ªùng',
            'time': 'th·ªùi gian','q√°': 'qu√°', 'ship': 'giao h√†ng', 'm': 'm√¨nh', 'mik': 'm√¨nh',
            '√™Ãâ':'·ªÉ','product':'s·∫£n ph·∫©m','quality':'ch·∫•t l∆∞·ª£ng','chat':'ch·∫•t','excelent':'ho√†n h·∫£o','bad':'t·ªá','fresh':'t∆∞∆°i','sad':'t·ªá',
            'date': 'h·∫°n s·ª≠ d·ª•ng','hsd': 'h·∫°n s·ª≠ d·ª•ng','quickly': 'nhanh','quick': 'nhanh','fast': 'nhanh','delivery': 'giao h√†ng','s√≠p': 'giao h√†ng',
            'beautiful': 'ƒë·∫πp tuy·ªát v·ªùi', 'tl': 'tr·∫£ l·ªùi', 'r': 'r·ªìi', 'shopE': 'c·ª≠a h√†ng','order': 'ƒë·∫∑t h√†ng',
            'ch·∫•t lg': 'ch·∫•t l∆∞·ª£ng','sd': 's·ª≠ d·ª•ng','dt': 'ƒëi·ªán tho·∫°i','nt': 'nh·∫Øn tin','tl': 'tr·∫£ l·ªùi','s√†i': 'x√†i','bjo':'bao gi·ªù',
            'thick': 'th√≠ch','thik': 'th√≠ch', 'sop': 'c·ª≠a h√†ng', 'shop': 'c·ª≠a h√†ng', 
            'fb':'facebook','face':'facebook','very': 'r·∫•t','qu·∫£ ng':'qu·∫£ng ',
            'dep': 'ƒë·∫πp','xau': 'x·∫•u','delicious': 'ngon', 'h√†g': 'h√†ng', 'q·ªßa': 'qu·∫£',
            'iu': 'y√™u','fake': 'gi·∫£ m·∫°o','trl':'tr·∫£ l·ªùi',
            'por': 't·ªá','poor': 't·ªá','ib':'nh·∫Øn tin','rep':'tr·∫£ l·ªùi','fback':'feedback','fedback':'feedback',
            'max': 'c·ª±c k·ª≥',
            'full':'ƒë·∫ßy ƒë·ªß', 'ful':'ƒë·∫ßy ƒë·ªß'
        }
        self.not_words = {"kh√¥ng", "kh√¥ng h·ªÅ", "ch·∫≥ng", "ch∆∞a", "kh√¥ng ph·∫£i", "ch·∫£", "m·∫•t",
                          "thi·∫øu", "ƒë·∫øch", "ƒë√©o", "k√©m", "n·ªè", "not",
                          "b·ªõt", "kh√¥ng bao gi·ªù", "ch∆∞a bao gi·ªù"}
        self.not_words = sorted(self.not_words, key=len, reverse=True)
        self.replacements = {
            'a':'√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠',
            'e':'√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá',
            'i':'√¨√≠·ªâƒ©·ªã',
            'o':'√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£',
            'u':'√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±',
            'y':'·ª≥√Ω·ª∑·ªπ·ªµ',
            'd':'ƒë',
            'A':'√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨',
            'E':'√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ',
            'I':'√å√ç·ªàƒ®·ªä',
            'O':'√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢',
            'U':'√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞',
            'Y':'·ª≤√ù·ª∂·ª∏·ª¥',
            'D':'ƒê'
        }

    def remove_HTML(self, text):
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)

    def convert_unicode(self, text):
        return unicodedata.normalize('NFC', text)

    def remove_elongated_chars(self, text):
        for char, replacements_str in self.replacements.items():
            pattern = rf"({char})\1+"
            text = re.sub(pattern, char, text)
        
        pattern = r"(\w)\1+"
        text = re.sub(pattern, r'\1', text)
        return text

    def handle_negation(self, text):
        pattern = r'\b(?:' + '|'.join(re.escape(word) for word in self.not_words) + r')\b'
        text = re.sub(pattern, 'kh√¥ng', text, flags=re.IGNORECASE)
        return text

    def normalize_acronyms(self, text):
        words = text.split()
        normalized_text = ' '.join([self.acronyms.get(word.lower(), word) for word in words])
        return normalized_text

    def word_segmentation(self, text):
        return ViTokenizer.tokenize(text)

    def remove_unnecessary_characters(self, text):
        text = re.sub(r'\s+', ' ', text)  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        text = re.sub(r'[^\w\s]', '', text)  # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
        return text.strip()

    def preprocess(self, text):
        if not isinstance(text, str):
            raise ValueError("Input text must be a string.")
        text = self.remove_HTML(text)
        text = self.normalize_acronyms(text)
        text = self.convert_unicode(text)
        text = self.remove_elongated_chars(text)
        text = self.handle_negation(text)
        text = self.word_segmentation(text)
        text = self.remove_unnecessary_characters(text)
        return text
