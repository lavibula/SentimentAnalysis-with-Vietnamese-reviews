{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8478679,"sourceType":"datasetVersion","datasetId":5056692},{"sourceId":8498142,"sourceType":"datasetVersion","datasetId":5071077}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Ignore all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.308062Z","iopub.execute_input":"2024-05-26T10:53:12.309139Z","iopub.status.idle":"2024-05-26T10:53:12.313956Z","shell.execute_reply.started":"2024-05-26T10:53:12.309091Z","shell.execute_reply":"2024-05-26T10:53:12.312955Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Define the file paths\ntopics_file ='/kaggle/input/uit-vsfc/train/topics.txt'\nsentiments_file ='/kaggle/input/uit-vsfc/train/sentiments.txt'\nsents_file ='/kaggle/input/uit-vsfc/train/sents.txt'\n\n# Read the files\ntopics = pd.read_csv(topics_file, header=None, names=['Topic'])\nsentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n\n# Read the sentences file line by line\nwith open(sents_file,'r', encoding='utf-8') as file:\n    sentences = file.readlines()\n\n# Remove newline characters and punctuation from sentences, and split by whitespace\nsentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n\n# Create a DataFrame for sentences\nsentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n\n# Combine the DataFrames along the columns\ncombined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n\n# Display the first few rows of the combined DataFrame\nprint(combined_df.head())\n\n# Replace values in'Topic'column\ntopic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\ncombined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n\n# Replace values in'Sentiment'column\nsentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\ncombined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n\n# Display the modified DataFrame\nprint(combined_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.353100Z","iopub.execute_input":"2024-05-26T10:53:12.353415Z","iopub.status.idle":"2024-05-26T10:53:12.483453Z","shell.execute_reply.started":"2024-05-26T10:53:12.353389Z","shell.execute_reply":"2024-05-26T10:53:12.482435Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"   Topic  Sentiment                                           Sentence\n0      1          2                           slide giáo trình đầy đủ \n1      0          2       nhiệt tình giảng dạy  gần gũi với sinh viên \n2      1          0                đi học đầy đủ full điểm chuyên cần \n3      0          0  chưa áp dụng công nghệ thông tin và các thiết ...\n4      0          2  thầy giảng bài hay  có nhiều bài tập ví dụ nga...\n              Topic Sentiment  \\\n0  TRAINING_PROGRAM  POSITIVE   \n1          LECTURER  POSITIVE   \n2  TRAINING_PROGRAM  NEGATIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  POSITIVE   \n\n                                            Sentence  \n0                           slide giáo trình đầy đủ   \n1       nhiệt tình giảng dạy  gần gũi với sinh viên   \n2                đi học đầy đủ full điểm chuyên cần   \n3  chưa áp dụng công nghệ thông tin và các thiết ...  \n4  thầy giảng bài hay  có nhiều bài tập ví dụ nga...  \n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.485375Z","iopub.execute_input":"2024-05-26T10:53:12.486212Z","iopub.status.idle":"2024-05-26T10:53:12.503773Z","shell.execute_reply.started":"2024-05-26T10:53:12.486176Z","shell.execute_reply":"2024-05-26T10:53:12.502858Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                  Topic Sentiment  \\\n0      TRAINING_PROGRAM  POSITIVE   \n1              LECTURER  POSITIVE   \n2      TRAINING_PROGRAM  NEGATIVE   \n3              LECTURER  NEGATIVE   \n4              LECTURER  POSITIVE   \n...                 ...       ...   \n11421  TRAINING_PROGRAM  NEGATIVE   \n11422          LECTURER  POSITIVE   \n11423          LECTURER  NEGATIVE   \n11424          LECTURER  POSITIVE   \n11425          LECTURER  POSITIVE   \n\n                                                Sentence  \n0                               slide giáo trình đầy đủ   \n1           nhiệt tình giảng dạy  gần gũi với sinh viên   \n2                    đi học đầy đủ full điểm chuyên cần   \n3      chưa áp dụng công nghệ thông tin và các thiết ...  \n4      thầy giảng bài hay  có nhiều bài tập ví dụ nga...  \n...                                                  ...  \n11421  chỉ vì môn game mà em học hai lần mà không qua...  \n11422                                em cảm ơn cô nhiều   \n11423                            giao bài tập quá nhiều   \n11424                 giáo viên dạy dễ hiểu  nhiệt tình   \n11425  gói gọn doubledot hay  tận tình  phù hợp với m...  \n\n[11426 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>POSITIVE</td>\n      <td>slide giáo trình đầy đủ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>nhiệt tình giảng dạy  gần gũi với sinh viên</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>đi học đầy đủ full điểm chuyên cần</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>chưa áp dụng công nghệ thông tin và các thiết ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>thầy giảng bài hay  có nhiều bài tập ví dụ nga...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11421</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>chỉ vì môn game mà em học hai lần mà không qua...</td>\n    </tr>\n    <tr>\n      <th>11422</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>em cảm ơn cô nhiều</td>\n    </tr>\n    <tr>\n      <th>11423</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giao bài tập quá nhiều</td>\n    </tr>\n    <tr>\n      <th>11424</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giáo viên dạy dễ hiểu  nhiệt tình</td>\n    </tr>\n    <tr>\n      <th>11425</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gói gọn doubledot hay  tận tình  phù hợp với m...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11426 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.to_csv('train.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.526280Z","iopub.execute_input":"2024-05-26T10:53:12.526585Z","iopub.status.idle":"2024-05-26T10:53:12.594208Z","shell.execute_reply.started":"2024-05-26T10:53:12.526561Z","shell.execute_reply":"2024-05-26T10:53:12.593361Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.600006Z","iopub.execute_input":"2024-05-26T10:53:12.600350Z","iopub.status.idle":"2024-05-26T10:53:12.611420Z","shell.execute_reply.started":"2024-05-26T10:53:12.600322Z","shell.execute_reply":"2024-05-26T10:53:12.610346Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"              Topic Sentiment  \\\n0  TRAINING_PROGRAM  POSITIVE   \n1          LECTURER  POSITIVE   \n2  TRAINING_PROGRAM  NEGATIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  POSITIVE   \n\n                                            Sentence  \n0                           slide giáo trình đầy đủ   \n1       nhiệt tình giảng dạy  gần gũi với sinh viên   \n2                đi học đầy đủ full điểm chuyên cần   \n3  chưa áp dụng công nghệ thông tin và các thiết ...  \n4  thầy giảng bài hay  có nhiều bài tập ví dụ nga...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>POSITIVE</td>\n      <td>slide giáo trình đầy đủ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>nhiệt tình giảng dạy  gần gũi với sinh viên</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>đi học đầy đủ full điểm chuyên cần</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>chưa áp dụng công nghệ thông tin và các thiết ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>thầy giảng bài hay  có nhiều bài tập ví dụ nga...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Valid","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Define the file paths\ntopics_file ='/kaggle/input/uit-vsfc/dev/topics.txt'\nsentiments_file ='/kaggle/input/uit-vsfc/dev/sentiments.txt'\nsents_file ='/kaggle/input/uit-vsfc/dev/sents.txt'\n\n# Read the files\ntopics = pd.read_csv(topics_file, header=None, names=['Topic'])\nsentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n\n# Read the sentences file line by line\nwith open(sents_file,'r', encoding='utf-8') as file:\n    sentences = file.readlines()\n\n# Remove newline characters and punctuation from sentences, and split by whitespace\nsentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n\n# Create a DataFrame for sentences\nsentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n\n# Combine the DataFrames along the columns\ncombined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n\n# Display the first few rows of the combined DataFrame\nprint(combined_df.head())\n\n# Replace values in'Topic'column\ntopic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\ncombined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n\n# Replace values in'Sentiment'column\nsentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\ncombined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n\n# Display the modified DataFrame\nprint(combined_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.671764Z","iopub.execute_input":"2024-05-26T10:53:12.672155Z","iopub.status.idle":"2024-05-26T10:53:12.726928Z","shell.execute_reply.started":"2024-05-26T10:53:12.672125Z","shell.execute_reply":"2024-05-26T10:53:12.725752Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"   Topic  Sentiment                                           Sentence\n0      1          0                            giáo trình chưa cụ thể \n1      0          0                                    giảng buồn ngủ \n2      0          2                       giáo viên vui tính  tận tâm \n3      0          0  giảng viên nên giao bài tập nhiều hơn  chia nh...\n4      0          0  giảng viên cần giảng bài chi tiết hơn  đi sâu ...\n              Topic Sentiment  \\\n0  TRAINING_PROGRAM  NEGATIVE   \n1          LECTURER  NEGATIVE   \n2          LECTURER  POSITIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  NEGATIVE   \n\n                                            Sentence  \n0                            giáo trình chưa cụ thể   \n1                                    giảng buồn ngủ   \n2                       giáo viên vui tính  tận tâm   \n3  giảng viên nên giao bài tập nhiều hơn  chia nh...  \n4  giảng viên cần giảng bài chi tiết hơn  đi sâu ...  \n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.731851Z","iopub.execute_input":"2024-05-26T10:53:12.732233Z","iopub.status.idle":"2024-05-26T10:53:12.745549Z","shell.execute_reply.started":"2024-05-26T10:53:12.732203Z","shell.execute_reply":"2024-05-26T10:53:12.744535Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 Topic Sentiment  \\\n0     TRAINING_PROGRAM  NEGATIVE   \n1             LECTURER  NEGATIVE   \n2             LECTURER  POSITIVE   \n3             LECTURER  NEGATIVE   \n4             LECTURER  NEGATIVE   \n...                ...       ...   \n1578          LECTURER  NEGATIVE   \n1579          LECTURER  POSITIVE   \n1580          LECTURER  NEGATIVE   \n1581  TRAINING_PROGRAM  NEGATIVE   \n1582          LECTURER  NEGATIVE   \n\n                                               Sentence  \n0                               giáo trình chưa cụ thể   \n1                                       giảng buồn ngủ   \n2                          giáo viên vui tính  tận tâm   \n3     giảng viên nên giao bài tập nhiều hơn  chia nh...  \n4     giảng viên cần giảng bài chi tiết hơn  đi sâu ...  \n...                                                 ...  \n1578                               hướng dẫn lab mơ hồ   \n1579  thầy cho chúng em những bài tập mang tính thực...  \n1580  thầy không dạy nhiều chủ yếu cho sinh viên tự ...  \n1581  em muốn đổi tên môn học vì tên môn là lập trìn...  \n1582  thầy vừa dạy vừa chat hoặc gọi điện thoại thườ...  \n\n[1583 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>giáo trình chưa cụ thể</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giảng buồn ngủ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giáo viên vui tính  tận tâm</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giảng viên nên giao bài tập nhiều hơn  chia nh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giảng viên cần giảng bài chi tiết hơn  đi sâu ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1578</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>hướng dẫn lab mơ hồ</td>\n    </tr>\n    <tr>\n      <th>1579</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>thầy cho chúng em những bài tập mang tính thực...</td>\n    </tr>\n    <tr>\n      <th>1580</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>thầy không dạy nhiều chủ yếu cho sinh viên tự ...</td>\n    </tr>\n    <tr>\n      <th>1581</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>em muốn đổi tên môn học vì tên môn là lập trìn...</td>\n    </tr>\n    <tr>\n      <th>1582</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>thầy vừa dạy vừa chat hoặc gọi điện thoại thườ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1583 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"combined_df.to_csv('dev.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.799254Z","iopub.execute_input":"2024-05-26T10:53:12.799562Z","iopub.status.idle":"2024-05-26T10:53:12.813095Z","shell.execute_reply.started":"2024-05-26T10:53:12.799536Z","shell.execute_reply":"2024-05-26T10:53:12.812075Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.866507Z","iopub.execute_input":"2024-05-26T10:53:12.866810Z","iopub.status.idle":"2024-05-26T10:53:12.876620Z","shell.execute_reply.started":"2024-05-26T10:53:12.866786Z","shell.execute_reply":"2024-05-26T10:53:12.875649Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"              Topic Sentiment  \\\n0  TRAINING_PROGRAM  NEGATIVE   \n1          LECTURER  NEGATIVE   \n2          LECTURER  POSITIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  NEGATIVE   \n\n                                            Sentence  \n0                            giáo trình chưa cụ thể   \n1                                    giảng buồn ngủ   \n2                       giáo viên vui tính  tận tâm   \n3  giảng viên nên giao bài tập nhiều hơn  chia nh...  \n4  giảng viên cần giảng bài chi tiết hơn  đi sâu ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>giáo trình chưa cụ thể</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giảng buồn ngủ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giáo viên vui tính  tận tâm</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giảng viên nên giao bài tập nhiều hơn  chia nh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giảng viên cần giảng bài chi tiết hơn  đi sâu ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Define the file paths\ntopics_file ='/kaggle/input/uit-vsfc/test/topics.txt'\nsentiments_file ='/kaggle/input/uit-vsfc/test/sentiments.txt'\nsents_file ='/kaggle/input/uit-vsfc/test/sents.txt'\n\n# Read the files\ntopics = pd.read_csv(topics_file, header=None, names=['Topic'])\nsentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n\n# Read the sentences file line by line\nwith open(sents_file,'r', encoding='utf-8') as file:\n    sentences = file.readlines()\n\n# Remove newline characters and punctuation from sentences, and split by whitespace\nsentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n\n# Create a DataFrame for sentences\nsentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n\n# Combine the DataFrames along the columns\ncombined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n\n# Display the first few rows of the combined DataFrame\nprint(combined_df.head())\n\n# Replace values in'Topic'column\ntopic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\ncombined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n\n# Replace values in'Sentiment'column\nsentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\ncombined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n\n# Display the modified DataFrame\nprint(combined_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.933318Z","iopub.execute_input":"2024-05-26T10:53:12.933645Z","iopub.status.idle":"2024-05-26T10:53:12.997497Z","shell.execute_reply.started":"2024-05-26T10:53:12.933618Z","shell.execute_reply":"2024-05-26T10:53:12.996363Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"   Topic  Sentiment                                           Sentence\n0      0          2                            nói tiếng anh lưu loát \n1      0          2                            giáo viên rất vui tính \n2      0          2                                     cô max có tâm \n3      0          2                         giảng bài thu hút  dí dỏm \n4      0          0  giáo viên không giảng dạy kiến thức  hướng dẫn...\n      Topic Sentiment                                           Sentence\n0  LECTURER  POSITIVE                            nói tiếng anh lưu loát \n1  LECTURER  POSITIVE                            giáo viên rất vui tính \n2  LECTURER  POSITIVE                                     cô max có tâm \n3  LECTURER  POSITIVE                         giảng bài thu hút  dí dỏm \n4  LECTURER  NEGATIVE  giáo viên không giảng dạy kiến thức  hướng dẫn...\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.999115Z","iopub.execute_input":"2024-05-26T10:53:12.999452Z","iopub.status.idle":"2024-05-26T10:53:13.012341Z","shell.execute_reply.started":"2024-05-26T10:53:12.999424Z","shell.execute_reply":"2024-05-26T10:53:13.011173Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                 Topic Sentiment  \\\n0             LECTURER  POSITIVE   \n1             LECTURER  POSITIVE   \n2             LECTURER  POSITIVE   \n3             LECTURER  POSITIVE   \n4             LECTURER  NEGATIVE   \n...                ...       ...   \n3161          LECTURER  NEGATIVE   \n3162          LECTURER  POSITIVE   \n3163          LECTURER  POSITIVE   \n3164          LECTURER  NEGATIVE   \n3165  TRAINING_PROGRAM   NEUTRAL   \n\n                                               Sentence  \n0                               nói tiếng anh lưu loát   \n1                               giáo viên rất vui tính   \n2                                        cô max có tâm   \n3                            giảng bài thu hút  dí dỏm   \n4     giáo viên không giảng dạy kiến thức  hướng dẫn...  \n...                                                 ...  \n3161  các slide khó hiểu  ngôn ngữ trong slide phức ...  \n3162                  giáo viên giảng dạy có tâm huyết   \n3163                     chia sẻ cho em nhiều điều hay   \n3164                                  em tiếp thu chậm   \n3165  em có học ở một trung tâm tiếng anh ở ngoài tr...  \n\n[3166 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>nói tiếng anh lưu loát</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giáo viên rất vui tính</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>cô max có tâm</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giảng bài thu hút  dí dỏm</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giáo viên không giảng dạy kiến thức  hướng dẫn...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3161</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>các slide khó hiểu  ngôn ngữ trong slide phức ...</td>\n    </tr>\n    <tr>\n      <th>3162</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giáo viên giảng dạy có tâm huyết</td>\n    </tr>\n    <tr>\n      <th>3163</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>chia sẻ cho em nhiều điều hay</td>\n    </tr>\n    <tr>\n      <th>3164</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>em tiếp thu chậm</td>\n    </tr>\n    <tr>\n      <th>3165</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEUTRAL</td>\n      <td>em có học ở một trung tâm tiếng anh ở ngoài tr...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3166 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.to_csv('test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.081996Z","iopub.execute_input":"2024-05-26T10:53:13.082326Z","iopub.status.idle":"2024-05-26T10:53:13.106416Z","shell.execute_reply.started":"2024-05-26T10:53:13.082298Z","shell.execute_reply":"2024-05-26T10:53:13.105414Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.408559Z","iopub.execute_input":"2024-05-26T10:53:13.409567Z","iopub.status.idle":"2024-05-26T10:53:13.420776Z","shell.execute_reply.started":"2024-05-26T10:53:13.409517Z","shell.execute_reply":"2024-05-26T10:53:13.419739Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      Topic Sentiment                                           Sentence\n0  LECTURER  POSITIVE                            nói tiếng anh lưu loát \n1  LECTURER  POSITIVE                            giáo viên rất vui tính \n2  LECTURER  POSITIVE                                     cô max có tâm \n3  LECTURER  POSITIVE                         giảng bài thu hút  dí dỏm \n4  LECTURER  NEGATIVE  giáo viên không giảng dạy kiến thức  hướng dẫn...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>nói tiếng anh lưu loát</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giáo viên rất vui tính</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>cô max có tâm</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>giảng bài thu hút  dí dỏm</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giáo viên không giảng dạy kiến thức  hướng dẫn...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load dataset\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Đường dẫn tới các file\ntrain_file ='/kaggle/working/train.csv'\nval_file ='/kaggle/working/dev.csv'\ntest_file ='/kaggle/working/test.csv'\n\n# Đọc các file CSV\ntrain_df = pd.read_csv(train_file)\nval_df = pd.read_csv(val_file)\ntest_df = pd.read_csv(test_file)\n\n# Lấy tất cả các tên cột từ cả ba DataFrame\nall_columns = set(train_df.columns).union(set(val_df.columns)).union(set(test_df.columns))\n\nall_columns","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.699866Z","iopub.execute_input":"2024-05-26T10:53:13.700704Z","iopub.status.idle":"2024-05-26T10:53:13.761596Z","shell.execute_reply.started":"2024-05-26T10:53:13.700667Z","shell.execute_reply":"2024-05-26T10:53:13.760454Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'Sentence', 'Sentiment', 'Topic'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess data","metadata":{}},{"cell_type":"code","source":"!pip install underthesea","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.769401Z","iopub.execute_input":"2024-05-26T10:53:13.769700Z","iopub.status.idle":"2024-05-26T10:53:30.776182Z","shell.execute_reply.started":"2024-05-26T10:53:13.769674Z","shell.execute_reply":"2024-05-26T10:53:30.774569Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collecting underthesea\n  Downloading underthesea-6.8.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\nCollecting python-crfsuite>=0.9.6 (from underthesea)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.31.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)\nCollecting underthesea-core==1.0.4 (from underthesea)\n  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)\nDownloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\nSuccessfully installed python-crfsuite-0.9.10 underthesea-6.8.0 underthesea-core-1.0.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyvi","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:30.778404Z","iopub.execute_input":"2024-05-26T10:53:30.778716Z","iopub.status.idle":"2024-05-26T10:53:45.158518Z","shell.execute_reply.started":"2024-05-26T10:53:30.778685Z","shell.execute_reply":"2024-05-26T10:53:45.157347Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting pyvi\n  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pyvi) (1.2.2)\nCollecting sklearn-crfsuite (from pyvi)\n  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.2.0)\nRequirement already satisfied: python-crfsuite>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (1.16.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (4.66.1)\nDownloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\nInstalling collected packages: sklearn-crfsuite, pyvi\nSuccessfully installed pyvi-0.1.1 sklearn-crfsuite-0.3.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import regex as re\nimport string\nimport emoji\n\nfrom nltk import flatten\nimport unicodedata\nfrom underthesea import word_tokenize\nfrom pyvi import ViTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:45.159961Z","iopub.execute_input":"2024-05-26T10:53:45.160314Z","iopub.status.idle":"2024-05-26T10:53:46.788861Z","shell.execute_reply.started":"2024-05-26T10:53:45.160279Z","shell.execute_reply":"2024-05-26T10:53:46.787863Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 1. Loại bỏ các thẻ HTML\ndef remove_HTML(text):\n    clean = re.compile('<.*?>')\n    return re.sub(clean,'', text)\n\n# 2. Chuyển đổi các ký tự Unicode về dạng chuẩn\ndef convert_unicode(text):\n    return unicodedata.normalize('NFC', text)\n\n# 3. Loại bỏ các ký tự kéo dài\ndef remove_elongated_chars(text):\n    replacements = {\n       'a':'àáảãạăằắẳẵặâầấẩẫậ',\n       'e':'èéẻẽẹêềếểễệ',\n       'i':'ìíỉĩị',\n       'o':'òóỏõọôồốổỗộơờớởỡợ',\n       'u':'ùúủũụưừứửữự',\n       'y':'ỳýỷỹỵ',\n       'd':'đ',\n       'A':'ÀÁẢÃẠĂẰẮẲẴẶÂẦẤẨẪẬ',\n       'E':'ÈÉẺẼẸÊỀẾỂỄỆ',\n       'I':'ÌÍỈĨỊ',\n       'O':'ÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢ',\n       'U':'ÙÚỦŨỤƯỪỨỬỮỰ',\n       'Y':'ỲÝỶỸỴ',\n       'D':'Đ'\n    }\n    \n    for char, replacements_str in replacements.items():\n        pattern = rf\"({char})\\1+\"\n        text = re.sub(pattern, char, text)\n    \n    pattern = r\"(\\w)\\1+\"\n    text = re.sub(pattern, r'\\1', text)\n    return text\n\n# 4. Xử lý các từ phủ định\ndef handle_negation(text):\n    not_words = {\"không\",'không hề', \"chẳng\", \"chưa\", \"không phải\", \"chả\", \"mất\",\n                 \"thiếu\", \"đếch\", \"đéo\", \"kém\", \"nỏ\", \"not\",\n                 \"bớt\", \"không bao giờ\", \"chưa bao giờ\"}\n    not_words = sorted(not_words, key=len, reverse=True)\n    pattern = r'\\b(?:'+'|'.join(re.escape(word) for word in not_words) + r')\\b'\n    text = re.sub(pattern,'không', text, flags=re.IGNORECASE)\n    return text\n\n# 5. Chuẩn hóa các từ viết tắt thường gặp\ndef normalize_acronyms(text):\n    acronyms = {\n       'ô kêi':'ok','okie':'ok','o kê':'ok',\n       'okey':'ok','ôkê':'ok','oki':'ok','oke': 'ok','okay':'ok','okê':'ok',\n       'tks': u'cám ơn','thks': u'cám ơn','thanks': u'cám ơn','ths': u'cám ơn','thank': u'cám ơn',\n       '⭐':'star','*':'star','🌟':'star',\n       'kg': u'không','not': u'không', u'kg': u'không','\"k': u'không','kh':u'không','kô':u'không','hok':u'không','kp': u'không phải',u'kô': u'không','\"ko': u'không', u'ko': u'không', u'k': u'không','khong': u'không', u'hok': u'không',\n       'cute': u'dễ thương','vs': u'với','wa':'quá','wá': u'quá','j': u'gì','“':'',\n       'sz': u'cỡ','size': u'cỡ', u'đx': u'được','dk': u'được','dc': u'được','đk': u'được',\n       'đc': u'được','authentic': u'chuẩn chính hãng',u'aut': u'chuẩn chính hãng', u'auth': u'chuẩn chính hãng','store': u'cửa hàng',\n       'shop': u'cửa hàng','sp': u'sản phẩm','gud': u'tốt','god': u'tốt','wel done':'tốt','good': u'tốt','gút': u'tốt',\n       'sấu': u'xấu','gut': u'tốt', u'tot': u'tốt', u'nice': u'tốt','perfect':'rất tốt','bt': u'bình thường',\n       'time': u'thời gian','qá': u'quá', u'ship': u'giao hàng', u'm': u'mình', u'mik': u'mình',\n       'ể':'ể','product':'sản phẩm','quality':'chất lượng','chat':'chất','excelent':'hoàn hảo','bad':'tệ','fresh':'tươi','sad':'tệ',\n       'date': u'hạn sử dụng','hsd': u'hạn sử dụng','quickly': u'nhanh','quick': u'nhanh','fast': u'nhanh','delivery': u'giao hàng',u'síp': u'giao hàng',\n       'beautiful': u'đẹp tuyệt vời', u'tl': u'trả lời', u'r': u'rồi', u'shopE': u'cửa hàng',u'order': u'đặt hàng',\n       'chất lg': u'chất lượng',u'sd': u'sử dụng',u'dt': u'điện thoại',u'nt': u'nhắn tin',u'tl': u'trả lời',u'sài': u'xài',u'bjo':u'bao giờ',\n       'thick': u'thích','thik': u'thích', u'sop': u'cửa hàng', u'shop': u'cửa hàng', \n       'fb':'facebook','face':'facebook','very': u'rất',u'quả ng':u'quảng ',\n       'dep': u'đẹp',u'xau': u'xấu','delicious': u'ngon', u'hàg': u'hàng', u'qủa': u'quả',\n       'iu': u'yêu','fake': u'giả mạo','trl':'trả lời',\n       'por': u'tệ','poor': u'tệ','ib':u'nhắn tin','rep':u'trả lời',u'fback':'feedback','fedback':'feedback',\n       'max': u'cực kỳ',\n       'full':'đầy đủ', 'ful':'đầy đủ'\n    }\n    words = text.split()\n    normalized_text =' '.join([acronyms.get(word.lower(), word) for word in words])\n    return normalized_text\n\n# 6. Phân đoạn từ cho tiếng Việt\ndef word_segmentation(text):\n    return word_tokenize(text, format=\"text\")\n\n# 7. Loại bỏ các ký tự không cần thiết khỏi văn bản\ndef remove_unnecessary_characters(text):\n    text = re.sub(r'\\s+', ' ', text)  # Loại bỏ khoảng trắng thừa\n    text = re.sub(r'[^\\w\\s]', '', text)  # Loại bỏ các ký tự đặc biệt\n    return text.strip()\n\n# 8. Kết hợp các hàm để tiền xử lý văn bản\ndef text_preprocess(text):\n    text = remove_HTML(text)\n    text = normalize_acronyms(text)\n    text = convert_unicode(text)\n    text = remove_elongated_chars(text)\n    text = handle_negation(text)\n    #text = word_segmentation(text)\n    text = remove_unnecessary_characters(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:46.791485Z","iopub.execute_input":"2024-05-26T10:53:46.791807Z","iopub.status.idle":"2024-05-26T10:53:46.817763Z","shell.execute_reply.started":"2024-05-26T10:53:46.791778Z","shell.execute_reply":"2024-05-26T10:53:46.816926Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"text = \"slide giáo trình đầy đủ\"\nprint(text_preprocess(text))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:46.818828Z","iopub.execute_input":"2024-05-26T10:53:46.819203Z","iopub.status.idle":"2024-05-26T10:53:46.842678Z","shell.execute_reply.started":"2024-05-26T10:53:46.819168Z","shell.execute_reply":"2024-05-26T10:53:46.841653Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"slide giáo trình đầy đủ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install tf-models-official\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:46.844095Z","iopub.execute_input":"2024-05-26T10:53:46.844781Z","iopub.status.idle":"2024-05-26T10:55:15.461519Z","shell.execute_reply.started":"2024-05-26T10:53:46.844745Z","shell.execute_reply":"2024-05-26T10:55:15.460460Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Collecting tf-models-official\n  Downloading tf_models_official-2.16.0-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (3.0.8)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.5.0)\nCollecting gin-config (from tf-models-official)\n  Downloading gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.126.0)\nCollecting immutabledict (from tf-models-official)\n  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.6.12)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (3.7.5)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.26.4)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.1.3)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.9.0.80)\nRequirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.1.4)\nRequirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (5.9.3)\nRequirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.0.0)\nCollecting pycocotools (from tf-models-official)\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (6.0.1)\nCollecting sacrebleu (from tf-models-official)\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.11.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.2.0)\nCollecting seqeval (from tf-models-official)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.16.0)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.9.4)\nRequirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.16.1)\nCollecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\nCollecting tensorflow-text~=2.16.1 (from tf-models-official)\n  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nCollecting tensorflow~=2.16.1 (from tf-models-official)\n  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting tf-keras>=2.16.0 (from tf-models-official)\n  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting tf-slim>=1.1.0 (from tf-models-official)\n  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.21.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.26.1)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2024.2.2)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (6.1.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2023.4)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (16.0.6)\nCollecting ml-dtypes~=0.3.1 (from tensorflow~=2.16.1->tf-models-official)\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (69.0.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.14.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.51.1)\nCollecting tensorboard<2.17,>=2.16 (from tensorflow~=2.16.1->tf-models-official)\n  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.2.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.35.0)\nRequirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (3.1.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.5.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.3.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (4.9)\nCollecting portalocker (from sacrebleu->tf-models-official)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (5.2.1)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official) (1.2.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (8.1.7)\nRequirement already satisfied: etils>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (1.6.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\nRequirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.5.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.16.1->tf-models-official) (0.42.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (2024.2.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (6.1.1)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (3.17.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.62.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.6)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (3.0.2)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.1.2)\nDownloading tf_models_official-2.16.0-py2.py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=0188b2cd398c4b27cd09f4b63a4c4246a0a18e17277577a4ab32950be32e633b\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: gin-config, tf-slim, tensorflow-model-optimization, portalocker, ml-dtypes, immutabledict, tensorboard, sacrebleu, seqeval, pycocotools, tensorflow, tf-keras, tensorflow-text, tf-models-official\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n  Attempting uninstall: tf-keras\n    Found existing installation: tf_keras 2.15.1\n    Uninstalling tf_keras-2.15.1:\n      Successfully uninstalled tf_keras-2.15.1\n  Attempting uninstall: tensorflow-text\n    Found existing installation: tensorflow-text 2.15.0\n    Uninstalling tensorflow-text-2.15.0:\n      Successfully uninstalled tensorflow-text-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gin-config-0.5.0 immutabledict-4.2.0 ml-dtypes-0.3.2 portalocker-2.8.2 pycocotools-2.0.7 sacrebleu-2.4.2 seqeval-1.2.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-model-optimization-0.8.0 tensorflow-text-2.16.1 tf-keras-2.16.0 tf-models-official-2.16.0 tf-slim-1.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tensorflow.keras.layers import Input, Dense, Dropout, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom official.nlp import optimization\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:15.462954Z","iopub.execute_input":"2024-05-26T10:55:15.463297Z","iopub.status.idle":"2024-05-26T10:55:25.914052Z","shell.execute_reply.started":"2024-05-26T10:55:15.463261Z","shell.execute_reply":"2024-05-26T10:55:25.912946Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Define pre-trained model and tokenizer\nPRETRAINED_MODEL = 'vinai/phobert-base'  # Choose your preferred Vietnamese BERT model\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\ntokenizer.max_model_input_sizes","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:25.915538Z","iopub.execute_input":"2024-05-26T10:55:25.916387Z","iopub.status.idle":"2024-05-26T10:55:27.200977Z","shell.execute_reply.started":"2024-05-26T10:55:25.916348Z","shell.execute_reply":"2024-05-26T10:55:27.199957Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae9b5928bcbc4c4fa66f258d961ee21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9e49d3a1d448f2afe4b333e9f439a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b95071991b4b5e95d8dcb0ca6de2fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ce5da06b93a4bfa985aa0ef943f07cf"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'vinai/phobert-base': 256, 'vinai/phobert-large': 256}"},"metadata":{}}]},{"cell_type":"code","source":"# Prepare data for TensorFlow\nMAX_SEQUENCE_LENGTH = tokenizer.model_max_length\nBATCH_SIZE = 16\nEPOCHS = 10\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.202248Z","iopub.execute_input":"2024-05-26T10:55:27.202532Z","iopub.status.idle":"2024-05-26T10:55:27.207253Z","shell.execute_reply.started":"2024-05-26T10:55:27.202508Z","shell.execute_reply":"2024-05-26T10:55:27.206174Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Define labels\n\n# Perform one-hot encoding on 'Topic' column\none_hot_encoded = pd.get_dummies(train_df['Topic'])\none_hot_encoded = one_hot_encoded.astype(int)\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\ntrain_df = pd.concat([train_df, one_hot_encoded], axis=1)\n\n# Drop the original 'result' column\ntrain_df.drop(columns=['Topic'], inplace=True)\n\n# Perform one-hot encoding on 'result' column\none_hot_encoded1 = pd.get_dummies(val_df['Topic'])\none_hot_encoded1 = one_hot_encoded1.astype(int)\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\nval_df = pd.concat([val_df, one_hot_encoded1], axis=1)\n\n# Drop the original 'result' column\nval_df.drop(columns=['Topic'], inplace=True)\n\n# Perform one-hot encoding on 'result' column\none_hot_encoded2 = pd.get_dummies(test_df['Topic'])\none_hot_encoded2 = one_hot_encoded2.astype(int)\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\ntest_df = pd.concat([test_df, one_hot_encoded2], axis=1)\n\n# Drop the original 'result' column\ntest_df.drop(columns=['Topic'], inplace=True)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.212193Z","iopub.execute_input":"2024-05-26T10:55:27.212495Z","iopub.status.idle":"2024-05-26T10:55:27.222056Z","shell.execute_reply.started":"2024-05-26T10:55:27.212459Z","shell.execute_reply":"2024-05-26T10:55:27.221116Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"\"# Define labels\\n\\n# Perform one-hot encoding on 'Topic' column\\none_hot_encoded = pd.get_dummies(train_df['Topic'])\\none_hot_encoded = one_hot_encoded.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\ntrain_df = pd.concat([train_df, one_hot_encoded], axis=1)\\n\\n# Drop the original 'result' column\\ntrain_df.drop(columns=['Topic'], inplace=True)\\n\\n# Perform one-hot encoding on 'result' column\\none_hot_encoded1 = pd.get_dummies(val_df['Topic'])\\none_hot_encoded1 = one_hot_encoded1.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\nval_df = pd.concat([val_df, one_hot_encoded1], axis=1)\\n\\n# Drop the original 'result' column\\nval_df.drop(columns=['Topic'], inplace=True)\\n\\n# Perform one-hot encoding on 'result' column\\none_hot_encoded2 = pd.get_dummies(test_df['Topic'])\\none_hot_encoded2 = one_hot_encoded2.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\ntest_df = pd.concat([test_df, one_hot_encoded2], axis=1)\\n\\n# Drop the original 'result' column\\ntest_df.drop(columns=['Topic'], inplace=True)\\n\\n\""},"metadata":{}}]},{"cell_type":"code","source":"# Define the mapping for sentiment\nsentiment_mapping = {'POSITIVE': 1, 'NEGATIVE': 2, 'NEUTRAL': 3}\n\n# Function to perform the customized one-hot encoding\ndef custom_one_hot_encoding(df):\n    # One-hot encode the 'Topic' column\n    one_hot_encoded = pd.get_dummies(df['Topic'])\n    # Map the 'Sentiment' values to the specified values\n    one_hot_encoded = one_hot_encoded * df['Sentiment'].map(sentiment_mapping).values[:, None]\n    # Concatenate the one-hot encoded DataFrame with the original DataFrame\n    df = pd.concat([df, one_hot_encoded], axis=1)\n    # Drop the original 'Topic' column\n    df.drop(columns=['Topic'], inplace=True)\n    return df\n\n# Apply the function to each DataFrame\ntrain_df = custom_one_hot_encoding(train_df)\nval_df = custom_one_hot_encoding(val_df)\ntest_df = custom_one_hot_encoding(test_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.223204Z","iopub.execute_input":"2024-05-26T10:55:27.223553Z","iopub.status.idle":"2024-05-26T10:55:27.262333Z","shell.execute_reply.started":"2024-05-26T10:55:27.223521Z","shell.execute_reply":"2024-05-26T10:55:27.261515Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(['Sentiment'], axis = 1)\nval_df = val_df.drop(['Sentiment'], axis = 1)\ntest_df = test_df.drop(['Sentiment'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.263495Z","iopub.execute_input":"2024-05-26T10:55:27.263819Z","iopub.status.idle":"2024-05-26T10:55:27.271464Z","shell.execute_reply.started":"2024-05-26T10:55:27.263791Z","shell.execute_reply":"2024-05-26T10:55:27.270442Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.272675Z","iopub.execute_input":"2024-05-26T10:55:27.273030Z","iopub.status.idle":"2024-05-26T10:55:27.284327Z","shell.execute_reply.started":"2024-05-26T10:55:27.272993Z","shell.execute_reply":"2024-05-26T10:55:27.283228Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def make_outputs(df):\n    outputs = []\n    for row in range(len(df)):\n        row_one_hot = []\n        for col in range(1, len(df.columns)):\n            sentiment = df.iloc[row, col]\n            if sentiment == 0: one_hot = [1, 0, 0, 0] #None\n            elif sentiment == 1: one_hot = [0, 1, 0, 0] # Pos\n            elif sentiment == 2: one_hot = [0, 0, 1, 0] # Neg\n            elif sentiment == 3: one_hot = [0, 0, 0, 1] # Neu\n            row_one_hot.append(one_hot)\n        outputs.append(row_one_hot)\n    return np.array(outputs, dtype='uint8')\ny_train = make_outputs(train_df)\ny_val = make_outputs(val_df)\ny_test = make_outputs(test_df)\n\nprint('Train outputs:', y_train.shape)\nprint('Validate outputs:', y_val.shape)\nprint('Test outputs:', y_test.shape)\ny_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.285652Z","iopub.execute_input":"2024-05-26T10:55:27.286065Z","iopub.status.idle":"2024-05-26T10:55:29.887633Z","shell.execute_reply.started":"2024-05-26T10:55:27.286031Z","shell.execute_reply":"2024-05-26T10:55:29.886576Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Train outputs: (11426, 4, 4)\nValidate outputs: (1583, 4, 4)\nTest outputs: (3166, 4, 4)\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"train_df.to_csv('train_tokenize.csv', index = False)\nval_df.to_csv('val_tokenize.csv', index = False)\ntest_df.to_csv('test_tokenize.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:29.889265Z","iopub.execute_input":"2024-05-26T10:55:29.890049Z","iopub.status.idle":"2024-05-26T10:55:29.982726Z","shell.execute_reply.started":"2024-05-26T10:55:29.890007Z","shell.execute_reply":"2024-05-26T10:55:29.981732Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/working/train_tokenize.csv\"\nVAL_PATH = \"/kaggle/working/val_tokenize.csv\"\nTEST_PATH = \"/kaggle/working/test_tokenize.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:29.984072Z","iopub.execute_input":"2024-05-26T10:55:29.984445Z","iopub.status.idle":"2024-05-26T10:55:29.989898Z","shell.execute_reply.started":"2024-05-26T10:55:29.984409Z","shell.execute_reply":"2024-05-26T10:55:29.988931Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nraw_datasets = load_dataset('csv', data_files={'train': TRAIN_PATH, 'val': VAL_PATH, 'test': TEST_PATH})\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:29.991439Z","iopub.execute_input":"2024-05-26T10:55:29.992193Z","iopub.status.idle":"2024-05-26T10:55:30.310416Z","shell.execute_reply.started":"2024-05-26T10:55:29.992156Z","shell.execute_reply":"2024-05-26T10:55:30.309366Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e340ffd06f41269407ae09dd89cdba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abee33aae9fc4ea68de4ccb5cd5d9380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7c006ffa2746e7b4ccb16c250ed460"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n        num_rows: 11426\n    })\n    val: Dataset({\n        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n        num_rows: 1583\n    })\n    test: Dataset({\n        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n        num_rows: 3166\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print('input_ids of sentence 1484:', raw_datasets['train'][1484])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:30.311731Z","iopub.execute_input":"2024-05-26T10:55:30.312053Z","iopub.status.idle":"2024-05-26T10:55:30.323132Z","shell.execute_reply.started":"2024-05-26T10:55:30.312026Z","shell.execute_reply":"2024-05-26T10:55:30.322145Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"input_ids of sentence 1484: {'Sentence': 'thầy tận tâm với học sinh  nhiệt tình với học sinh ', 'FACILITY': 0, 'LECTURER': 1, 'OTHERS': 0, 'TRAINING_PROGRAM': 0}\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(dataset):\n    clean_texts = list(map(text_preprocess, dataset['Sentence']))\n    return tokenizer(clean_texts, max_length=tokenizer.model_max_length, padding='max_length', truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:30.324350Z","iopub.execute_input":"2024-05-26T10:55:30.324737Z","iopub.status.idle":"2024-05-26T10:55:30.332745Z","shell.execute_reply.started":"2024-05-26T10:55:30.324700Z","shell.execute_reply":"2024-05-26T10:55:30.331837Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\nprint('input_ids of sentence 1484:', tokenized_datasets['train'][1484]['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:30.334310Z","iopub.execute_input":"2024-05-26T10:55:30.334704Z","iopub.status.idle":"2024-05-26T10:55:50.686264Z","shell.execute_reply.started":"2024-05-26T10:55:30.334669Z","shell.execute_reply":"2024-05-26T10:55:50.685226Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c7c5a75e2044a296deaaa20891f115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00acd5650b9b4ceeb4e4b039b1356220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3166 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62284d09817c4515b92dfd8d74fa8961"}},"metadata":{}},{"name":"stdout","text":"input_ids of sentence 1484: [0, 1249, 1855, 2652, 15, 222, 418, 2515, 939, 15, 222, 418, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Decode:', tokenizer.decode(tokenized_datasets['train'][1484]['input_ids']))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:50.687773Z","iopub.execute_input":"2024-05-26T10:55:50.688489Z","iopub.status.idle":"2024-05-26T10:55:50.698161Z","shell.execute_reply.started":"2024-05-26T10:55:50.688442Z","shell.execute_reply":"2024-05-26T10:55:50.697266Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Decode: <s> thầy tận tâm với học sinh nhiệt tình với học sinh </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_datasets['train']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:50.699840Z","iopub.execute_input":"2024-05-26T10:55:50.700151Z","iopub.status.idle":"2024-05-26T10:55:50.733720Z","shell.execute_reply.started":"2024-05-26T10:55:50.700126Z","shell.execute_reply":"2024-05-26T10:55:50.732515Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 11426\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"], batch_size=BATCH_SIZE, shuffle=True\n)\n\nvalidation_dataset = tokenized_datasets[\"val\"].to_tf_dataset(\n    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"], batch_size=BATCH_SIZE, shuffle=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:50.735349Z","iopub.execute_input":"2024-05-26T10:55:50.735709Z","iopub.status.idle":"2024-05-26T10:55:51.916942Z","shell.execute_reply.started":"2024-05-26T10:55:50.735677Z","shell.execute_reply":"2024-05-26T10:55:51.916099Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tensorflow.keras import layers, Model","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.918151Z","iopub.execute_input":"2024-05-26T10:55:51.918412Z","iopub.status.idle":"2024-05-26T10:55:51.923329Z","shell.execute_reply.started":"2024-05-26T10:55:51.918387Z","shell.execute_reply":"2024-05-26T10:55:51.922092Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class BertLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(BertLayer, self).__init__(**kwargs)\n        self.bert = TFAutoModel.from_pretrained(PRETRAINED_MODEL)\n\n    def call(self, inputs):\n        input_ids, attention_mask, token_type_ids = inputs\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        return outputs[0]\n\ndef create_model(optimizer):\n    # Build your model on top of BERT\n    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='token_type_ids')\n\n    bert_layer = BertLayer()([input_ids, attention_mask, token_type_ids])\n    pooled_output = bert_layer[:, 0, :]  # Take only the first token\n    #dense = layers.Dense(16, activation='softmax')(pooled_output)\n    dense = concatenate([\n        Dense(\n            units = 4, \n            activation = 'softmax',\n            name = label,\n        )(pooled_output) for label in train_df.columns[1:]\n    ], axis = -1)\n    # Define your model\n    model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=dense)\n\n    # Compile your model\n    #model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n    \n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.924765Z","iopub.execute_input":"2024-05-26T10:55:51.925086Z","iopub.status.idle":"2024-05-26T10:55:51.937262Z","shell.execute_reply.started":"2024-05-26T10:55:51.925060Z","shell.execute_reply":"2024-05-26T10:55:51.936284Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\noptimizer = Adam(learning_rate=1e-5)\ntype(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.938396Z","iopub.execute_input":"2024-05-26T10:55:51.938719Z","iopub.status.idle":"2024-05-26T10:55:51.961706Z","shell.execute_reply.started":"2024-05-26T10:55:51.938693Z","shell.execute_reply":"2024-05-26T10:55:51.960639Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"keras.src.optimizers.adam.Adam"},"metadata":{}}]},{"cell_type":"code","source":"model = create_model(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.962989Z","iopub.execute_input":"2024-05-26T10:55:51.963378Z","iopub.status.idle":"2024-05-26T10:56:23.023072Z","shell.execute_reply.started":"2024-05-26T10:55:51.963343Z","shell.execute_reply":"2024-05-26T10:56:23.022109Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"tf_model.h5:   0%|          | 0.00/740M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab0f46a6d7049f088ad99181e729ee9"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_type_ids      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_layer          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mBertLayer\u001b[0m)         │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ token_type_ids[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item (\u001b[38;5;33mGetItem\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bert_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ FACILITY (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LECTURER (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ OTHERS (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ TRAINING_PROGRAM    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ FACILITY[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ LECTURER[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│                     │                   │            │ OTHERS[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ TRAINING_PROGRAM… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_type_ids      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_layer          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ FACILITY (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LECTURER (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ OTHERS (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ TRAINING_PROGRAM    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ FACILITY[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ LECTURER[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│                     │                   │            │ OTHERS[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ TRAINING_PROGRAM… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Create TensorFlow Datasets\ndef encode_data(tokenized_datasets, labels):\n    return (\n        {\n            'input_ids': tf.constant(tokenized_datasets['input_ids']),\n            'attention_mask': tf.constant(tokenized_datasets['attention_mask']),\n            'token_type_ids': tf.constant(tokenized_datasets['token_type_ids'])\n        },\n        tf.constant(labels)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:23.029551Z","iopub.execute_input":"2024-05-26T10:56:23.029847Z","iopub.status.idle":"2024-05-26T10:56:23.035361Z","shell.execute_reply.started":"2024-05-26T10:56:23.029821Z","shell.execute_reply":"2024-05-26T10:56:23.034450Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_data = encode_data(tokenized_datasets['train'], np.reshape(y_train, (-1, 16)))\nval_data = encode_data(tokenized_datasets['val'], np.reshape(y_val, (-1, 16)))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:23.036479Z","iopub.execute_input":"2024-05-26T10:56:23.036739Z","iopub.status.idle":"2024-05-26T10:56:30.462063Z","shell.execute_reply.started":"2024-05-26T10:56:23.036716Z","shell.execute_reply":"2024-05-26T10:56:30.461054Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:30.463398Z","iopub.execute_input":"2024-05-26T10:56:30.463788Z","iopub.status.idle":"2024-05-26T10:56:30.473912Z","shell.execute_reply.started":"2024-05-26T10:56:30.463757Z","shell.execute_reply":"2024-05-26T10:56:30.472850Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"({'input_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[    0, 48090,  4368, ...,     1,     1,     1],\n         [    0,  2515,   939, ...,     1,     1,     1],\n         [    0,    57,   222, ...,     1,     1,     1],\n         ...,\n         [    0,   574,   387, ...,     1,     1,     1],\n         [    0,  4368,  1430, ...,     1,     1,     1],\n         [    0,  1685,  2953, ...,     1,     1,     1]], dtype=int32)>,\n  'attention_mask': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         ...,\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n  'token_type_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>},\n <tf.Tensor: shape=(11426, 16), dtype=uint8, numpy=\n array([[1, 0, 0, ..., 1, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 1, 0],\n        ...,\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)"},"metadata":{}}]},{"cell_type":"code","source":"# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(BATCH_SIZE)\nval_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:30.475219Z","iopub.execute_input":"2024-05-26T10:56:30.475561Z","iopub.status.idle":"2024-05-26T10:56:31.279483Z","shell.execute_reply.started":"2024-05-26T10:56:30.475534Z","shell.execute_reply":"2024-05-26T10:56:31.278307Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:31.281112Z","iopub.execute_input":"2024-05-26T10:56:31.281743Z","iopub.status.idle":"2024-05-26T10:56:31.291567Z","shell.execute_reply.started":"2024-05-26T10:56:31.281706Z","shell.execute_reply":"2024-05-26T10:56:31.290414Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"({'input_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[    0, 48090,  4368, ...,     1,     1,     1],\n         [    0,  2515,   939, ...,     1,     1,     1],\n         [    0,    57,   222, ...,     1,     1,     1],\n         ...,\n         [    0,   574,   387, ...,     1,     1,     1],\n         [    0,  4368,  1430, ...,     1,     1,     1],\n         [    0,  1685,  2953, ...,     1,     1,     1]], dtype=int32)>,\n  'attention_mask': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         ...,\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n  'token_type_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>},\n <tf.Tensor: shape=(11426, 16), dtype=uint8, numpy=\n array([[1, 0, 0, ..., 1, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 1, 0],\n        ...,\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"# Huấn luyện mô hình\ndef train_model(model, train_dataset, val_dataset, epochs=3):\n\n    for epoch in range(epochs):\n        history = model.fit(\n                        train_dataset,\n                        epochs=epochs,\n                        validation_data=val_dataset\n                    )\n\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {history.history['loss'][0]:.4f}, Train Accuracy: {history.history['accuracy'][0]:.4f}\")\n        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {history.history['val_loss'][0]:.4f}, Val Accuracy: {history.history['val_accuracy'][0]:.4f}\")\n    \n    model.save('sentiment_model.h5')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:31.293041Z","iopub.execute_input":"2024-05-26T10:56:31.295439Z","iopub.status.idle":"2024-05-26T10:56:31.303270Z","shell.execute_reply.started":"2024-05-26T10:56:31.295411Z","shell.execute_reply":"2024-05-26T10:56:31.302165Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"'# Huấn luyện mô hình\\ndef train_model(model, train_dataset, val_dataset, epochs=3):\\n\\n    for epoch in range(epochs):\\n        history = model.fit(\\n                        train_dataset,\\n                        epochs=epochs,\\n                        validation_data=val_dataset\\n                    )\\n\\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {history.history[\\'loss\\'][0]:.4f}, Train Accuracy: {history.history[\\'accuracy\\'][0]:.4f}\")\\n        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {history.history[\\'val_loss\\'][0]:.4f}, Val Accuracy: {history.history[\\'val_accuracy\\'][0]:.4f}\")\\n    \\n    model.save(\\'sentiment_model.h5\\')'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade wandb tensorflow\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:31.304532Z","iopub.execute_input":"2024-05-26T10:56:31.304888Z","iopub.status.idle":"2024-05-26T10:56:49.582556Z","shell.execute_reply.started":"2024-05-26T10:56:31.304844Z","shell.execute_reply":"2024-05-26T10:56:49.581441Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nCollecting wandb\n  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.2.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\nDownloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Uninstalling wandb-0.16.6:\n      Successfully uninstalled wandb-0.16.6\nSuccessfully installed wandb-0.17.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nimport tensorflow as tf\nimport numpy as np\nimport warnings\nimport logging\n# Set TensorFlow logging level to ERROR to suppress warnings and informational messages\ntf.get_logger().setLevel('ERROR')\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\n\n# Additional method to suppress specific warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings and errors: 0 = all logs, 1 = filter out INFO, 2 = filter out WARNING, 3 = filter out ERROR\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:49.584074Z","iopub.execute_input":"2024-05-26T10:56:49.584390Z","iopub.status.idle":"2024-05-26T10:56:50.018960Z","shell.execute_reply.started":"2024-05-26T10:56:49.584358Z","shell.execute_reply":"2024-05-26T10:56:50.018066Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport os\nimport tensorflow as tf\n\n# Define the checkpoint path\ncheckpoint_path = \"sentiment_model_checkpoint.weights.h5\"\n# Set your WandB API key\nwandb.login(key=\"b9575849263a9312a73f76d71d270c8751628e10\")\n\n# Initialize WandB with a unique run ID based on current timestamp\nrun_id = \"9n6h5k0i\"\nwandb.init(project=\"absa-vietnamese\", id=run_id, resume=\"allow\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:50.020297Z","iopub.execute_input":"2024-05-26T10:56:50.020699Z","iopub.status.idle":"2024-05-26T10:56:53.938510Z","shell.execute_reply.started":"2024-05-26T10:56:50.020656Z","shell.execute_reply":"2024-05-26T10:56:53.937557Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlavibuu\u001b[0m (\u001b[33mlavibu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240526_105652-9n6h5k0i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Resuming run <strong><a href='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i' target=\"_blank\">azure-valley-25</a></strong> to <a href='https://wandb.ai/lavibu/absa-vietnamese' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lavibu/absa-vietnamese' target=\"_blank\">https://wandb.ai/lavibu/absa-vietnamese</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i' target=\"_blank\">https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i</a>"},"metadata":{}},{"execution_count":50,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7fdc70164280>"},"metadata":{}}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Định nghĩa early_stop_callback\nearly_stop_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  \n    patience=5,  \n    restore_best_weights=True  \n)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True,\n    verbose=1\n)\n\n\n# Thiết lập mức độ log của TensorFlow để chỉ hiển thị các lỗi nghiêm trọng\ntf.get_logger().setLevel('ERROR')\n\n# Check if checkpoint exists and load weights\nif os.path.exists(checkpoint_path):\n    model.load_weights(checkpoint_path)\n    print(\"Checkpoint loaded.\")\n\n# Define the training function\ndef train_model(model, train_dataset, val_dataset, epochs=3):\n    for epoch in range(epochs):\n        history = model.fit(\n            train_dataset,\n            epochs=1,  # Train for 1 epoch at a time\n            validation_data=val_dataset,\n            callbacks=[\n                early_stop_callback,\n                model_checkpoint_callback,\n            ],\n            verbose=1\n        )\n        # Calculate average train and validation loss\n        avg_train_loss = np.mean(history.history['loss'])\n        avg_val_loss = np.mean(history.history['val_loss'])\n\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"train_loss: {avg_train_loss:.4f} - val_loss: {avg_val_loss:.4f}\")\n\n        # Log metrics to WandB\n        wandb.log({\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n# Train the model\ntrain_model(model, train_dataset, val_dataset, epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:00:36.404367Z","iopub.execute_input":"2024-05-26T13:00:36.404735Z","iopub.status.idle":"2024-05-26T13:29:11.280915Z","shell.execute_reply.started":"2024-05-26T13:00:36.404706Z","shell.execute_reply":"2024-05-26T13:29:11.279732Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Checkpoint loaded.\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1145\nEpoch 1: val_loss improved from inf to 0.12247, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1145 - val_loss: 0.1225\nEpoch 1/10\ntrain_loss: 0.1155 - val_loss: 0.1225\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1144\nEpoch 1: val_loss improved from 0.12247 to 0.12241, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1144 - val_loss: 0.1224\nEpoch 2/10\ntrain_loss: 0.1154 - val_loss: 0.1224\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1144\nEpoch 1: val_loss improved from 0.12241 to 0.12236, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1144 - val_loss: 0.1224\nEpoch 3/10\ntrain_loss: 0.1153 - val_loss: 0.1224\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1143\nEpoch 1: val_loss improved from 0.12236 to 0.12230, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1143 - val_loss: 0.1223\nEpoch 4/10\ntrain_loss: 0.1152 - val_loss: 0.1223\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1142\nEpoch 1: val_loss improved from 0.12230 to 0.12225, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1142 - val_loss: 0.1222\nEpoch 5/10\ntrain_loss: 0.1151 - val_loss: 0.1222\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1141\nEpoch 1: val_loss improved from 0.12225 to 0.12220, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1141 - val_loss: 0.1222\nEpoch 6/10\ntrain_loss: 0.1150 - val_loss: 0.1222\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1140\nEpoch 1: val_loss improved from 0.12220 to 0.12214, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1140 - val_loss: 0.1221\nEpoch 7/10\ntrain_loss: 0.1149 - val_loss: 0.1221\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1139\nEpoch 1: val_loss improved from 0.12214 to 0.12209, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1139 - val_loss: 0.1221\nEpoch 8/10\ntrain_loss: 0.1149 - val_loss: 0.1221\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1138\nEpoch 1: val_loss improved from 0.12209 to 0.12204, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1138 - val_loss: 0.1220\nEpoch 9/10\ntrain_loss: 0.1148 - val_loss: 0.1220\n\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1137\nEpoch 1: val_loss improved from 0.12204 to 0.12199, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1137 - val_loss: 0.1220\nEpoch 10/10\ntrain_loss: 0.1147 - val_loss: 0.1220\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data = encode_data(tokenized_datasets['test'], np.reshape(y_test, (-1, 16)))\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T11:26:07.690481Z","iopub.execute_input":"2024-05-26T11:26:07.690876Z","iopub.status.idle":"2024-05-26T11:26:09.454992Z","shell.execute_reply.started":"2024-05-26T11:26:07.690835Z","shell.execute_reply":"2024-05-26T11:26:09.453779Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test dataset\ntest_loss= model.evaluate(test_dataset)\nprint(f\"Test Loss: {test_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:42:50.893744Z","iopub.execute_input":"2024-05-26T13:42:50.895086Z","iopub.status.idle":"2024-05-26T13:44:12.812338Z","shell.execute_reply.started":"2024-05-26T13:42:50.895044Z","shell.execute_reply":"2024-05-26T13:44:12.811229Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - loss: 0.1278\nTest Loss: 0.1264\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Reload the model","metadata":{}},{"cell_type":"code","source":"reloaded_model = create_model(optimizer)\nreloaded_model.load_weights('/kaggle/working/sentiment_model_checkpoint.weights.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:12.814295Z","iopub.execute_input":"2024-05-26T13:44:12.814624Z","iopub.status.idle":"2024-05-26T13:44:25.637384Z","shell.execute_reply.started":"2024-05-26T13:44:12.814593Z","shell.execute_reply":"2024-05-26T13:44:25.636113Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_type_ids      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_layer_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mBertLayer\u001b[0m)         │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n│                     │                   │            │ token_type_ids[\u001b[38;5;34m0\u001b[0m… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bert_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ FACILITY (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LECTURER (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ OTHERS (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ TRAINING_PROGRAM    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ FACILITY[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ LECTURER[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│                     │                   │            │ OTHERS[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n│                     │                   │            │ TRAINING_PROGRAM… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ token_type_ids      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ bert_layer_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                     │                   │            │ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ get_item_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ FACILITY (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ LECTURER (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ OTHERS (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ TRAINING_PROGRAM    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ FACILITY[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ LECTURER[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│                     │                   │            │ OTHERS[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n│                     │                   │            │ TRAINING_PROGRAM… │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,914\u001b[0m (144.20 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,914</span> (144.20 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m24,610\u001b[0m (96.14 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,610</span> (96.14 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(reloaded_model, to_file='architecture.png', rankdir='LR', dpi=52, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:25.638790Z","iopub.execute_input":"2024-05-26T13:44:25.639151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Tiến hành dự đoán\ndef predict_sentiment(text, loaded_model):\n    inputs = tokenizer(text, max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf')\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    token_type_ids = inputs['token_type_ids']\n    predictions = loaded_model.predict([input_ids, attention_mask, token_type_ids])\n    return predictions\n\ndef print_acsa_pred(predictions):\n    # Giải thích kết quả\n    topics = [\"FACILITY\", \"LECTURER\", \"OTHERS\", \"TRAINING_PROGRAMS\"]\n    sentiments = [\"None\", \"Positive\", \"Negative\", \"Neutral\"] \n    y_pred = predictions.reshape(len(predictions), -1, 4)\n    for i in range(4):\n        sentiment_index = np.argmax(y_pred[0][i])\n        topic = topics[i]\n        sentiment = sentiments[sentiment_index]\n        if sentiment != \"None\":\n            print(f\"{topic}: {sentiment}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def report(predictions):\n    sentiment_indices = []\n\n    # Reshape predictions to match the expected format\n    y_pred = predictions.reshape(len(predictions), -1, 4)\n\n    # Iterate over each category\n    for i in range(4):\n        # Get the sentiment index for the current category\n        sentiment_index = np.argmax(y_pred[0][i])\n\n        # Append the sentiment index to the list\n        sentiment_indices.append(sentiment_index)\n\n    # Return the list of sentiment indices for each category\n    return sentiment_indices\n","metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:25.748662Z","iopub.status.idle":"2024-05-26T13:44:25.756617Z","shell.execute_reply.started":"2024-05-26T13:44:25.748627Z","shell.execute_reply":"2024-05-26T13:44:25.755563Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# Ví dụ sử dụng\ntext = \"giảng viên rất nhiệt tình và dễ hiểu\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:25.757913Z","iopub.execute_input":"2024-05-26T13:44:25.758768Z","iopub.status.idle":"2024-05-26T13:44:29.264203Z","shell.execute_reply.started":"2024-05-26T13:44:25.758740Z","shell.execute_reply":"2024-05-26T13:44:29.262939Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"W0000 00:00:1716731067.690088     158 assert_op.cc:38] Ignoring Assert operator functional_5_1/bert_layer_2_1/tf_roberta_model_2/roberta/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n[[9.9983847e-01 2.8307688e-05 1.2431138e-04 8.7628086e-06 4.6013118e-04\n  9.7507465e-01 2.4146359e-02 3.1887632e-04 9.9894303e-01 6.7140500e-04\n  2.9901156e-04 8.6607928e-05 9.9920678e-01 3.2926607e-04 3.7895472e-04\n  8.5075684e-05]]\n[[9.9983847e-01 2.8307688e-05 1.2431138e-04 8.7628086e-06]\n [4.6013118e-04 9.7507465e-01 2.4146359e-02 3.1887632e-04]\n [9.9894303e-01 6.7140500e-04 2.9901156e-04 8.6607928e-05]\n [9.9920678e-01 3.2926607e-04 3.7895472e-04 8.5075684e-05]]\nLECTURER: Positive\n[0, 1, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ví dụ sử dụng\ntext = \"dạy chán, không quan tâm học sinh\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.265495Z","iopub.execute_input":"2024-05-26T13:44:29.267540Z","iopub.status.idle":"2024-05-26T13:44:29.382356Z","shell.execute_reply.started":"2024-05-26T13:44:29.267495Z","shell.execute_reply":"2024-05-26T13:44:29.381041Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n[[0.99337995 0.00157052 0.00389787 0.00115165 0.05584941 0.21189219\n  0.7064535  0.02580487 0.9671745  0.0112598  0.01326553 0.00830019\n  0.96330786 0.00729772 0.02645061 0.00294381]]\n[[0.99337995 0.00157052 0.00389787 0.00115165]\n [0.05584941 0.21189219 0.7064535  0.02580487]\n [0.9671745  0.0112598  0.01326553 0.00830019]\n [0.96330786 0.00729772 0.02645061 0.00294381]]\nLECTURER: Negative\n[0, 2, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ví dụ sử dụng\ntext = \"chương trình học khô khan, khó hiểu\"\npredictions = predict_sentiment(text, reloaded_model)\nprint_acsa_pred(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.383627Z","iopub.execute_input":"2024-05-26T13:44:29.383993Z","iopub.status.idle":"2024-05-26T13:44:29.488725Z","shell.execute_reply.started":"2024-05-26T13:44:29.383959Z","shell.execute_reply":"2024-05-26T13:44:29.487614Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ví dụ sử dụng\ntext = \"phòng học dơ bẩn, ẩm mốc, và cực kỳ tối\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.491052Z","iopub.execute_input":"2024-05-26T13:44:29.493225Z","iopub.status.idle":"2024-05-26T13:44:29.604059Z","shell.execute_reply.started":"2024-05-26T13:44:29.493194Z","shell.execute_reply":"2024-05-26T13:44:29.602730Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n[[0.09857271 0.01481084 0.88182676 0.00478971 0.6561089  0.13309951\n  0.19698283 0.01380874 0.94935995 0.01791392 0.02512837 0.00759772\n  0.91343355 0.04684234 0.03586247 0.00386167]]\n[[0.09857271 0.01481084 0.88182676 0.00478971]\n [0.6561089  0.13309951 0.19698283 0.01380874]\n [0.94935995 0.01791392 0.02512837 0.00759772]\n [0.91343355 0.04684234 0.03586247 0.00386167]]\nFACILITY: Negative\n[2, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Ví dụ sử dụng\ntext = \"sợ ma\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.609058Z","iopub.execute_input":"2024-05-26T13:44:29.609473Z","iopub.status.idle":"2024-05-26T13:44:29.722344Z","shell.execute_reply.started":"2024-05-26T13:44:29.609421Z","shell.execute_reply":"2024-05-26T13:44:29.721415Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n[[0.9393128  0.00159395 0.05603147 0.00306187 0.49649248 0.1425621\n  0.2945748  0.06637062 0.75500363 0.06792098 0.07609788 0.10097753\n  0.8580513  0.07133412 0.03397258 0.03664197]]\n[[0.9393128  0.00159395 0.05603147 0.00306187]\n [0.49649248 0.1425621  0.2945748  0.06637062]\n [0.75500363 0.06792098 0.07609788 0.10097753]\n [0.8580513  0.07133412 0.03397258 0.03664197]]\n[0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"y_test_argmax = np.argmax(y_test, axis=-1)\ny_test_argmax","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:45:23.951260Z","iopub.execute_input":"2024-05-26T13:45:23.951668Z","iopub.status.idle":"2024-05-26T13:45:23.962395Z","shell.execute_reply.started":"2024-05-26T13:45:23.951637Z","shell.execute_reply":"2024-05-26T13:45:23.961034Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"array([[0, 1, 0, 0],\n       [0, 1, 0, 0],\n       [0, 1, 0, 0],\n       ...,\n       [0, 1, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 0, 3]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Predict on test data","metadata":{}},{"cell_type":"code","source":"def predict(model, inputs, batch_size=1, verbose=0):\n    y_pred = model.predict(inputs, batch_size=batch_size, verbose=verbose)\n    y_pred = y_pred.reshape(len(y_pred), -1, 4)\n    return np.argmax(y_pred, axis=-1) # sentiment values (position that have max value)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:00:02.379323Z","iopub.execute_input":"2024-05-26T14:00:02.379732Z","iopub.status.idle":"2024-05-26T14:00:02.387422Z","shell.execute_reply.started":"2024-05-26T14:00:02.379697Z","shell.execute_reply":"2024-05-26T14:00:02.386193Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"def print_acsa_pred(replacements, categories, sentence_pred):\n    sentiments = map(lambda x: replacements[x], sentence_pred)\n    for category, sentiment in zip(categories, sentiments): \n        if sentiment: print(f'=> {category},{sentiment}')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:00:05.647478Z","iopub.execute_input":"2024-05-26T14:00:05.648171Z","iopub.status.idle":"2024-05-26T14:00:05.654949Z","shell.execute_reply.started":"2024-05-26T14:00:05.648135Z","shell.execute_reply":"2024-05-26T14:00:05.653694Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"y_pred = predict(reloaded_model, test_dataset, BATCH_SIZE, verbose=1)\nreloaded_model.evaluate(test_dataset, batch_size=BATCH_SIZE, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:01:54.844285Z","iopub.execute_input":"2024-05-26T15:01:54.844735Z","iopub.status.idle":"2024-05-26T15:03:18.271021Z","shell.execute_reply.started":"2024-05-26T15:01:54.844701Z","shell.execute_reply":"2024-05-26T15:03:18.269964Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 212ms/step\n\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - loss: 0.1278\n","output_type":"stream"},{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"0.12644782662391663"},"metadata":{}}]},{"cell_type":"code","source":"y_test[1]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:03:18.273033Z","iopub.execute_input":"2024-05-26T15:03:18.273345Z","iopub.status.idle":"2024-05-26T15:03:18.281965Z","shell.execute_reply.started":"2024-05-26T15:03:18.273318Z","shell.execute_reply":"2024-05-26T15:03:18.280445Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"y_test_argmax[1]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:04:22.706657Z","iopub.execute_input":"2024-05-26T15:04:22.707506Z","iopub.status.idle":"2024-05-26T15:04:22.715583Z","shell.execute_reply.started":"2024-05-26T15:04:22.707464Z","shell.execute_reply":"2024-05-26T15:04:22.714320Z"},"trusted":true},"execution_count":125,"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 0, 0])"},"metadata":{}}]},{"cell_type":"code","source":"y_pred[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:03:18.283509Z","iopub.execute_input":"2024-05-26T15:03:18.283968Z","iopub.status.idle":"2024-05-26T15:03:18.294356Z","shell.execute_reply.started":"2024-05-26T15:03:18.283924Z","shell.execute_reply":"2024-05-26T15:03:18.293288Z"},"trusted":true},"execution_count":124,"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 0, 0])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replacements = {0: None, 1: 'positive', 2: 'negative', 3: 'neutral'}\ncategories = test_df.columns[1:]\nfor i in range(50):\n    print('Example:', test_df['Sentence'][i])\n    print_acsa_pred(replacements, categories, y_pred[i])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:03:40.065202Z","iopub.execute_input":"2024-05-26T14:03:40.065592Z","iopub.status.idle":"2024-05-26T14:03:40.098122Z","shell.execute_reply.started":"2024-05-26T14:03:40.065561Z","shell.execute_reply":"2024-05-26T14:03:40.096927Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"Example: nói tiếng anh lưu loát \nExample: giáo viên rất vui tính \n=> LECTURER,positive\nExample: cô max có tâm \n=> LECTURER,positive\nExample: giảng bài thu hút  dí dỏm \n=> LECTURER,negative\nExample: giáo viên không giảng dạy kiến thức  hướng dẫn thực hành trong quá trình học \n=> LECTURER,negative\nExample: thầy dạy nhiệt tình và tâm huyết \n=> LECTURER,positive\nExample: tính điểm thi đua các nhóm \n=> TRAINING_PROGRAM,negative\nExample: thầy nhiệt tình giảng lại cho học sinh \n=> LECTURER,positive\nExample: có đôi lúc nói hơi nhanh làm sinh viên không theo kịp \n=> LECTURER,negative\nExample: giảng dạy nhiệt tình  liên hệ thực tế khá nhiều  tương tác với sinh viên tương đối tốt \n=> LECTURER,positive\nExample: giảng viên nhiệt tình trong công tác giảng dạy \n=> LECTURER,positive\nExample: cô rất nhiệt tình  dễ thương  dạy dễ hiểu \n=> LECTURER,positive\nExample: trong trường macbook thầy số hai thì không có máy nào số một \n=> LECTURER,negative\nExample: sinh viên không tiếp thu kịp cũng như không hiểu gì \n=> LECTURER,negative\nExample: thầy nhiệt tình giúp đỡ sinh viên trong quá trình thực hành \n=> LECTURER,positive\nExample: thầy rất tận tình và giúp đỡ sinh viên rất nhiều \n=> LECTURER,positive\nExample: giảng viên giải thích kỹ và chi tiết \n=> LECTURER,positive\nExample: thầy dạy rất chi tiết  lý thuyết đầy đủ \n=> LECTURER,positive\nExample: còn những phần tìm bao đóng  chứng minh dạng chuẩn chưa làm rõ \n=> TRAINING_PROGRAM,negative\nExample: cung cấp kiến thức bổ ích \n=> LECTURER,positive\nExample: bắt đầu buổi học đúng giờ \nExample: giảng dạy dễ hiểu  tận tâm \n=> LECTURER,positive\nExample: giảng viên nhiệt tình  giải đáp thắc mắc đầy đủ \n=> LECTURER,positive\nExample: nên đưa ra một vài phương pháp học lập trình hay cho sinh viên \n=> TRAINING_PROGRAM,negative\nExample: giảng viên dạy sôi nổi \n=> LECTURER,positive\nExample: em học bên chất lượng cao mà phòng máy không cung cấp đủ máy  vì máy hư hoặc không cài chương trình  \n=> FACILITY,negative\nExample: chưa giỏi chuyên môn cho lắm \nExample: cách mà cô tiếp cận với sinh viên \n=> LECTURER,positive\nExample: trợ giàng nhiệt tình  quan tâm và theo sát sinh viên \n=> LECTURER,positive\nExample: giảng viên tận tâm  gần gũi với học sinh \n=> LECTURER,positive\nExample: thầy dạy rất nhiệt tình và giảng kỹ bài không cần cải thiện gì cả \n=> LECTURER,positive\nExample: phòng học thoáng mát  trang thiết bị đầy đủ \n=> FACILITY,negative\nExample: thầy rất nhiệt tình thân thiện và vui tính cách giảng dạy rất dễ hiểu \n=> LECTURER,positive\nExample: không nhiệt tình chỉ dẫn và luôn gây khó khăn cho sinh viên \n=> LECTURER,negative\nExample: giữa lý thuyết từ vựng với trò chơi để dễ tiếp thu \nExample: môn học này giúp chúng em hiểu ra những vấn đề cơ bản \n=> TRAINING_PROGRAM,negative\nExample: giáo trình chưa có hợp lý \nExample: mong thầy xem xét lại việc nộp bài \n=> LECTURER,negative\nExample: cô wzjwz234  dạy rất nhiệt tính  tận tâm tận lực  giải thích từng câu code giúp học sinh hiểu rõ vấn đề  rất thích cô dạy môn hướng đối tượng này \n=> LECTURER,positive\nExample: cung cấp bài tập đa dạng \nExample: hay gõ micro vào bảng hoặc bàn \nExample: ổn \nExample: tốc độ dạy của giảng viên nhanh  nên có một số nội dung không cung cấp đầy đủ kiến thức \n=> LECTURER,negative\nExample: cô vui tính  học không áp lực \n=> LECTURER,positive\nExample: cô dạy rất tốt và nhiệt tình \n=> LECTURER,positive\nExample: phần lớn chỉ là lý thuyết và bài tập \n=> TRAINING_PROGRAM,negative\nExample: thầy nhiệt tình trả lời thắc mắc của sinh viên \n=> LECTURER,positive\nExample: thiết bị phòng học như quạt máy cần phải đầu tư thêm \n=> FACILITY,negative\nExample: cô nhiệt tình  giảng bài hiệu quả \n=> LECTURER,positive\nExample: khi nghỉ học  cần thông báo kịp thời trên web của khoa wzjwz148 \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Report","metadata":{}},{"cell_type":"code","source":"aspect_test = []\naspect_pred = []\n\nfor row_test, row_pred in zip(y_test_argmax, y_pred):\n    for index, (col_test, col_pred) in enumerate(zip(row_test, row_pred)):\n        aspect_test.append(bool(col_test) * categories[index])\n        aspect_pred.append(bool(col_pred) * categories[index])\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:04:44.900404Z","iopub.execute_input":"2024-05-26T14:04:44.901375Z","iopub.status.idle":"2024-05-26T14:04:44.963981Z","shell.execute_reply.started":"2024-05-26T14:04:44.901338Z","shell.execute_reply":"2024-05-26T14:04:44.962858Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\naspect_report = classification_report(aspect_test, aspect_pred, digits=4, zero_division=1, output_dict=True)\nprint(classification_report(aspect_test, aspect_pred, digits=4, zero_division=1))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:04:58.503417Z","iopub.execute_input":"2024-05-26T14:04:58.504649Z","iopub.status.idle":"2024-05-26T14:04:58.995939Z","shell.execute_reply.started":"2024-05-26T14:04:58.504603Z","shell.execute_reply":"2024-05-26T14:04:58.994869Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"                  precision    recall  f1-score   support\n\n                     0.9275    0.9707    0.9486      9498\n        FACILITY     0.9619    0.6966    0.8080       145\n        LECTURER     0.9260    0.8856    0.9054      2290\n          OTHERS     0.6800    0.1069    0.1848       159\nTRAINING_PROGRAM     0.7419    0.5227    0.6133       572\n\n        accuracy                         0.9211     12664\n       macro avg     0.8475    0.6365    0.6920     12664\n    weighted avg     0.9161    0.9211    0.9144     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test_flat = y_test_argmax.flatten()\ny_pred_flat = y_pred.flatten()\ntarget_names = list(map(str, replacements.values()))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:05:30.522266Z","iopub.execute_input":"2024-05-26T14:05:30.522713Z","iopub.status.idle":"2024-05-26T14:05:30.530159Z","shell.execute_reply.started":"2024-05-26T14:05:30.522679Z","shell.execute_reply":"2024-05-26T14:05:30.528741Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"polarity_report = classification_report(y_test_flat, y_pred_flat, digits=4, output_dict=True)\nprint(classification_report(y_test_flat, y_pred_flat, target_names=target_names, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:05:32.784389Z","iopub.execute_input":"2024-05-26T14:05:32.785082Z","iopub.status.idle":"2024-05-26T14:05:32.859834Z","shell.execute_reply.started":"2024-05-26T14:05:32.785045Z","shell.execute_reply":"2024-05-26T14:05:32.858606Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        None     0.9275    0.9707    0.9486      9498\n    positive     0.8517    0.8088    0.8297      1590\n    negative     0.7481    0.6366    0.6879      1409\n     neutral     0.5714    0.0479    0.0884       167\n\n    accuracy                         0.9011     12664\n   macro avg     0.7747    0.6160    0.6386     12664\nweighted avg     0.8933    0.9011    0.8933     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"aspect_polarity_test = []\naspect_polarity_pred = []\n\nfor row_test, row_pred in zip(y_test_argmax, y_pred):\n    for index, (col_test, col_pred) in enumerate(zip(row_test, row_pred)):\n        aspect_polarity_test.append(f'{categories[index]},{replacements[col_test]}')\n        aspect_polarity_pred.append(f'{categories[index]},{replacements[col_pred]}')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:01.963074Z","iopub.execute_input":"2024-05-26T14:06:01.963962Z","iopub.status.idle":"2024-05-26T14:06:02.040019Z","shell.execute_reply.started":"2024-05-26T14:06:01.963921Z","shell.execute_reply":"2024-05-26T14:06:02.038689Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"aspect_polarity_report = classification_report(aspect_polarity_test, aspect_polarity_pred, digits=4, zero_division=1, output_dict=True)\nprint(classification_report(aspect_polarity_test, aspect_polarity_pred, digits=4, zero_division=1))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:04.445490Z","iopub.execute_input":"2024-05-26T14:06:04.446055Z","iopub.status.idle":"2024-05-26T14:06:04.990759Z","shell.execute_reply.started":"2024-05-26T14:06:04.446009Z","shell.execute_reply":"2024-05-26T14:06:04.989592Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"                           precision    recall  f1-score   support\n\n            FACILITY,None     0.9856    0.9987    0.9921      3021\n        FACILITY,negative     0.9429    0.7174    0.8148       138\n         FACILITY,neutral     1.0000    0.0000    0.0000         2\n        FACILITY,positive     1.0000    0.0000    0.0000         5\n            LECTURER,None     0.7316    0.8151    0.7711       876\n        LECTURER,negative     0.7636    0.6941    0.7272       791\n         LECTURER,neutral     1.0000    0.0135    0.0267        74\n        LECTURER,positive     0.8619    0.8891    0.8753      1425\n              OTHERS,None     0.9548    0.9973    0.9756      3007\n          OTHERS,negative     0.4000    0.0312    0.0580        64\n           OTHERS,neutral     0.5385    0.1591    0.2456        44\n          OTHERS,positive     0.1429    0.0196    0.0345        51\n    TRAINING_PROGRAM,None     0.9012    0.9599    0.9296      2594\nTRAINING_PROGRAM,negative     0.6676    0.5938    0.6285       416\n TRAINING_PROGRAM,neutral     1.0000    0.0000    0.0000        47\nTRAINING_PROGRAM,positive     0.5455    0.1651    0.2535       109\n\n                 accuracy                         0.9011     12664\n                macro avg     0.7772    0.4409    0.4583     12664\n             weighted avg     0.8932    0.9011    0.8891     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"aspect_dict = aspect_report['macro avg']\naspect_dict['accuracy'] = aspect_report['accuracy']\n\npolarity_dict  = polarity_report['macro avg']\npolarity_dict['accuracy'] = polarity_report['accuracy']\n\naspect_polarity_dict = aspect_polarity_report['macro avg']\naspect_polarity_dict['accuracy'] = aspect_polarity_report['accuracy']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:20.218230Z","iopub.execute_input":"2024-05-26T14:06:20.219107Z","iopub.status.idle":"2024-05-26T14:06:20.227089Z","shell.execute_reply.started":"2024-05-26T14:06:20.219059Z","shell.execute_reply":"2024-05-26T14:06:20.225873Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"df_report = pd.DataFrame.from_dict([aspect_dict, polarity_dict, aspect_polarity_dict])\ndf_report.index = ['Aspect Detection', 'Polarity Detection', 'Aspect + Polarity']\ndf_report.drop('support', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:30.426845Z","iopub.execute_input":"2024-05-26T14:06:30.427238Z","iopub.status.idle":"2024-05-26T14:06:30.453822Z","shell.execute_reply.started":"2024-05-26T14:06:30.427206Z","shell.execute_reply":"2024-05-26T14:06:30.452868Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"                    precision    recall  f1-score  accuracy\nAspect Detection     0.847468  0.636503  0.692016  0.921115\nPolarity Detection   0.774670  0.616015  0.638642  0.901058\nAspect + Polarity    0.777240  0.440870  0.458276  0.901058","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1-score</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Aspect Detection</th>\n      <td>0.847468</td>\n      <td>0.636503</td>\n      <td>0.692016</td>\n      <td>0.921115</td>\n    </tr>\n    <tr>\n      <th>Polarity Detection</th>\n      <td>0.774670</td>\n      <td>0.616015</td>\n      <td>0.638642</td>\n      <td>0.901058</td>\n    </tr>\n    <tr>\n      <th>Aspect + Polarity</th>\n      <td>0.777240</td>\n      <td>0.440870</td>\n      <td>0.458276</td>\n      <td>0.901058</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"example_input = text_preprocess(input('Enter your sentence: '))\ntokenized_input = tokenizer(example_input, padding='max_length', truncation=True)\nfeatures = {x: [[tokenized_input[x]]] for x in tokenizer.model_input_names}\n\npred = predict(reloaded_model, tf.data.Dataset.from_tensor_slices(features))\nprint_acsa_pred(replacements, categories, pred[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:57:46.020371Z","iopub.execute_input":"2024-05-26T14:57:46.020827Z","iopub.status.idle":"2024-05-26T14:57:52.266145Z","shell.execute_reply.started":"2024-05-26T14:57:46.020792Z","shell.execute_reply":"2024-05-26T14:57:52.265043Z"},"trusted":true},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your sentence:  sợ ma quá\n"}]}]}