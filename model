{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8478679,"sourceType":"datasetVersion","datasetId":5056692},{"sourceId":8498142,"sourceType":"datasetVersion","datasetId":5071077}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"import warnings\n\n# Ignore all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.308062Z","iopub.execute_input":"2024-05-26T10:53:12.309139Z","iopub.status.idle":"2024-05-26T10:53:12.313956Z","shell.execute_reply.started":"2024-05-26T10:53:12.309091Z","shell.execute_reply":"2024-05-26T10:53:12.312955Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Define the file paths\ntopics_file ='/kaggle/input/uit-vsfc/train/topics.txt'\nsentiments_file ='/kaggle/input/uit-vsfc/train/sentiments.txt'\nsents_file ='/kaggle/input/uit-vsfc/train/sents.txt'\n\n# Read the files\ntopics = pd.read_csv(topics_file, header=None, names=['Topic'])\nsentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n\n# Read the sentences file line by line\nwith open(sents_file,'r', encoding='utf-8') as file:\n    sentences = file.readlines()\n\n# Remove newline characters and punctuation from sentences, and split by whitespace\nsentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n\n# Create a DataFrame for sentences\nsentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n\n# Combine the DataFrames along the columns\ncombined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n\n# Display the first few rows of the combined DataFrame\nprint(combined_df.head())\n\n# Replace values in'Topic'column\ntopic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\ncombined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n\n# Replace values in'Sentiment'column\nsentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\ncombined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n\n# Display the modified DataFrame\nprint(combined_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.353100Z","iopub.execute_input":"2024-05-26T10:53:12.353415Z","iopub.status.idle":"2024-05-26T10:53:12.483453Z","shell.execute_reply.started":"2024-05-26T10:53:12.353389Z","shell.execute_reply":"2024-05-26T10:53:12.482435Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"   Topic  Sentiment                                           Sentence\n0      1          2                           slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß \n1      0          2       nhi·ªát t√¨nh gi·∫£ng d·∫°y  g·∫ßn g≈©i v·ªõi sinh vi√™n \n2      1          0                ƒëi h·ªçc ƒë·∫ßy ƒë·ªß full ƒëi·ªÉm chuy√™n c·∫ßn \n3      0          0  ch∆∞a √°p d·ª•ng c√¥ng ngh·ªá th√¥ng tin v√† c√°c thi·∫øt ...\n4      0          2  th·∫ßy gi·∫£ng b√†i hay  c√≥ nhi·ªÅu b√†i t·∫≠p v√≠ d·ª• nga...\n              Topic Sentiment  \\\n0  TRAINING_PROGRAM  POSITIVE   \n1          LECTURER  POSITIVE   \n2  TRAINING_PROGRAM  NEGATIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  POSITIVE   \n\n                                            Sentence  \n0                           slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß   \n1       nhi·ªát t√¨nh gi·∫£ng d·∫°y  g·∫ßn g≈©i v·ªõi sinh vi√™n   \n2                ƒëi h·ªçc ƒë·∫ßy ƒë·ªß full ƒëi·ªÉm chuy√™n c·∫ßn   \n3  ch∆∞a √°p d·ª•ng c√¥ng ngh·ªá th√¥ng tin v√† c√°c thi·∫øt ...  \n4  th·∫ßy gi·∫£ng b√†i hay  c√≥ nhi·ªÅu b√†i t·∫≠p v√≠ d·ª• nga...  \n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.485375Z","iopub.execute_input":"2024-05-26T10:53:12.486212Z","iopub.status.idle":"2024-05-26T10:53:12.503773Z","shell.execute_reply.started":"2024-05-26T10:53:12.486176Z","shell.execute_reply":"2024-05-26T10:53:12.502858Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                  Topic Sentiment  \\\n0      TRAINING_PROGRAM  POSITIVE   \n1              LECTURER  POSITIVE   \n2      TRAINING_PROGRAM  NEGATIVE   \n3              LECTURER  NEGATIVE   \n4              LECTURER  POSITIVE   \n...                 ...       ...   \n11421  TRAINING_PROGRAM  NEGATIVE   \n11422          LECTURER  POSITIVE   \n11423          LECTURER  NEGATIVE   \n11424          LECTURER  POSITIVE   \n11425          LECTURER  POSITIVE   \n\n                                                Sentence  \n0                               slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß   \n1           nhi·ªát t√¨nh gi·∫£ng d·∫°y  g·∫ßn g≈©i v·ªõi sinh vi√™n   \n2                    ƒëi h·ªçc ƒë·∫ßy ƒë·ªß full ƒëi·ªÉm chuy√™n c·∫ßn   \n3      ch∆∞a √°p d·ª•ng c√¥ng ngh·ªá th√¥ng tin v√† c√°c thi·∫øt ...  \n4      th·∫ßy gi·∫£ng b√†i hay  c√≥ nhi·ªÅu b√†i t·∫≠p v√≠ d·ª• nga...  \n...                                                  ...  \n11421  ch·ªâ v√¨ m√¥n game m√† em h·ªçc hai l·∫ßn m√† kh√¥ng qua...  \n11422                                em c·∫£m ∆°n c√¥ nhi·ªÅu   \n11423                            giao b√†i t·∫≠p qu√° nhi·ªÅu   \n11424                 gi√°o vi√™n d·∫°y d·ªÖ hi·ªÉu  nhi·ªát t√¨nh   \n11425  g√≥i g·ªçn doubledot hay  t·∫≠n t√¨nh  ph√π h·ª£p v·ªõi m...  \n\n[11426 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>POSITIVE</td>\n      <td>slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>nhi·ªát t√¨nh gi·∫£ng d·∫°y  g·∫ßn g≈©i v·ªõi sinh vi√™n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>ƒëi h·ªçc ƒë·∫ßy ƒë·ªß full ƒëi·ªÉm chuy√™n c·∫ßn</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>ch∆∞a √°p d·ª•ng c√¥ng ngh·ªá th√¥ng tin v√† c√°c thi·∫øt ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>th·∫ßy gi·∫£ng b√†i hay  c√≥ nhi·ªÅu b√†i t·∫≠p v√≠ d·ª• nga...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11421</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>ch·ªâ v√¨ m√¥n game m√† em h·ªçc hai l·∫ßn m√† kh√¥ng qua...</td>\n    </tr>\n    <tr>\n      <th>11422</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>em c·∫£m ∆°n c√¥ nhi·ªÅu</td>\n    </tr>\n    <tr>\n      <th>11423</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>giao b√†i t·∫≠p qu√° nhi·ªÅu</td>\n    </tr>\n    <tr>\n      <th>11424</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi√°o vi√™n d·∫°y d·ªÖ hi·ªÉu  nhi·ªát t√¨nh</td>\n    </tr>\n    <tr>\n      <th>11425</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>g√≥i g·ªçn doubledot hay  t·∫≠n t√¨nh  ph√π h·ª£p v·ªõi m...</td>\n    </tr>\n  </tbody>\n</table>\n<p>11426 rows √ó 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.to_csv('train.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.526280Z","iopub.execute_input":"2024-05-26T10:53:12.526585Z","iopub.status.idle":"2024-05-26T10:53:12.594208Z","shell.execute_reply.started":"2024-05-26T10:53:12.526561Z","shell.execute_reply":"2024-05-26T10:53:12.593361Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.600006Z","iopub.execute_input":"2024-05-26T10:53:12.600350Z","iopub.status.idle":"2024-05-26T10:53:12.611420Z","shell.execute_reply.started":"2024-05-26T10:53:12.600322Z","shell.execute_reply":"2024-05-26T10:53:12.610346Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"              Topic Sentiment  \\\n0  TRAINING_PROGRAM  POSITIVE   \n1          LECTURER  POSITIVE   \n2  TRAINING_PROGRAM  NEGATIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  POSITIVE   \n\n                                            Sentence  \n0                           slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß   \n1       nhi·ªát t√¨nh gi·∫£ng d·∫°y  g·∫ßn g≈©i v·ªõi sinh vi√™n   \n2                ƒëi h·ªçc ƒë·∫ßy ƒë·ªß full ƒëi·ªÉm chuy√™n c·∫ßn   \n3  ch∆∞a √°p d·ª•ng c√¥ng ngh·ªá th√¥ng tin v√† c√°c thi·∫øt ...  \n4  th·∫ßy gi·∫£ng b√†i hay  c√≥ nhi·ªÅu b√†i t·∫≠p v√≠ d·ª• nga...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>POSITIVE</td>\n      <td>slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>nhi·ªát t√¨nh gi·∫£ng d·∫°y  g·∫ßn g≈©i v·ªõi sinh vi√™n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>ƒëi h·ªçc ƒë·∫ßy ƒë·ªß full ƒëi·ªÉm chuy√™n c·∫ßn</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>ch∆∞a √°p d·ª•ng c√¥ng ngh·ªá th√¥ng tin v√† c√°c thi·∫øt ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>th·∫ßy gi·∫£ng b√†i hay  c√≥ nhi·ªÅu b√†i t·∫≠p v√≠ d·ª• nga...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Valid","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Define the file paths\ntopics_file ='/kaggle/input/uit-vsfc/dev/topics.txt'\nsentiments_file ='/kaggle/input/uit-vsfc/dev/sentiments.txt'\nsents_file ='/kaggle/input/uit-vsfc/dev/sents.txt'\n\n# Read the files\ntopics = pd.read_csv(topics_file, header=None, names=['Topic'])\nsentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n\n# Read the sentences file line by line\nwith open(sents_file,'r', encoding='utf-8') as file:\n    sentences = file.readlines()\n\n# Remove newline characters and punctuation from sentences, and split by whitespace\nsentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n\n# Create a DataFrame for sentences\nsentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n\n# Combine the DataFrames along the columns\ncombined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n\n# Display the first few rows of the combined DataFrame\nprint(combined_df.head())\n\n# Replace values in'Topic'column\ntopic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\ncombined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n\n# Replace values in'Sentiment'column\nsentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\ncombined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n\n# Display the modified DataFrame\nprint(combined_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.671764Z","iopub.execute_input":"2024-05-26T10:53:12.672155Z","iopub.status.idle":"2024-05-26T10:53:12.726928Z","shell.execute_reply.started":"2024-05-26T10:53:12.672125Z","shell.execute_reply":"2024-05-26T10:53:12.725752Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"   Topic  Sentiment                                           Sentence\n0      1          0                            gi√°o tr√¨nh ch∆∞a c·ª• th·ªÉ \n1      0          0                                    gi·∫£ng bu·ªìn ng·ªß \n2      0          2                       gi√°o vi√™n vui t√≠nh  t·∫≠n t√¢m \n3      0          0  gi·∫£ng vi√™n n√™n giao b√†i t·∫≠p nhi·ªÅu h∆°n  chia nh...\n4      0          0  gi·∫£ng vi√™n c·∫ßn gi·∫£ng b√†i chi ti·∫øt h∆°n  ƒëi s√¢u ...\n              Topic Sentiment  \\\n0  TRAINING_PROGRAM  NEGATIVE   \n1          LECTURER  NEGATIVE   \n2          LECTURER  POSITIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  NEGATIVE   \n\n                                            Sentence  \n0                            gi√°o tr√¨nh ch∆∞a c·ª• th·ªÉ   \n1                                    gi·∫£ng bu·ªìn ng·ªß   \n2                       gi√°o vi√™n vui t√≠nh  t·∫≠n t√¢m   \n3  gi·∫£ng vi√™n n√™n giao b√†i t·∫≠p nhi·ªÅu h∆°n  chia nh...  \n4  gi·∫£ng vi√™n c·∫ßn gi·∫£ng b√†i chi ti·∫øt h∆°n  ƒëi s√¢u ...  \n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.731851Z","iopub.execute_input":"2024-05-26T10:53:12.732233Z","iopub.status.idle":"2024-05-26T10:53:12.745549Z","shell.execute_reply.started":"2024-05-26T10:53:12.732203Z","shell.execute_reply":"2024-05-26T10:53:12.744535Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 Topic Sentiment  \\\n0     TRAINING_PROGRAM  NEGATIVE   \n1             LECTURER  NEGATIVE   \n2             LECTURER  POSITIVE   \n3             LECTURER  NEGATIVE   \n4             LECTURER  NEGATIVE   \n...                ...       ...   \n1578          LECTURER  NEGATIVE   \n1579          LECTURER  POSITIVE   \n1580          LECTURER  NEGATIVE   \n1581  TRAINING_PROGRAM  NEGATIVE   \n1582          LECTURER  NEGATIVE   \n\n                                               Sentence  \n0                               gi√°o tr√¨nh ch∆∞a c·ª• th·ªÉ   \n1                                       gi·∫£ng bu·ªìn ng·ªß   \n2                          gi√°o vi√™n vui t√≠nh  t·∫≠n t√¢m   \n3     gi·∫£ng vi√™n n√™n giao b√†i t·∫≠p nhi·ªÅu h∆°n  chia nh...  \n4     gi·∫£ng vi√™n c·∫ßn gi·∫£ng b√†i chi ti·∫øt h∆°n  ƒëi s√¢u ...  \n...                                                 ...  \n1578                               h∆∞·ªõng d·∫´n lab m∆° h·ªì   \n1579  th·∫ßy cho ch√∫ng em nh·ªØng b√†i t·∫≠p mang t√≠nh th·ª±c...  \n1580  th·∫ßy kh√¥ng d·∫°y nhi·ªÅu ch·ªß y·∫øu cho sinh vi√™n t·ª± ...  \n1581  em mu·ªën ƒë·ªïi t√™n m√¥n h·ªçc v√¨ t√™n m√¥n l√† l·∫≠p tr√¨n...  \n1582  th·∫ßy v·ª´a d·∫°y v·ª´a chat ho·∫∑c g·ªçi ƒëi·ªán tho·∫°i th∆∞·ªù...  \n\n[1583 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>gi√°o tr√¨nh ch∆∞a c·ª• th·ªÉ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi·∫£ng bu·ªìn ng·ªß</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi√°o vi√™n vui t√≠nh  t·∫≠n t√¢m</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi·∫£ng vi√™n n√™n giao b√†i t·∫≠p nhi·ªÅu h∆°n  chia nh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi·∫£ng vi√™n c·∫ßn gi·∫£ng b√†i chi ti·∫øt h∆°n  ƒëi s√¢u ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1578</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>h∆∞·ªõng d·∫´n lab m∆° h·ªì</td>\n    </tr>\n    <tr>\n      <th>1579</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>th·∫ßy cho ch√∫ng em nh·ªØng b√†i t·∫≠p mang t√≠nh th·ª±c...</td>\n    </tr>\n    <tr>\n      <th>1580</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>th·∫ßy kh√¥ng d·∫°y nhi·ªÅu ch·ªß y·∫øu cho sinh vi√™n t·ª± ...</td>\n    </tr>\n    <tr>\n      <th>1581</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>em mu·ªën ƒë·ªïi t√™n m√¥n h·ªçc v√¨ t√™n m√¥n l√† l·∫≠p tr√¨n...</td>\n    </tr>\n    <tr>\n      <th>1582</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>th·∫ßy v·ª´a d·∫°y v·ª´a chat ho·∫∑c g·ªçi ƒëi·ªán tho·∫°i th∆∞·ªù...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1583 rows √ó 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"combined_df.to_csv('dev.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.799254Z","iopub.execute_input":"2024-05-26T10:53:12.799562Z","iopub.status.idle":"2024-05-26T10:53:12.813095Z","shell.execute_reply.started":"2024-05-26T10:53:12.799536Z","shell.execute_reply":"2024-05-26T10:53:12.812075Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.866507Z","iopub.execute_input":"2024-05-26T10:53:12.866810Z","iopub.status.idle":"2024-05-26T10:53:12.876620Z","shell.execute_reply.started":"2024-05-26T10:53:12.866786Z","shell.execute_reply":"2024-05-26T10:53:12.875649Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"              Topic Sentiment  \\\n0  TRAINING_PROGRAM  NEGATIVE   \n1          LECTURER  NEGATIVE   \n2          LECTURER  POSITIVE   \n3          LECTURER  NEGATIVE   \n4          LECTURER  NEGATIVE   \n\n                                            Sentence  \n0                            gi√°o tr√¨nh ch∆∞a c·ª• th·ªÉ   \n1                                    gi·∫£ng bu·ªìn ng·ªß   \n2                       gi√°o vi√™n vui t√≠nh  t·∫≠n t√¢m   \n3  gi·∫£ng vi√™n n√™n giao b√†i t·∫≠p nhi·ªÅu h∆°n  chia nh...  \n4  gi·∫£ng vi√™n c·∫ßn gi·∫£ng b√†i chi ti·∫øt h∆°n  ƒëi s√¢u ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEGATIVE</td>\n      <td>gi√°o tr√¨nh ch∆∞a c·ª• th·ªÉ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi·∫£ng bu·ªìn ng·ªß</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi√°o vi√™n vui t√≠nh  t·∫≠n t√¢m</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi·∫£ng vi√™n n√™n giao b√†i t·∫≠p nhi·ªÅu h∆°n  chia nh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi·∫£ng vi√™n c·∫ßn gi·∫£ng b√†i chi ti·∫øt h∆°n  ƒëi s√¢u ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n# Define the file paths\ntopics_file ='/kaggle/input/uit-vsfc/test/topics.txt'\nsentiments_file ='/kaggle/input/uit-vsfc/test/sentiments.txt'\nsents_file ='/kaggle/input/uit-vsfc/test/sents.txt'\n\n# Read the files\ntopics = pd.read_csv(topics_file, header=None, names=['Topic'])\nsentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n\n# Read the sentences file line by line\nwith open(sents_file,'r', encoding='utf-8') as file:\n    sentences = file.readlines()\n\n# Remove newline characters and punctuation from sentences, and split by whitespace\nsentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n\n# Create a DataFrame for sentences\nsentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n\n# Combine the DataFrames along the columns\ncombined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n\n# Display the first few rows of the combined DataFrame\nprint(combined_df.head())\n\n# Replace values in'Topic'column\ntopic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\ncombined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n\n# Replace values in'Sentiment'column\nsentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\ncombined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n\n# Display the modified DataFrame\nprint(combined_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.933318Z","iopub.execute_input":"2024-05-26T10:53:12.933645Z","iopub.status.idle":"2024-05-26T10:53:12.997497Z","shell.execute_reply.started":"2024-05-26T10:53:12.933618Z","shell.execute_reply":"2024-05-26T10:53:12.996363Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"   Topic  Sentiment                                           Sentence\n0      0          2                            n√≥i ti·∫øng anh l∆∞u lo√°t \n1      0          2                            gi√°o vi√™n r·∫•t vui t√≠nh \n2      0          2                                     c√¥ max c√≥ t√¢m \n3      0          2                         gi·∫£ng b√†i thu h√∫t  d√≠ d·ªèm \n4      0          0  gi√°o vi√™n kh√¥ng gi·∫£ng d·∫°y ki·∫øn th·ª©c  h∆∞·ªõng d·∫´n...\n      Topic Sentiment                                           Sentence\n0  LECTURER  POSITIVE                            n√≥i ti·∫øng anh l∆∞u lo√°t \n1  LECTURER  POSITIVE                            gi√°o vi√™n r·∫•t vui t√≠nh \n2  LECTURER  POSITIVE                                     c√¥ max c√≥ t√¢m \n3  LECTURER  POSITIVE                         gi·∫£ng b√†i thu h√∫t  d√≠ d·ªèm \n4  LECTURER  NEGATIVE  gi√°o vi√™n kh√¥ng gi·∫£ng d·∫°y ki·∫øn th·ª©c  h∆∞·ªõng d·∫´n...\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:12.999115Z","iopub.execute_input":"2024-05-26T10:53:12.999452Z","iopub.status.idle":"2024-05-26T10:53:13.012341Z","shell.execute_reply.started":"2024-05-26T10:53:12.999424Z","shell.execute_reply":"2024-05-26T10:53:13.011173Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                 Topic Sentiment  \\\n0             LECTURER  POSITIVE   \n1             LECTURER  POSITIVE   \n2             LECTURER  POSITIVE   \n3             LECTURER  POSITIVE   \n4             LECTURER  NEGATIVE   \n...                ...       ...   \n3161          LECTURER  NEGATIVE   \n3162          LECTURER  POSITIVE   \n3163          LECTURER  POSITIVE   \n3164          LECTURER  NEGATIVE   \n3165  TRAINING_PROGRAM   NEUTRAL   \n\n                                               Sentence  \n0                               n√≥i ti·∫øng anh l∆∞u lo√°t   \n1                               gi√°o vi√™n r·∫•t vui t√≠nh   \n2                                        c√¥ max c√≥ t√¢m   \n3                            gi·∫£ng b√†i thu h√∫t  d√≠ d·ªèm   \n4     gi√°o vi√™n kh√¥ng gi·∫£ng d·∫°y ki·∫øn th·ª©c  h∆∞·ªõng d·∫´n...  \n...                                                 ...  \n3161  c√°c slide kh√≥ hi·ªÉu  ng√¥n ng·ªØ trong slide ph·ª©c ...  \n3162                  gi√°o vi√™n gi·∫£ng d·∫°y c√≥ t√¢m huy·∫øt   \n3163                     chia s·∫ª cho em nhi·ªÅu ƒëi·ªÅu hay   \n3164                                  em ti·∫øp thu ch·∫≠m   \n3165  em c√≥ h·ªçc ·ªü m·ªôt trung t√¢m ti·∫øng anh ·ªü ngo√†i tr...  \n\n[3166 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>n√≥i ti·∫øng anh l∆∞u lo√°t</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi√°o vi√™n r·∫•t vui t√≠nh</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>c√¥ max c√≥ t√¢m</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi·∫£ng b√†i thu h√∫t  d√≠ d·ªèm</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi√°o vi√™n kh√¥ng gi·∫£ng d·∫°y ki·∫øn th·ª©c  h∆∞·ªõng d·∫´n...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3161</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>c√°c slide kh√≥ hi·ªÉu  ng√¥n ng·ªØ trong slide ph·ª©c ...</td>\n    </tr>\n    <tr>\n      <th>3162</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi√°o vi√™n gi·∫£ng d·∫°y c√≥ t√¢m huy·∫øt</td>\n    </tr>\n    <tr>\n      <th>3163</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>chia s·∫ª cho em nhi·ªÅu ƒëi·ªÅu hay</td>\n    </tr>\n    <tr>\n      <th>3164</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>em ti·∫øp thu ch·∫≠m</td>\n    </tr>\n    <tr>\n      <th>3165</th>\n      <td>TRAINING_PROGRAM</td>\n      <td>NEUTRAL</td>\n      <td>em c√≥ h·ªçc ·ªü m·ªôt trung t√¢m ti·∫øng anh ·ªü ngo√†i tr...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3166 rows √ó 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.to_csv('test.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.081996Z","iopub.execute_input":"2024-05-26T10:53:13.082326Z","iopub.status.idle":"2024-05-26T10:53:13.106416Z","shell.execute_reply.started":"2024-05-26T10:53:13.082298Z","shell.execute_reply":"2024-05-26T10:53:13.105414Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"combined_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.408559Z","iopub.execute_input":"2024-05-26T10:53:13.409567Z","iopub.status.idle":"2024-05-26T10:53:13.420776Z","shell.execute_reply.started":"2024-05-26T10:53:13.409517Z","shell.execute_reply":"2024-05-26T10:53:13.419739Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"      Topic Sentiment                                           Sentence\n0  LECTURER  POSITIVE                            n√≥i ti·∫øng anh l∆∞u lo√°t \n1  LECTURER  POSITIVE                            gi√°o vi√™n r·∫•t vui t√≠nh \n2  LECTURER  POSITIVE                                     c√¥ max c√≥ t√¢m \n3  LECTURER  POSITIVE                         gi·∫£ng b√†i thu h√∫t  d√≠ d·ªèm \n4  LECTURER  NEGATIVE  gi√°o vi√™n kh√¥ng gi·∫£ng d·∫°y ki·∫øn th·ª©c  h∆∞·ªõng d·∫´n...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>n√≥i ti·∫øng anh l∆∞u lo√°t</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi√°o vi√™n r·∫•t vui t√≠nh</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>c√¥ max c√≥ t√¢m</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LECTURER</td>\n      <td>POSITIVE</td>\n      <td>gi·∫£ng b√†i thu h√∫t  d√≠ d·ªèm</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>LECTURER</td>\n      <td>NEGATIVE</td>\n      <td>gi√°o vi√™n kh√¥ng gi·∫£ng d·∫°y ki·∫øn th·ª©c  h∆∞·ªõng d·∫´n...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load dataset\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# ƒê∆∞·ªùng d·∫´n t·ªõi c√°c file\ntrain_file ='/kaggle/working/train.csv'\nval_file ='/kaggle/working/dev.csv'\ntest_file ='/kaggle/working/test.csv'\n\n# ƒê·ªçc c√°c file CSV\ntrain_df = pd.read_csv(train_file)\nval_df = pd.read_csv(val_file)\ntest_df = pd.read_csv(test_file)\n\n# L·∫•y t·∫•t c·∫£ c√°c t√™n c·ªôt t·ª´ c·∫£ ba DataFrame\nall_columns = set(train_df.columns).union(set(val_df.columns)).union(set(test_df.columns))\n\nall_columns","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.699866Z","iopub.execute_input":"2024-05-26T10:53:13.700704Z","iopub.status.idle":"2024-05-26T10:53:13.761596Z","shell.execute_reply.started":"2024-05-26T10:53:13.700667Z","shell.execute_reply":"2024-05-26T10:53:13.760454Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'Sentence', 'Sentiment', 'Topic'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess data","metadata":{}},{"cell_type":"code","source":"!pip install underthesea","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:13.769401Z","iopub.execute_input":"2024-05-26T10:53:13.769700Z","iopub.status.idle":"2024-05-26T10:53:30.776182Z","shell.execute_reply.started":"2024-05-26T10:53:13.769674Z","shell.execute_reply":"2024-05-26T10:53:30.774569Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collecting underthesea\n  Downloading underthesea-6.8.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\nCollecting python-crfsuite>=0.9.6 (from underthesea)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.31.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)\nCollecting underthesea-core==1.0.4 (from underthesea)\n  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)\nDownloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\nSuccessfully installed python-crfsuite-0.9.10 underthesea-6.8.0 underthesea-core-1.0.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pyvi","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:30.778404Z","iopub.execute_input":"2024-05-26T10:53:30.778716Z","iopub.status.idle":"2024-05-26T10:53:45.158518Z","shell.execute_reply.started":"2024-05-26T10:53:30.778685Z","shell.execute_reply":"2024-05-26T10:53:45.157347Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting pyvi\n  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pyvi) (1.2.2)\nCollecting sklearn-crfsuite (from pyvi)\n  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.2.0)\nRequirement already satisfied: python-crfsuite>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (1.16.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (4.66.1)\nDownloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\nInstalling collected packages: sklearn-crfsuite, pyvi\nSuccessfully installed pyvi-0.1.1 sklearn-crfsuite-0.3.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import regex as re\nimport string\nimport emoji\n\nfrom nltk import flatten\nimport unicodedata\nfrom underthesea import word_tokenize\nfrom pyvi import ViTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:45.159961Z","iopub.execute_input":"2024-05-26T10:53:45.160314Z","iopub.status.idle":"2024-05-26T10:53:46.788861Z","shell.execute_reply.started":"2024-05-26T10:53:45.160279Z","shell.execute_reply":"2024-05-26T10:53:46.787863Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 1. Lo·∫°i b·ªè c√°c th·∫ª HTML\ndef remove_HTML(text):\n    clean = re.compile('<.*?>')\n    return re.sub(clean,'', text)\n\n# 2. Chuy·ªÉn ƒë·ªïi c√°c k√Ω t·ª± Unicode v·ªÅ d·∫°ng chu·∫©n\ndef convert_unicode(text):\n    return unicodedata.normalize('NFC', text)\n\n# 3. Lo·∫°i b·ªè c√°c k√Ω t·ª± k√©o d√†i\ndef remove_elongated_chars(text):\n    replacements = {\n       'a':'√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠',\n       'e':'√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá',\n       'i':'√¨√≠·ªâƒ©·ªã',\n       'o':'√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£',\n       'u':'√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±',\n       'y':'·ª≥√Ω·ª∑·ªπ·ªµ',\n       'd':'ƒë',\n       'A':'√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨',\n       'E':'√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ',\n       'I':'√å√ç·ªàƒ®·ªä',\n       'O':'√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢',\n       'U':'√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞',\n       'Y':'·ª≤√ù·ª∂·ª∏·ª¥',\n       'D':'ƒê'\n    }\n    \n    for char, replacements_str in replacements.items():\n        pattern = rf\"({char})\\1+\"\n        text = re.sub(pattern, char, text)\n    \n    pattern = r\"(\\w)\\1+\"\n    text = re.sub(pattern, r'\\1', text)\n    return text\n\n# 4. X·ª≠ l√Ω c√°c t·ª´ ph·ªß ƒë·ªãnh\ndef handle_negation(text):\n    not_words = {\"kh√¥ng\",'kh√¥ng h·ªÅ', \"ch·∫≥ng\", \"ch∆∞a\", \"kh√¥ng ph·∫£i\", \"ch·∫£\", \"m·∫•t\",\n                 \"thi·∫øu\", \"ƒë·∫øch\", \"ƒë√©o\", \"k√©m\", \"n·ªè\", \"not\",\n                 \"b·ªõt\", \"kh√¥ng bao gi·ªù\", \"ch∆∞a bao gi·ªù\"}\n    not_words = sorted(not_words, key=len, reverse=True)\n    pattern = r'\\b(?:'+'|'.join(re.escape(word) for word in not_words) + r')\\b'\n    text = re.sub(pattern,'kh√¥ng', text, flags=re.IGNORECASE)\n    return text\n\n# 5. Chu·∫©n h√≥a c√°c t·ª´ vi·∫øt t·∫Øt th∆∞·ªùng g·∫∑p\ndef normalize_acronyms(text):\n    acronyms = {\n       '√¥ k√™i':'ok','okie':'ok','o k√™':'ok',\n       'okey':'ok','√¥k√™':'ok','oki':'ok','oke': 'ok','okay':'ok','ok√™':'ok',\n       'tks': u'c√°m ∆°n','thks': u'c√°m ∆°n','thanks': u'c√°m ∆°n','ths': u'c√°m ∆°n','thank': u'c√°m ∆°n',\n       '‚≠ê':'star','*':'star','üåü':'star',\n       'kg': u'kh√¥ng','not': u'kh√¥ng', u'kg': u'kh√¥ng','\"k': u'kh√¥ng','kh':u'kh√¥ng','k√¥':u'kh√¥ng','hok':u'kh√¥ng','kp': u'kh√¥ng ph·∫£i',u'k√¥': u'kh√¥ng','\"ko': u'kh√¥ng', u'ko': u'kh√¥ng', u'k': u'kh√¥ng','khong': u'kh√¥ng', u'hok': u'kh√¥ng',\n       'cute': u'd·ªÖ th∆∞∆°ng','vs': u'v·ªõi','wa':'qu√°','w√°': u'qu√°','j': u'g√¨','‚Äú':'',\n       'sz': u'c·ª°','size': u'c·ª°', u'ƒëx': u'ƒë∆∞·ª£c','dk': u'ƒë∆∞·ª£c','dc': u'ƒë∆∞·ª£c','ƒëk': u'ƒë∆∞·ª£c',\n       'ƒëc': u'ƒë∆∞·ª£c','authentic': u'chu·∫©n ch√≠nh h√£ng',u'aut': u'chu·∫©n ch√≠nh h√£ng', u'auth': u'chu·∫©n ch√≠nh h√£ng','store': u'c·ª≠a h√†ng',\n       'shop': u'c·ª≠a h√†ng','sp': u's·∫£n ph·∫©m','gud': u't·ªët','god': u't·ªët','wel done':'t·ªët','good': u't·ªët','g√∫t': u't·ªët',\n       's·∫•u': u'x·∫•u','gut': u't·ªët', u'tot': u't·ªët', u'nice': u't·ªët','perfect':'r·∫•t t·ªët','bt': u'b√¨nh th∆∞·ªùng',\n       'time': u'th·ªùi gian','q√°': u'qu√°', u'ship': u'giao h√†ng', u'm': u'm√¨nh', u'mik': u'm√¨nh',\n       '√™Ãâ':'·ªÉ','product':'s·∫£n ph·∫©m','quality':'ch·∫•t l∆∞·ª£ng','chat':'ch·∫•t','excelent':'ho√†n h·∫£o','bad':'t·ªá','fresh':'t∆∞∆°i','sad':'t·ªá',\n       'date': u'h·∫°n s·ª≠ d·ª•ng','hsd': u'h·∫°n s·ª≠ d·ª•ng','quickly': u'nhanh','quick': u'nhanh','fast': u'nhanh','delivery': u'giao h√†ng',u's√≠p': u'giao h√†ng',\n       'beautiful': u'ƒë·∫πp tuy·ªát v·ªùi', u'tl': u'tr·∫£ l·ªùi', u'r': u'r·ªìi', u'shopE': u'c·ª≠a h√†ng',u'order': u'ƒë·∫∑t h√†ng',\n       'ch·∫•t lg': u'ch·∫•t l∆∞·ª£ng',u'sd': u's·ª≠ d·ª•ng',u'dt': u'ƒëi·ªán tho·∫°i',u'nt': u'nh·∫Øn tin',u'tl': u'tr·∫£ l·ªùi',u's√†i': u'x√†i',u'bjo':u'bao gi·ªù',\n       'thick': u'th√≠ch','thik': u'th√≠ch', u'sop': u'c·ª≠a h√†ng', u'shop': u'c·ª≠a h√†ng', \n       'fb':'facebook','face':'facebook','very': u'r·∫•t',u'qu·∫£ ng':u'qu·∫£ng ',\n       'dep': u'ƒë·∫πp',u'xau': u'x·∫•u','delicious': u'ngon', u'h√†g': u'h√†ng', u'q·ªßa': u'qu·∫£',\n       'iu': u'y√™u','fake': u'gi·∫£ m·∫°o','trl':'tr·∫£ l·ªùi',\n       'por': u't·ªá','poor': u't·ªá','ib':u'nh·∫Øn tin','rep':u'tr·∫£ l·ªùi',u'fback':'feedback','fedback':'feedback',\n       'max': u'c·ª±c k·ª≥',\n       'full':'ƒë·∫ßy ƒë·ªß', 'ful':'ƒë·∫ßy ƒë·ªß'\n    }\n    words = text.split()\n    normalized_text =' '.join([acronyms.get(word.lower(), word) for word in words])\n    return normalized_text\n\n# 6. Ph√¢n ƒëo·∫°n t·ª´ cho ti·∫øng Vi·ªát\ndef word_segmentation(text):\n    return word_tokenize(text, format=\"text\")\n\n# 7. Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt kh·ªèi vƒÉn b·∫£n\ndef remove_unnecessary_characters(text):\n    text = re.sub(r'\\s+', ' ', text)  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n    text = re.sub(r'[^\\w\\s]', '', text)  # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát\n    return text.strip()\n\n# 8. K·∫øt h·ª£p c√°c h√†m ƒë·ªÉ ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\ndef text_preprocess(text):\n    text = remove_HTML(text)\n    text = normalize_acronyms(text)\n    text = convert_unicode(text)\n    text = remove_elongated_chars(text)\n    text = handle_negation(text)\n    #text = word_segmentation(text)\n    text = remove_unnecessary_characters(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:46.791485Z","iopub.execute_input":"2024-05-26T10:53:46.791807Z","iopub.status.idle":"2024-05-26T10:53:46.817763Z","shell.execute_reply.started":"2024-05-26T10:53:46.791778Z","shell.execute_reply":"2024-05-26T10:53:46.816926Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"text = \"slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß\"\nprint(text_preprocess(text))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:46.818828Z","iopub.execute_input":"2024-05-26T10:53:46.819203Z","iopub.status.idle":"2024-05-26T10:53:46.842678Z","shell.execute_reply.started":"2024-05-26T10:53:46.819168Z","shell.execute_reply":"2024-05-26T10:53:46.841653Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"slide gi√°o tr√¨nh ƒë·∫ßy ƒë·ªß\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install tf-models-official\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:53:46.844095Z","iopub.execute_input":"2024-05-26T10:53:46.844781Z","iopub.status.idle":"2024-05-26T10:55:15.461519Z","shell.execute_reply.started":"2024-05-26T10:53:46.844745Z","shell.execute_reply":"2024-05-26T10:55:15.460460Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Collecting tf-models-official\n  Downloading tf_models_official-2.16.0-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (3.0.8)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.5.0)\nCollecting gin-config (from tf-models-official)\n  Downloading gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.126.0)\nCollecting immutabledict (from tf-models-official)\n  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.6.12)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (3.7.5)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.26.4)\nRequirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.1.3)\nRequirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.9.0.80)\nRequirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.1.4)\nRequirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (5.9.3)\nRequirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.0.0)\nCollecting pycocotools (from tf-models-official)\n  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (6.0.1)\nCollecting sacrebleu (from tf-models-official)\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.11.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.2.0)\nCollecting seqeval (from tf-models-official)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.16.0)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.9.4)\nRequirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.16.1)\nCollecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\nCollecting tensorflow-text~=2.16.1 (from tf-models-official)\n  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\nCollecting tensorflow~=2.16.1 (from tf-models-official)\n  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting tf-keras>=2.16.0 (from tf-models-official)\n  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting tf-slim>=1.1.0 (from tf-models-official)\n  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.21.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.26.1)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.2.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\nRequirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2024.2.2)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.9.0.post0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (8.0.4)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (6.1.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2023.4)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (16.0.6)\nCollecting ml-dtypes~=0.3.1 (from tensorflow~=2.16.1->tf-models-official)\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (69.0.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.14.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.51.1)\nCollecting tensorboard<2.17,>=2.16 (from tensorflow~=2.16.1->tf-models-official)\n  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.2.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.35.0)\nRequirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (3.1.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.5.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.3.0)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (4.9)\nCollecting portalocker (from sacrebleu->tf-models-official)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (5.2.1)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official) (1.2.2)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (8.1.7)\nRequirement already satisfied: etils>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (1.6.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\nRequirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.5.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.16.1->tf-models-official) (0.42.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (2024.2.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (6.1.1)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (3.17.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.62.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.6)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (3.0.2)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.1.2)\nDownloading tf_models_official-2.16.0-py2.py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\nDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=0188b2cd398c4b27cd09f4b63a4c4246a0a18e17277577a4ab32950be32e633b\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: gin-config, tf-slim, tensorflow-model-optimization, portalocker, ml-dtypes, immutabledict, tensorboard, sacrebleu, seqeval, pycocotools, tensorflow, tf-keras, tensorflow-text, tf-models-official\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n  Attempting uninstall: tf-keras\n    Found existing installation: tf_keras 2.15.1\n    Uninstalling tf_keras-2.15.1:\n      Successfully uninstalled tf_keras-2.15.1\n  Attempting uninstall: tensorflow-text\n    Found existing installation: tensorflow-text 2.15.0\n    Uninstalling tensorflow-text-2.15.0:\n      Successfully uninstalled tensorflow-text-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gin-config-0.5.0 immutabledict-4.2.0 ml-dtypes-0.3.2 portalocker-2.8.2 pycocotools-2.0.7 sacrebleu-2.4.2 seqeval-1.2.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-model-optimization-0.8.0 tensorflow-text-2.16.1 tf-keras-2.16.0 tf-models-official-2.16.0 tf-slim-1.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tensorflow.keras.layers import Input, Dense, Dropout, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom official.nlp import optimization\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:15.462954Z","iopub.execute_input":"2024-05-26T10:55:15.463297Z","iopub.status.idle":"2024-05-26T10:55:25.914052Z","shell.execute_reply.started":"2024-05-26T10:55:15.463261Z","shell.execute_reply":"2024-05-26T10:55:25.912946Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Define pre-trained model and tokenizer\nPRETRAINED_MODEL = 'vinai/phobert-base'  # Choose your preferred Vietnamese BERT model\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\ntokenizer.max_model_input_sizes","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:25.915538Z","iopub.execute_input":"2024-05-26T10:55:25.916387Z","iopub.status.idle":"2024-05-26T10:55:27.200977Z","shell.execute_reply.started":"2024-05-26T10:55:25.916348Z","shell.execute_reply":"2024-05-26T10:55:27.199957Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae9b5928bcbc4c4fa66f258d961ee21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9e49d3a1d448f2afe4b333e9f439a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b95071991b4b5e95d8dcb0ca6de2fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ce5da06b93a4bfa985aa0ef943f07cf"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'vinai/phobert-base': 256, 'vinai/phobert-large': 256}"},"metadata":{}}]},{"cell_type":"code","source":"# Prepare data for TensorFlow\nMAX_SEQUENCE_LENGTH = tokenizer.model_max_length\nBATCH_SIZE = 16\nEPOCHS = 10\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.202248Z","iopub.execute_input":"2024-05-26T10:55:27.202532Z","iopub.status.idle":"2024-05-26T10:55:27.207253Z","shell.execute_reply.started":"2024-05-26T10:55:27.202508Z","shell.execute_reply":"2024-05-26T10:55:27.206174Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Define labels\n\n# Perform one-hot encoding on 'Topic' column\none_hot_encoded = pd.get_dummies(train_df['Topic'])\none_hot_encoded = one_hot_encoded.astype(int)\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\ntrain_df = pd.concat([train_df, one_hot_encoded], axis=1)\n\n# Drop the original 'result' column\ntrain_df.drop(columns=['Topic'], inplace=True)\n\n# Perform one-hot encoding on 'result' column\none_hot_encoded1 = pd.get_dummies(val_df['Topic'])\none_hot_encoded1 = one_hot_encoded1.astype(int)\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\nval_df = pd.concat([val_df, one_hot_encoded1], axis=1)\n\n# Drop the original 'result' column\nval_df.drop(columns=['Topic'], inplace=True)\n\n# Perform one-hot encoding on 'result' column\none_hot_encoded2 = pd.get_dummies(test_df['Topic'])\none_hot_encoded2 = one_hot_encoded2.astype(int)\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\ntest_df = pd.concat([test_df, one_hot_encoded2], axis=1)\n\n# Drop the original 'result' column\ntest_df.drop(columns=['Topic'], inplace=True)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.212193Z","iopub.execute_input":"2024-05-26T10:55:27.212495Z","iopub.status.idle":"2024-05-26T10:55:27.222056Z","shell.execute_reply.started":"2024-05-26T10:55:27.212459Z","shell.execute_reply":"2024-05-26T10:55:27.221116Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"\"# Define labels\\n\\n# Perform one-hot encoding on 'Topic' column\\none_hot_encoded = pd.get_dummies(train_df['Topic'])\\none_hot_encoded = one_hot_encoded.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\ntrain_df = pd.concat([train_df, one_hot_encoded], axis=1)\\n\\n# Drop the original 'result' column\\ntrain_df.drop(columns=['Topic'], inplace=True)\\n\\n# Perform one-hot encoding on 'result' column\\none_hot_encoded1 = pd.get_dummies(val_df['Topic'])\\none_hot_encoded1 = one_hot_encoded1.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\nval_df = pd.concat([val_df, one_hot_encoded1], axis=1)\\n\\n# Drop the original 'result' column\\nval_df.drop(columns=['Topic'], inplace=True)\\n\\n# Perform one-hot encoding on 'result' column\\none_hot_encoded2 = pd.get_dummies(test_df['Topic'])\\none_hot_encoded2 = one_hot_encoded2.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\ntest_df = pd.concat([test_df, one_hot_encoded2], axis=1)\\n\\n# Drop the original 'result' column\\ntest_df.drop(columns=['Topic'], inplace=True)\\n\\n\""},"metadata":{}}]},{"cell_type":"code","source":"# Define the mapping for sentiment\nsentiment_mapping = {'POSITIVE': 1, 'NEGATIVE': 2, 'NEUTRAL': 3}\n\n# Function to perform the customized one-hot encoding\ndef custom_one_hot_encoding(df):\n    # One-hot encode the 'Topic' column\n    one_hot_encoded = pd.get_dummies(df['Topic'])\n    # Map the 'Sentiment' values to the specified values\n    one_hot_encoded = one_hot_encoded * df['Sentiment'].map(sentiment_mapping).values[:, None]\n    # Concatenate the one-hot encoded DataFrame with the original DataFrame\n    df = pd.concat([df, one_hot_encoded], axis=1)\n    # Drop the original 'Topic' column\n    df.drop(columns=['Topic'], inplace=True)\n    return df\n\n# Apply the function to each DataFrame\ntrain_df = custom_one_hot_encoding(train_df)\nval_df = custom_one_hot_encoding(val_df)\ntest_df = custom_one_hot_encoding(test_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.223204Z","iopub.execute_input":"2024-05-26T10:55:27.223553Z","iopub.status.idle":"2024-05-26T10:55:27.262333Z","shell.execute_reply.started":"2024-05-26T10:55:27.223521Z","shell.execute_reply":"2024-05-26T10:55:27.261515Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(['Sentiment'], axis = 1)\nval_df = val_df.drop(['Sentiment'], axis = 1)\ntest_df = test_df.drop(['Sentiment'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.263495Z","iopub.execute_input":"2024-05-26T10:55:27.263819Z","iopub.status.idle":"2024-05-26T10:55:27.271464Z","shell.execute_reply.started":"2024-05-26T10:55:27.263791Z","shell.execute_reply":"2024-05-26T10:55:27.270442Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.272675Z","iopub.execute_input":"2024-05-26T10:55:27.273030Z","iopub.status.idle":"2024-05-26T10:55:27.284327Z","shell.execute_reply.started":"2024-05-26T10:55:27.272993Z","shell.execute_reply":"2024-05-26T10:55:27.283228Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def make_outputs(df):\n    outputs = []\n    for row in range(len(df)):\n        row_one_hot = []\n        for col in range(1, len(df.columns)):\n            sentiment = df.iloc[row, col]\n            if sentiment == 0: one_hot = [1, 0, 0, 0] #None\n            elif sentiment == 1: one_hot = [0, 1, 0, 0] # Pos\n            elif sentiment == 2: one_hot = [0, 0, 1, 0] # Neg\n            elif sentiment == 3: one_hot = [0, 0, 0, 1] # Neu\n            row_one_hot.append(one_hot)\n        outputs.append(row_one_hot)\n    return np.array(outputs, dtype='uint8')\ny_train = make_outputs(train_df)\ny_val = make_outputs(val_df)\ny_test = make_outputs(test_df)\n\nprint('Train outputs:', y_train.shape)\nprint('Validate outputs:', y_val.shape)\nprint('Test outputs:', y_test.shape)\ny_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:27.285652Z","iopub.execute_input":"2024-05-26T10:55:27.286065Z","iopub.status.idle":"2024-05-26T10:55:29.887633Z","shell.execute_reply.started":"2024-05-26T10:55:27.286031Z","shell.execute_reply":"2024-05-26T10:55:29.886576Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Train outputs: (11426, 4, 4)\nValidate outputs: (1583, 4, 4)\nTest outputs: (3166, 4, 4)\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"train_df.to_csv('train_tokenize.csv', index = False)\nval_df.to_csv('val_tokenize.csv', index = False)\ntest_df.to_csv('test_tokenize.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:29.889265Z","iopub.execute_input":"2024-05-26T10:55:29.890049Z","iopub.status.idle":"2024-05-26T10:55:29.982726Z","shell.execute_reply.started":"2024-05-26T10:55:29.890007Z","shell.execute_reply":"2024-05-26T10:55:29.981732Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/working/train_tokenize.csv\"\nVAL_PATH = \"/kaggle/working/val_tokenize.csv\"\nTEST_PATH = \"/kaggle/working/test_tokenize.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:29.984072Z","iopub.execute_input":"2024-05-26T10:55:29.984445Z","iopub.status.idle":"2024-05-26T10:55:29.989898Z","shell.execute_reply.started":"2024-05-26T10:55:29.984409Z","shell.execute_reply":"2024-05-26T10:55:29.988931Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nraw_datasets = load_dataset('csv', data_files={'train': TRAIN_PATH, 'val': VAL_PATH, 'test': TEST_PATH})\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:29.991439Z","iopub.execute_input":"2024-05-26T10:55:29.992193Z","iopub.status.idle":"2024-05-26T10:55:30.310416Z","shell.execute_reply.started":"2024-05-26T10:55:29.992156Z","shell.execute_reply":"2024-05-26T10:55:30.309366Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79e340ffd06f41269407ae09dd89cdba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating val split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abee33aae9fc4ea68de4ccb5cd5d9380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b7c006ffa2746e7b4ccb16c250ed460"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n        num_rows: 11426\n    })\n    val: Dataset({\n        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n        num_rows: 1583\n    })\n    test: Dataset({\n        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n        num_rows: 3166\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print('input_ids of sentence 1484:', raw_datasets['train'][1484])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:30.311731Z","iopub.execute_input":"2024-05-26T10:55:30.312053Z","iopub.status.idle":"2024-05-26T10:55:30.323132Z","shell.execute_reply.started":"2024-05-26T10:55:30.312026Z","shell.execute_reply":"2024-05-26T10:55:30.322145Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"input_ids of sentence 1484: {'Sentence': 'th·∫ßy t·∫≠n t√¢m v·ªõi h·ªçc sinh  nhi·ªát t√¨nh v·ªõi h·ªçc sinh ', 'FACILITY': 0, 'LECTURER': 1, 'OTHERS': 0, 'TRAINING_PROGRAM': 0}\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(dataset):\n    clean_texts = list(map(text_preprocess, dataset['Sentence']))\n    return tokenizer(clean_texts, max_length=tokenizer.model_max_length, padding='max_length', truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:30.324350Z","iopub.execute_input":"2024-05-26T10:55:30.324737Z","iopub.status.idle":"2024-05-26T10:55:30.332745Z","shell.execute_reply.started":"2024-05-26T10:55:30.324700Z","shell.execute_reply":"2024-05-26T10:55:30.331837Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\nprint('input_ids of sentence 1484:', tokenized_datasets['train'][1484]['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:30.334310Z","iopub.execute_input":"2024-05-26T10:55:30.334704Z","iopub.status.idle":"2024-05-26T10:55:50.686264Z","shell.execute_reply.started":"2024-05-26T10:55:30.334669Z","shell.execute_reply":"2024-05-26T10:55:50.685226Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58c7c5a75e2044a296deaaa20891f115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00acd5650b9b4ceeb4e4b039b1356220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3166 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62284d09817c4515b92dfd8d74fa8961"}},"metadata":{}},{"name":"stdout","text":"input_ids of sentence 1484: [0, 1249, 1855, 2652, 15, 222, 418, 2515, 939, 15, 222, 418, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Decode:', tokenizer.decode(tokenized_datasets['train'][1484]['input_ids']))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:50.687773Z","iopub.execute_input":"2024-05-26T10:55:50.688489Z","iopub.status.idle":"2024-05-26T10:55:50.698161Z","shell.execute_reply.started":"2024-05-26T10:55:50.688442Z","shell.execute_reply":"2024-05-26T10:55:50.697266Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Decode: <s> th·∫ßy t·∫≠n t√¢m v·ªõi h·ªçc sinh nhi·ªát t√¨nh v·ªõi h·ªçc sinh </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_datasets['train']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:50.699840Z","iopub.execute_input":"2024-05-26T10:55:50.700151Z","iopub.status.idle":"2024-05-26T10:55:50.733720Z","shell.execute_reply.started":"2024-05-26T10:55:50.700126Z","shell.execute_reply":"2024-05-26T10:55:50.732515Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 11426\n})"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"], batch_size=BATCH_SIZE, shuffle=True\n)\n\nvalidation_dataset = tokenized_datasets[\"val\"].to_tf_dataset(\n    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"], batch_size=BATCH_SIZE, shuffle=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:50.735349Z","iopub.execute_input":"2024-05-26T10:55:50.735709Z","iopub.status.idle":"2024-05-26T10:55:51.916942Z","shell.execute_reply.started":"2024-05-26T10:55:50.735677Z","shell.execute_reply":"2024-05-26T10:55:51.916099Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tensorflow.keras import layers, Model","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.918151Z","iopub.execute_input":"2024-05-26T10:55:51.918412Z","iopub.status.idle":"2024-05-26T10:55:51.923329Z","shell.execute_reply.started":"2024-05-26T10:55:51.918387Z","shell.execute_reply":"2024-05-26T10:55:51.922092Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"class BertLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(BertLayer, self).__init__(**kwargs)\n        self.bert = TFAutoModel.from_pretrained(PRETRAINED_MODEL)\n\n    def call(self, inputs):\n        input_ids, attention_mask, token_type_ids = inputs\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        return outputs[0]\n\ndef create_model(optimizer):\n    # Build your model on top of BERT\n    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='token_type_ids')\n\n    bert_layer = BertLayer()([input_ids, attention_mask, token_type_ids])\n    pooled_output = bert_layer[:, 0, :]  # Take only the first token\n    #dense = layers.Dense(16, activation='softmax')(pooled_output)\n    dense = concatenate([\n        Dense(\n            units = 4, \n            activation = 'softmax',\n            name = label,\n        )(pooled_output) for label in train_df.columns[1:]\n    ], axis = -1)\n    # Define your model\n    model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=dense)\n\n    # Compile your model\n    #model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n    \n    model.summary()\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.924765Z","iopub.execute_input":"2024-05-26T10:55:51.925086Z","iopub.status.idle":"2024-05-26T10:55:51.937262Z","shell.execute_reply.started":"2024-05-26T10:55:51.925060Z","shell.execute_reply":"2024-05-26T10:55:51.936284Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\noptimizer = Adam(learning_rate=1e-5)\ntype(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.938396Z","iopub.execute_input":"2024-05-26T10:55:51.938719Z","iopub.status.idle":"2024-05-26T10:55:51.961706Z","shell.execute_reply.started":"2024-05-26T10:55:51.938693Z","shell.execute_reply":"2024-05-26T10:55:51.960639Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"keras.src.optimizers.adam.Adam"},"metadata":{}}]},{"cell_type":"code","source":"model = create_model(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:55:51.962989Z","iopub.execute_input":"2024-05-26T10:55:51.963378Z","iopub.status.idle":"2024-05-26T10:56:23.023072Z","shell.execute_reply.started":"2024-05-26T10:55:51.963343Z","shell.execute_reply":"2024-05-26T10:56:23.022109Z"},"trusted":true},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":"tf_model.h5:   0%|          | 0.00/740M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab0f46a6d7049f088ad99181e729ee9"}},"metadata":{}},{"name":"stderr","text":"Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ input_ids           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ attention_mask      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ token_type_ids      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ bert_layer          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m768\u001b[0m)  ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  ‚îÇ\n‚îÇ (\u001b[38;5;33mBertLayer\u001b[0m)         ‚îÇ                   ‚îÇ            ‚îÇ attention_mask[\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ token_type_ids[\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ get_item (\u001b[38;5;33mGetItem\u001b[0m)  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ bert_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ FACILITY (\u001b[38;5;33mDense\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ LECTURER (\u001b[38;5;33mDense\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ OTHERS (\u001b[38;5;33mDense\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ TRAINING_PROGRAM    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    ‚îÇ\n‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ concatenate         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ FACILITY[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   ‚îÇ\n‚îÇ (\u001b[38;5;33mConcatenate\u001b[0m)       ‚îÇ                   ‚îÇ            ‚îÇ LECTURER[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ OTHERS[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ TRAINING_PROGRAM‚Ä¶ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ input_ids           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ attention_mask      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ token_type_ids      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ bert_layer          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         ‚îÇ                   ‚îÇ            ‚îÇ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ bert_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ FACILITY (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ LECTURER (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ OTHERS (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ TRAINING_PROGRAM    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ concatenate         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ FACILITY[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       ‚îÇ                   ‚îÇ            ‚îÇ LECTURER[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ OTHERS[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ TRAINING_PROGRAM‚Ä¶ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Create TensorFlow Datasets\ndef encode_data(tokenized_datasets, labels):\n    return (\n        {\n            'input_ids': tf.constant(tokenized_datasets['input_ids']),\n            'attention_mask': tf.constant(tokenized_datasets['attention_mask']),\n            'token_type_ids': tf.constant(tokenized_datasets['token_type_ids'])\n        },\n        tf.constant(labels)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:23.029551Z","iopub.execute_input":"2024-05-26T10:56:23.029847Z","iopub.status.idle":"2024-05-26T10:56:23.035361Z","shell.execute_reply.started":"2024-05-26T10:56:23.029821Z","shell.execute_reply":"2024-05-26T10:56:23.034450Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train_data = encode_data(tokenized_datasets['train'], np.reshape(y_train, (-1, 16)))\nval_data = encode_data(tokenized_datasets['val'], np.reshape(y_val, (-1, 16)))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:23.036479Z","iopub.execute_input":"2024-05-26T10:56:23.036739Z","iopub.status.idle":"2024-05-26T10:56:30.462063Z","shell.execute_reply.started":"2024-05-26T10:56:23.036716Z","shell.execute_reply":"2024-05-26T10:56:30.461054Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:30.463398Z","iopub.execute_input":"2024-05-26T10:56:30.463788Z","iopub.status.idle":"2024-05-26T10:56:30.473912Z","shell.execute_reply.started":"2024-05-26T10:56:30.463757Z","shell.execute_reply":"2024-05-26T10:56:30.472850Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"({'input_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[    0, 48090,  4368, ...,     1,     1,     1],\n         [    0,  2515,   939, ...,     1,     1,     1],\n         [    0,    57,   222, ...,     1,     1,     1],\n         ...,\n         [    0,   574,   387, ...,     1,     1,     1],\n         [    0,  4368,  1430, ...,     1,     1,     1],\n         [    0,  1685,  2953, ...,     1,     1,     1]], dtype=int32)>,\n  'attention_mask': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         ...,\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n  'token_type_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>},\n <tf.Tensor: shape=(11426, 16), dtype=uint8, numpy=\n array([[1, 0, 0, ..., 1, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 1, 0],\n        ...,\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)"},"metadata":{}}]},{"cell_type":"code","source":"# Create TensorFlow datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(BATCH_SIZE)\nval_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:30.475219Z","iopub.execute_input":"2024-05-26T10:56:30.475561Z","iopub.status.idle":"2024-05-26T10:56:31.279483Z","shell.execute_reply.started":"2024-05-26T10:56:30.475534Z","shell.execute_reply":"2024-05-26T10:56:31.278307Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:31.281112Z","iopub.execute_input":"2024-05-26T10:56:31.281743Z","iopub.status.idle":"2024-05-26T10:56:31.291567Z","shell.execute_reply.started":"2024-05-26T10:56:31.281706Z","shell.execute_reply":"2024-05-26T10:56:31.290414Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"({'input_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[    0, 48090,  4368, ...,     1,     1,     1],\n         [    0,  2515,   939, ...,     1,     1,     1],\n         [    0,    57,   222, ...,     1,     1,     1],\n         ...,\n         [    0,   574,   387, ...,     1,     1,     1],\n         [    0,  4368,  1430, ...,     1,     1,     1],\n         [    0,  1685,  2953, ...,     1,     1,     1]], dtype=int32)>,\n  'attention_mask': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         ...,\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0],\n         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n  'token_type_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n  array([[0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         ...,\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0],\n         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>},\n <tf.Tensor: shape=(11426, 16), dtype=uint8, numpy=\n array([[1, 0, 0, ..., 1, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 1, 0],\n        ...,\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"# Hu·∫•n luy·ªán m√¥ h√¨nh\ndef train_model(model, train_dataset, val_dataset, epochs=3):\n\n    for epoch in range(epochs):\n        history = model.fit(\n                        train_dataset,\n                        epochs=epochs,\n                        validation_data=val_dataset\n                    )\n\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {history.history['loss'][0]:.4f}, Train Accuracy: {history.history['accuracy'][0]:.4f}\")\n        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {history.history['val_loss'][0]:.4f}, Val Accuracy: {history.history['val_accuracy'][0]:.4f}\")\n    \n    model.save('sentiment_model.h5')\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:31.293041Z","iopub.execute_input":"2024-05-26T10:56:31.295439Z","iopub.status.idle":"2024-05-26T10:56:31.303270Z","shell.execute_reply.started":"2024-05-26T10:56:31.295411Z","shell.execute_reply":"2024-05-26T10:56:31.302165Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"'# Hu·∫•n luy·ªán m√¥ h√¨nh\\ndef train_model(model, train_dataset, val_dataset, epochs=3):\\n\\n    for epoch in range(epochs):\\n        history = model.fit(\\n                        train_dataset,\\n                        epochs=epochs,\\n                        validation_data=val_dataset\\n                    )\\n\\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {history.history[\\'loss\\'][0]:.4f}, Train Accuracy: {history.history[\\'accuracy\\'][0]:.4f}\")\\n        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {history.history[\\'val_loss\\'][0]:.4f}, Val Accuracy: {history.history[\\'val_accuracy\\'][0]:.4f}\")\\n    \\n    model.save(\\'sentiment_model.h5\\')'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install --upgrade wandb tensorflow\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:31.304532Z","iopub.execute_input":"2024-05-26T10:56:31.304888Z","iopub.status.idle":"2024-05-26T10:56:49.582556Z","shell.execute_reply.started":"2024-05-26T10:56:31.304844Z","shell.execute_reply":"2024-05-26T10:56:49.581441Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nCollecting wandb\n  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.2.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\nDownloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.16.6\n    Uninstalling wandb-0.16.6:\n      Successfully uninstalled wandb-0.16.6\nSuccessfully installed wandb-0.17.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nimport tensorflow as tf\nimport numpy as np\nimport warnings\nimport logging\n# Set TensorFlow logging level to ERROR to suppress warnings and informational messages\ntf.get_logger().setLevel('ERROR')\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\n\n# Additional method to suppress specific warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings and errors: 0 = all logs, 1 = filter out INFO, 2 = filter out WARNING, 3 = filter out ERROR\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:49.584074Z","iopub.execute_input":"2024-05-26T10:56:49.584390Z","iopub.status.idle":"2024-05-26T10:56:50.018960Z","shell.execute_reply.started":"2024-05-26T10:56:49.584358Z","shell.execute_reply":"2024-05-26T10:56:50.018066Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport os\nimport tensorflow as tf\n\n# Define the checkpoint path\ncheckpoint_path = \"sentiment_model_checkpoint.weights.h5\"\n# Set your WandB API key\nwandb.login(key=\"b9575849263a9312a73f76d71d270c8751628e10\")\n\n# Initialize WandB with a unique run ID based on current timestamp\nrun_id = \"9n6h5k0i\"\nwandb.init(project=\"absa-vietnamese\", id=run_id, resume=\"allow\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T10:56:50.020297Z","iopub.execute_input":"2024-05-26T10:56:50.020699Z","iopub.status.idle":"2024-05-26T10:56:53.938510Z","shell.execute_reply.started":"2024-05-26T10:56:50.020656Z","shell.execute_reply":"2024-05-26T10:56:53.937557Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlavibuu\u001b[0m (\u001b[33mlavibu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240526_105652-9n6h5k0i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Resuming run <strong><a href='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i' target=\"_blank\">azure-valley-25</a></strong> to <a href='https://wandb.ai/lavibu/absa-vietnamese' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lavibu/absa-vietnamese' target=\"_blank\">https://wandb.ai/lavibu/absa-vietnamese</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i' target=\"_blank\">https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i</a>"},"metadata":{}},{"execution_count":50,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7fdc70164280>"},"metadata":{}}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ƒê·ªãnh nghƒ©a early_stop_callback\nearly_stop_callback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',  \n    patience=5,  \n    restore_best_weights=True  \n)\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path,\n    save_weights_only=True,\n    monitor='val_loss',\n    mode='min',\n    save_best_only=True,\n    verbose=1\n)\n\n\n# Thi·∫øt l·∫≠p m·ª©c ƒë·ªô log c·ªßa TensorFlow ƒë·ªÉ ch·ªâ hi·ªÉn th·ªã c√°c l·ªói nghi√™m tr·ªçng\ntf.get_logger().setLevel('ERROR')\n\n# Check if checkpoint exists and load weights\nif os.path.exists(checkpoint_path):\n    model.load_weights(checkpoint_path)\n    print(\"Checkpoint loaded.\")\n\n# Define the training function\ndef train_model(model, train_dataset, val_dataset, epochs=3):\n    for epoch in range(epochs):\n        history = model.fit(\n            train_dataset,\n            epochs=1,  # Train for 1 epoch at a time\n            validation_data=val_dataset,\n            callbacks=[\n                early_stop_callback,\n                model_checkpoint_callback,\n            ],\n            verbose=1\n        )\n        # Calculate average train and validation loss\n        avg_train_loss = np.mean(history.history['loss'])\n        avg_val_loss = np.mean(history.history['val_loss'])\n\n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"train_loss: {avg_train_loss:.4f} - val_loss: {avg_val_loss:.4f}\")\n\n        # Log metrics to WandB\n        wandb.log({\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n# Train the model\ntrain_model(model, train_dataset, val_dataset, epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:00:36.404367Z","iopub.execute_input":"2024-05-26T13:00:36.404735Z","iopub.status.idle":"2024-05-26T13:29:11.280915Z","shell.execute_reply.started":"2024-05-26T13:00:36.404706Z","shell.execute_reply":"2024-05-26T13:29:11.279732Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Checkpoint loaded.\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1145\nEpoch 1: val_loss improved from inf to 0.12247, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1145 - val_loss: 0.1225\nEpoch 1/10\ntrain_loss: 0.1155 - val_loss: 0.1225\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1144\nEpoch 1: val_loss improved from 0.12247 to 0.12241, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1144 - val_loss: 0.1224\nEpoch 2/10\ntrain_loss: 0.1154 - val_loss: 0.1224\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1144\nEpoch 1: val_loss improved from 0.12241 to 0.12236, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1144 - val_loss: 0.1224\nEpoch 3/10\ntrain_loss: 0.1153 - val_loss: 0.1224\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1143\nEpoch 1: val_loss improved from 0.12236 to 0.12230, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1143 - val_loss: 0.1223\nEpoch 4/10\ntrain_loss: 0.1152 - val_loss: 0.1223\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1142\nEpoch 1: val_loss improved from 0.12230 to 0.12225, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1142 - val_loss: 0.1222\nEpoch 5/10\ntrain_loss: 0.1151 - val_loss: 0.1222\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1141\nEpoch 1: val_loss improved from 0.12225 to 0.12220, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1141 - val_loss: 0.1222\nEpoch 6/10\ntrain_loss: 0.1150 - val_loss: 0.1222\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1140\nEpoch 1: val_loss improved from 0.12220 to 0.12214, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1140 - val_loss: 0.1221\nEpoch 7/10\ntrain_loss: 0.1149 - val_loss: 0.1221\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1139\nEpoch 1: val_loss improved from 0.12214 to 0.12209, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1139 - val_loss: 0.1221\nEpoch 8/10\ntrain_loss: 0.1149 - val_loss: 0.1221\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1138\nEpoch 1: val_loss improved from 0.12209 to 0.12204, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1138 - val_loss: 0.1220\nEpoch 9/10\ntrain_loss: 0.1148 - val_loss: 0.1220\n\u001b[1m714/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1137\nEpoch 1: val_loss improved from 0.12204 to 0.12199, saving model to sentiment_model_checkpoint.weights.h5\n\u001b[1m715/715\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1137 - val_loss: 0.1220\nEpoch 10/10\ntrain_loss: 0.1147 - val_loss: 0.1220\n","output_type":"stream"}]},{"cell_type":"code","source":"test_data = encode_data(tokenized_datasets['test'], np.reshape(y_test, (-1, 16)))\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T11:26:07.690481Z","iopub.execute_input":"2024-05-26T11:26:07.690876Z","iopub.status.idle":"2024-05-26T11:26:09.454992Z","shell.execute_reply.started":"2024-05-26T11:26:07.690835Z","shell.execute_reply":"2024-05-26T11:26:09.453779Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test dataset\ntest_loss= model.evaluate(test_dataset)\nprint(f\"Test Loss: {test_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:42:50.893744Z","iopub.execute_input":"2024-05-26T13:42:50.895086Z","iopub.status.idle":"2024-05-26T13:44:12.812338Z","shell.execute_reply.started":"2024-05-26T13:42:50.895044Z","shell.execute_reply":"2024-05-26T13:44:12.811229Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"\u001b[1m198/198\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - loss: 0.1278\nTest Loss: 0.1264\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Reload the model","metadata":{}},{"cell_type":"code","source":"reloaded_model = create_model(optimizer)\nreloaded_model.load_weights('/kaggle/working/sentiment_model_checkpoint.weights.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:12.814295Z","iopub.execute_input":"2024-05-26T13:44:12.814624Z","iopub.status.idle":"2024-05-26T13:44:25.637384Z","shell.execute_reply.started":"2024-05-26T13:44:12.814593Z","shell.execute_reply":"2024-05-26T13:44:25.636113Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ input_ids           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ attention_mask      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ token_type_ids      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ -                 ‚îÇ\n‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ bert_layer_2        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m768\u001b[0m)  ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  ‚îÇ\n‚îÇ (\u001b[38;5;33mBertLayer\u001b[0m)         ‚îÇ                   ‚îÇ            ‚îÇ attention_mask[\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ token_type_ids[\u001b[38;5;34m0\u001b[0m‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ get_item_2          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ bert_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n‚îÇ (\u001b[38;5;33mGetItem\u001b[0m)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ FACILITY (\u001b[38;5;33mDense\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ LECTURER (\u001b[38;5;33mDense\u001b[0m)    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ OTHERS (\u001b[38;5;33mDense\u001b[0m)      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ TRAINING_PROGRAM    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         ‚îÇ      \u001b[38;5;34m3,076\u001b[0m ‚îÇ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  ‚îÇ\n‚îÇ (\u001b[38;5;33mDense\u001b[0m)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ concatenate_2       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        ‚îÇ          \u001b[38;5;34m0\u001b[0m ‚îÇ FACILITY[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   ‚îÇ\n‚îÇ (\u001b[38;5;33mConcatenate\u001b[0m)       ‚îÇ                   ‚îÇ            ‚îÇ LECTURER[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ OTHERS[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ TRAINING_PROGRAM‚Ä¶ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\"> Layer (type)        </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape      </span>‚îÉ<span style=\"font-weight: bold\">    Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to      </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ input_ids           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ attention_mask      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ token_type_ids      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                 ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ bert_layer_2        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         ‚îÇ                   ‚îÇ            ‚îÇ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ get_item_2          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ bert_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ FACILITY (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ LECTURER (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ OTHERS (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ TRAINING_PROGRAM    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> ‚îÇ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             ‚îÇ                   ‚îÇ            ‚îÇ                   ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ concatenate_2       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ FACILITY[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   ‚îÇ\n‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       ‚îÇ                   ‚îÇ            ‚îÇ LECTURER[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ OTHERS[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     ‚îÇ\n‚îÇ                     ‚îÇ                   ‚îÇ            ‚îÇ TRAINING_PROGRAM‚Ä¶ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,914\u001b[0m (144.20 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,914</span> (144.20 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m24,610\u001b[0m (96.14 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,610</span> (96.14 KB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(reloaded_model, to_file='architecture.png', rankdir='LR', dpi=52, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:25.638790Z","iopub.execute_input":"2024-05-26T13:44:25.639151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Ti·∫øn h√†nh d·ª± ƒëo√°n\ndef predict_sentiment(text, loaded_model):\n    inputs = tokenizer(text, max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf')\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n    token_type_ids = inputs['token_type_ids']\n    predictions = loaded_model.predict([input_ids, attention_mask, token_type_ids])\n    return predictions\n\ndef print_acsa_pred(predictions):\n    # Gi·∫£i th√≠ch k·∫øt qu·∫£\n    topics = [\"FACILITY\", \"LECTURER\", \"OTHERS\", \"TRAINING_PROGRAMS\"]\n    sentiments = [\"None\", \"Positive\", \"Negative\", \"Neutral\"] \n    y_pred = predictions.reshape(len(predictions), -1, 4)\n    for i in range(4):\n        sentiment_index = np.argmax(y_pred[0][i])\n        topic = topics[i]\n        sentiment = sentiments[sentiment_index]\n        if sentiment != \"None\":\n            print(f\"{topic}: {sentiment}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def report(predictions):\n    sentiment_indices = []\n\n    # Reshape predictions to match the expected format\n    y_pred = predictions.reshape(len(predictions), -1, 4)\n\n    # Iterate over each category\n    for i in range(4):\n        # Get the sentiment index for the current category\n        sentiment_index = np.argmax(y_pred[0][i])\n\n        # Append the sentiment index to the list\n        sentiment_indices.append(sentiment_index)\n\n    # Return the list of sentiment indices for each category\n    return sentiment_indices\n","metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:25.748662Z","iopub.status.idle":"2024-05-26T13:44:25.756617Z","shell.execute_reply.started":"2024-05-26T13:44:25.748627Z","shell.execute_reply":"2024-05-26T13:44:25.755563Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# V√≠ d·ª• s·ª≠ d·ª•ng\ntext = \"gi·∫£ng vi√™n r·∫•t nhi·ªát t√¨nh v√† d·ªÖ hi·ªÉu\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:25.757913Z","iopub.execute_input":"2024-05-26T13:44:25.758768Z","iopub.status.idle":"2024-05-26T13:44:29.264203Z","shell.execute_reply.started":"2024-05-26T13:44:25.758740Z","shell.execute_reply":"2024-05-26T13:44:29.262939Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"W0000 00:00:1716731067.690088     158 assert_op.cc:38] Ignoring Assert operator functional_5_1/bert_layer_2_1/tf_roberta_model_2/roberta/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n[[9.9983847e-01 2.8307688e-05 1.2431138e-04 8.7628086e-06 4.6013118e-04\n  9.7507465e-01 2.4146359e-02 3.1887632e-04 9.9894303e-01 6.7140500e-04\n  2.9901156e-04 8.6607928e-05 9.9920678e-01 3.2926607e-04 3.7895472e-04\n  8.5075684e-05]]\n[[9.9983847e-01 2.8307688e-05 1.2431138e-04 8.7628086e-06]\n [4.6013118e-04 9.7507465e-01 2.4146359e-02 3.1887632e-04]\n [9.9894303e-01 6.7140500e-04 2.9901156e-04 8.6607928e-05]\n [9.9920678e-01 3.2926607e-04 3.7895472e-04 8.5075684e-05]]\nLECTURER: Positive\n[0, 1, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# V√≠ d·ª• s·ª≠ d·ª•ng\ntext = \"d·∫°y ch√°n, kh√¥ng quan t√¢m h·ªçc sinh\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.265495Z","iopub.execute_input":"2024-05-26T13:44:29.267540Z","iopub.status.idle":"2024-05-26T13:44:29.382356Z","shell.execute_reply.started":"2024-05-26T13:44:29.267495Z","shell.execute_reply":"2024-05-26T13:44:29.381041Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n[[0.99337995 0.00157052 0.00389787 0.00115165 0.05584941 0.21189219\n  0.7064535  0.02580487 0.9671745  0.0112598  0.01326553 0.00830019\n  0.96330786 0.00729772 0.02645061 0.00294381]]\n[[0.99337995 0.00157052 0.00389787 0.00115165]\n [0.05584941 0.21189219 0.7064535  0.02580487]\n [0.9671745  0.0112598  0.01326553 0.00830019]\n [0.96330786 0.00729772 0.02645061 0.00294381]]\nLECTURER: Negative\n[0, 2, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# V√≠ d·ª• s·ª≠ d·ª•ng\ntext = \"ch∆∞∆°ng tr√¨nh h·ªçc kh√¥ khan, kh√≥ hi·ªÉu\"\npredictions = predict_sentiment(text, reloaded_model)\nprint_acsa_pred(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.383627Z","iopub.execute_input":"2024-05-26T13:44:29.383993Z","iopub.status.idle":"2024-05-26T13:44:29.488725Z","shell.execute_reply.started":"2024-05-26T13:44:29.383959Z","shell.execute_reply":"2024-05-26T13:44:29.487614Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# V√≠ d·ª• s·ª≠ d·ª•ng\ntext = \"ph√≤ng h·ªçc d∆° b·∫©n, ·∫©m m·ªëc, v√† c·ª±c k·ª≥ t·ªëi\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.491052Z","iopub.execute_input":"2024-05-26T13:44:29.493225Z","iopub.status.idle":"2024-05-26T13:44:29.604059Z","shell.execute_reply.started":"2024-05-26T13:44:29.493194Z","shell.execute_reply":"2024-05-26T13:44:29.602730Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n[[0.09857271 0.01481084 0.88182676 0.00478971 0.6561089  0.13309951\n  0.19698283 0.01380874 0.94935995 0.01791392 0.02512837 0.00759772\n  0.91343355 0.04684234 0.03586247 0.00386167]]\n[[0.09857271 0.01481084 0.88182676 0.00478971]\n [0.6561089  0.13309951 0.19698283 0.01380874]\n [0.94935995 0.01791392 0.02512837 0.00759772]\n [0.91343355 0.04684234 0.03586247 0.00386167]]\nFACILITY: Negative\n[2, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"# V√≠ d·ª• s·ª≠ d·ª•ng\ntext = \"s·ª£ ma\"\npredictions = predict_sentiment(text, reloaded_model)\nprint(predictions)\nprint(predictions.reshape(y_test[1].shape))\nprint_acsa_pred(predictions)\nprint(report(predictions))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:44:29.609058Z","iopub.execute_input":"2024-05-26T13:44:29.609473Z","iopub.status.idle":"2024-05-26T13:44:29.722344Z","shell.execute_reply.started":"2024-05-26T13:44:29.609421Z","shell.execute_reply":"2024-05-26T13:44:29.721415Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n[[0.9393128  0.00159395 0.05603147 0.00306187 0.49649248 0.1425621\n  0.2945748  0.06637062 0.75500363 0.06792098 0.07609788 0.10097753\n  0.8580513  0.07133412 0.03397258 0.03664197]]\n[[0.9393128  0.00159395 0.05603147 0.00306187]\n [0.49649248 0.1425621  0.2945748  0.06637062]\n [0.75500363 0.06792098 0.07609788 0.10097753]\n [0.8580513  0.07133412 0.03397258 0.03664197]]\n[0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"y_test_argmax = np.argmax(y_test, axis=-1)\ny_test_argmax","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:45:23.951260Z","iopub.execute_input":"2024-05-26T13:45:23.951668Z","iopub.status.idle":"2024-05-26T13:45:23.962395Z","shell.execute_reply.started":"2024-05-26T13:45:23.951637Z","shell.execute_reply":"2024-05-26T13:45:23.961034Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"array([[0, 1, 0, 0],\n       [0, 1, 0, 0],\n       [0, 1, 0, 0],\n       ...,\n       [0, 1, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 0, 3]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Predict on test data","metadata":{}},{"cell_type":"code","source":"def predict(model, inputs, batch_size=1, verbose=0):\n    y_pred = model.predict(inputs, batch_size=batch_size, verbose=verbose)\n    y_pred = y_pred.reshape(len(y_pred), -1, 4)\n    return np.argmax(y_pred, axis=-1) # sentiment values (position that have max value)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:00:02.379323Z","iopub.execute_input":"2024-05-26T14:00:02.379732Z","iopub.status.idle":"2024-05-26T14:00:02.387422Z","shell.execute_reply.started":"2024-05-26T14:00:02.379697Z","shell.execute_reply":"2024-05-26T14:00:02.386193Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"def print_acsa_pred(replacements, categories, sentence_pred):\n    sentiments = map(lambda x: replacements[x], sentence_pred)\n    for category, sentiment in zip(categories, sentiments): \n        if sentiment: print(f'=> {category},{sentiment}')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:00:05.647478Z","iopub.execute_input":"2024-05-26T14:00:05.648171Z","iopub.status.idle":"2024-05-26T14:00:05.654949Z","shell.execute_reply.started":"2024-05-26T14:00:05.648135Z","shell.execute_reply":"2024-05-26T14:00:05.653694Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"y_pred = predict(reloaded_model, test_dataset, BATCH_SIZE, verbose=1)\nreloaded_model.evaluate(test_dataset, batch_size=BATCH_SIZE, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:01:54.844285Z","iopub.execute_input":"2024-05-26T15:01:54.844735Z","iopub.status.idle":"2024-05-26T15:03:18.271021Z","shell.execute_reply.started":"2024-05-26T15:01:54.844701Z","shell.execute_reply":"2024-05-26T15:03:18.269964Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"\u001b[1m198/198\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 212ms/step\n\u001b[1m198/198\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - loss: 0.1278\n","output_type":"stream"},{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"0.12644782662391663"},"metadata":{}}]},{"cell_type":"code","source":"y_test[1]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:03:18.273033Z","iopub.execute_input":"2024-05-26T15:03:18.273345Z","iopub.status.idle":"2024-05-26T15:03:18.281965Z","shell.execute_reply.started":"2024-05-26T15:03:18.273318Z","shell.execute_reply":"2024-05-26T15:03:18.280445Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"y_test_argmax[1]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:04:22.706657Z","iopub.execute_input":"2024-05-26T15:04:22.707506Z","iopub.status.idle":"2024-05-26T15:04:22.715583Z","shell.execute_reply.started":"2024-05-26T15:04:22.707464Z","shell.execute_reply":"2024-05-26T15:04:22.714320Z"},"trusted":true},"execution_count":125,"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 0, 0])"},"metadata":{}}]},{"cell_type":"code","source":"y_pred[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T15:03:18.283509Z","iopub.execute_input":"2024-05-26T15:03:18.283968Z","iopub.status.idle":"2024-05-26T15:03:18.294356Z","shell.execute_reply.started":"2024-05-26T15:03:18.283924Z","shell.execute_reply":"2024-05-26T15:03:18.293288Z"},"trusted":true},"execution_count":124,"outputs":[{"execution_count":124,"output_type":"execute_result","data":{"text/plain":"array([0, 0, 0, 0])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"replacements = {0: None, 1: 'positive', 2: 'negative', 3: 'neutral'}\ncategories = test_df.columns[1:]\nfor i in range(50):\n    print('Example:', test_df['Sentence'][i])\n    print_acsa_pred(replacements, categories, y_pred[i])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:03:40.065202Z","iopub.execute_input":"2024-05-26T14:03:40.065592Z","iopub.status.idle":"2024-05-26T14:03:40.098122Z","shell.execute_reply.started":"2024-05-26T14:03:40.065561Z","shell.execute_reply":"2024-05-26T14:03:40.096927Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"Example: n√≥i ti·∫øng anh l∆∞u lo√°t \nExample: gi√°o vi√™n r·∫•t vui t√≠nh \n=> LECTURER,positive\nExample: c√¥ max c√≥ t√¢m \n=> LECTURER,positive\nExample: gi·∫£ng b√†i thu h√∫t  d√≠ d·ªèm \n=> LECTURER,negative\nExample: gi√°o vi√™n kh√¥ng gi·∫£ng d·∫°y ki·∫øn th·ª©c  h∆∞·ªõng d·∫´n th·ª±c h√†nh trong qu√° tr√¨nh h·ªçc \n=> LECTURER,negative\nExample: th·∫ßy d·∫°y nhi·ªát t√¨nh v√† t√¢m huy·∫øt \n=> LECTURER,positive\nExample: t√≠nh ƒëi·ªÉm thi ƒëua c√°c nh√≥m \n=> TRAINING_PROGRAM,negative\nExample: th·∫ßy nhi·ªát t√¨nh gi·∫£ng l·∫°i cho h·ªçc sinh \n=> LECTURER,positive\nExample: c√≥ ƒë√¥i l√∫c n√≥i h∆°i nhanh l√†m sinh vi√™n kh√¥ng theo k·ªãp \n=> LECTURER,negative\nExample: gi·∫£ng d·∫°y nhi·ªát t√¨nh  li√™n h·ªá th·ª±c t·∫ø kh√° nhi·ªÅu  t∆∞∆°ng t√°c v·ªõi sinh vi√™n t∆∞∆°ng ƒë·ªëi t·ªët \n=> LECTURER,positive\nExample: gi·∫£ng vi√™n nhi·ªát t√¨nh trong c√¥ng t√°c gi·∫£ng d·∫°y \n=> LECTURER,positive\nExample: c√¥ r·∫•t nhi·ªát t√¨nh  d·ªÖ th∆∞∆°ng  d·∫°y d·ªÖ hi·ªÉu \n=> LECTURER,positive\nExample: trong tr∆∞·ªùng macbook th·∫ßy s·ªë hai th√¨ kh√¥ng c√≥ m√°y n√†o s·ªë m·ªôt \n=> LECTURER,negative\nExample: sinh vi√™n kh√¥ng ti·∫øp thu k·ªãp c≈©ng nh∆∞ kh√¥ng hi·ªÉu g√¨ \n=> LECTURER,negative\nExample: th·∫ßy nhi·ªát t√¨nh gi√∫p ƒë·ª° sinh vi√™n trong qu√° tr√¨nh th·ª±c h√†nh \n=> LECTURER,positive\nExample: th·∫ßy r·∫•t t·∫≠n t√¨nh v√† gi√∫p ƒë·ª° sinh vi√™n r·∫•t nhi·ªÅu \n=> LECTURER,positive\nExample: gi·∫£ng vi√™n gi·∫£i th√≠ch k·ªπ v√† chi ti·∫øt \n=> LECTURER,positive\nExample: th·∫ßy d·∫°y r·∫•t chi ti·∫øt  l√Ω thuy·∫øt ƒë·∫ßy ƒë·ªß \n=> LECTURER,positive\nExample: c√≤n nh·ªØng ph·∫ßn t√¨m bao ƒë√≥ng  ch·ª©ng minh d·∫°ng chu·∫©n ch∆∞a l√†m r√µ \n=> TRAINING_PROGRAM,negative\nExample: cung c·∫•p ki·∫øn th·ª©c b·ªï √≠ch \n=> LECTURER,positive\nExample: b·∫Øt ƒë·∫ßu bu·ªïi h·ªçc ƒë√∫ng gi·ªù \nExample: gi·∫£ng d·∫°y d·ªÖ hi·ªÉu  t·∫≠n t√¢m \n=> LECTURER,positive\nExample: gi·∫£ng vi√™n nhi·ªát t√¨nh  gi·∫£i ƒë√°p th·∫Øc m·∫Øc ƒë·∫ßy ƒë·ªß \n=> LECTURER,positive\nExample: n√™n ƒë∆∞a ra m·ªôt v√†i ph∆∞∆°ng ph√°p h·ªçc l·∫≠p tr√¨nh hay cho sinh vi√™n \n=> TRAINING_PROGRAM,negative\nExample: gi·∫£ng vi√™n d·∫°y s√¥i n·ªïi \n=> LECTURER,positive\nExample: em h·ªçc b√™n ch·∫•t l∆∞·ª£ng cao m√† ph√≤ng m√°y kh√¥ng cung c·∫•p ƒë·ªß m√°y  v√¨ m√°y h∆∞ ho·∫∑c kh√¥ng c√†i ch∆∞∆°ng tr√¨nh  \n=> FACILITY,negative\nExample: ch∆∞a gi·ªèi chuy√™n m√¥n cho l·∫Øm \nExample: c√°ch m√† c√¥ ti·∫øp c·∫≠n v·ªõi sinh vi√™n \n=> LECTURER,positive\nExample: tr·ª£ gi√†ng nhi·ªát t√¨nh  quan t√¢m v√† theo s√°t sinh vi√™n \n=> LECTURER,positive\nExample: gi·∫£ng vi√™n t·∫≠n t√¢m  g·∫ßn g≈©i v·ªõi h·ªçc sinh \n=> LECTURER,positive\nExample: th·∫ßy d·∫°y r·∫•t nhi·ªát t√¨nh v√† gi·∫£ng k·ªπ b√†i kh√¥ng c·∫ßn c·∫£i thi·ªán g√¨ c·∫£ \n=> LECTURER,positive\nExample: ph√≤ng h·ªçc tho√°ng m√°t  trang thi·∫øt b·ªã ƒë·∫ßy ƒë·ªß \n=> FACILITY,negative\nExample: th·∫ßy r·∫•t nhi·ªát t√¨nh th√¢n thi·ªán v√† vui t√≠nh c√°ch gi·∫£ng d·∫°y r·∫•t d·ªÖ hi·ªÉu \n=> LECTURER,positive\nExample: kh√¥ng nhi·ªát t√¨nh ch·ªâ d·∫´n v√† lu√¥n g√¢y kh√≥ khƒÉn cho sinh vi√™n \n=> LECTURER,negative\nExample: gi·ªØa l√Ω thuy·∫øt t·ª´ v·ª±ng v·ªõi tr√≤ ch∆°i ƒë·ªÉ d·ªÖ ti·∫øp thu \nExample: m√¥n h·ªçc n√†y gi√∫p ch√∫ng em hi·ªÉu ra nh·ªØng v·∫•n ƒë·ªÅ c∆° b·∫£n \n=> TRAINING_PROGRAM,negative\nExample: gi√°o tr√¨nh ch∆∞a c√≥ h·ª£p l√Ω \nExample: mong th·∫ßy xem x√©t l·∫°i vi·ªác n·ªôp b√†i \n=> LECTURER,negative\nExample: c√¥ wzjwz234  d·∫°y r·∫•t nhi·ªát t√≠nh  t·∫≠n t√¢m t·∫≠n l·ª±c  gi·∫£i th√≠ch t·ª´ng c√¢u code gi√∫p h·ªçc sinh hi·ªÉu r√µ v·∫•n ƒë·ªÅ  r·∫•t th√≠ch c√¥ d·∫°y m√¥n h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng n√†y \n=> LECTURER,positive\nExample: cung c·∫•p b√†i t·∫≠p ƒëa d·∫°ng \nExample: hay g√µ micro v√†o b·∫£ng ho·∫∑c b√†n \nExample: ·ªïn \nExample: t·ªëc ƒë·ªô d·∫°y c·ªßa gi·∫£ng vi√™n nhanh  n√™n c√≥ m·ªôt s·ªë n·ªôi dung kh√¥ng cung c·∫•p ƒë·∫ßy ƒë·ªß ki·∫øn th·ª©c \n=> LECTURER,negative\nExample: c√¥ vui t√≠nh  h·ªçc kh√¥ng √°p l·ª±c \n=> LECTURER,positive\nExample: c√¥ d·∫°y r·∫•t t·ªët v√† nhi·ªát t√¨nh \n=> LECTURER,positive\nExample: ph·∫ßn l·ªõn ch·ªâ l√† l√Ω thuy·∫øt v√† b√†i t·∫≠p \n=> TRAINING_PROGRAM,negative\nExample: th·∫ßy nhi·ªát t√¨nh tr·∫£ l·ªùi th·∫Øc m·∫Øc c·ªßa sinh vi√™n \n=> LECTURER,positive\nExample: thi·∫øt b·ªã ph√≤ng h·ªçc nh∆∞ qu·∫°t m√°y c·∫ßn ph·∫£i ƒë·∫ßu t∆∞ th√™m \n=> FACILITY,negative\nExample: c√¥ nhi·ªát t√¨nh  gi·∫£ng b√†i hi·ªáu qu·∫£ \n=> LECTURER,positive\nExample: khi ngh·ªâ h·ªçc  c·∫ßn th√¥ng b√°o k·ªãp th·ªùi tr√™n web c·ªßa khoa wzjwz148 \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Report","metadata":{}},{"cell_type":"code","source":"aspect_test = []\naspect_pred = []\n\nfor row_test, row_pred in zip(y_test_argmax, y_pred):\n    for index, (col_test, col_pred) in enumerate(zip(row_test, row_pred)):\n        aspect_test.append(bool(col_test) * categories[index])\n        aspect_pred.append(bool(col_pred) * categories[index])\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:04:44.900404Z","iopub.execute_input":"2024-05-26T14:04:44.901375Z","iopub.status.idle":"2024-05-26T14:04:44.963981Z","shell.execute_reply.started":"2024-05-26T14:04:44.901338Z","shell.execute_reply":"2024-05-26T14:04:44.962858Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\naspect_report = classification_report(aspect_test, aspect_pred, digits=4, zero_division=1, output_dict=True)\nprint(classification_report(aspect_test, aspect_pred, digits=4, zero_division=1))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:04:58.503417Z","iopub.execute_input":"2024-05-26T14:04:58.504649Z","iopub.status.idle":"2024-05-26T14:04:58.995939Z","shell.execute_reply.started":"2024-05-26T14:04:58.504603Z","shell.execute_reply":"2024-05-26T14:04:58.994869Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"                  precision    recall  f1-score   support\n\n                     0.9275    0.9707    0.9486      9498\n        FACILITY     0.9619    0.6966    0.8080       145\n        LECTURER     0.9260    0.8856    0.9054      2290\n          OTHERS     0.6800    0.1069    0.1848       159\nTRAINING_PROGRAM     0.7419    0.5227    0.6133       572\n\n        accuracy                         0.9211     12664\n       macro avg     0.8475    0.6365    0.6920     12664\n    weighted avg     0.9161    0.9211    0.9144     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test_flat = y_test_argmax.flatten()\ny_pred_flat = y_pred.flatten()\ntarget_names = list(map(str, replacements.values()))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:05:30.522266Z","iopub.execute_input":"2024-05-26T14:05:30.522713Z","iopub.status.idle":"2024-05-26T14:05:30.530159Z","shell.execute_reply.started":"2024-05-26T14:05:30.522679Z","shell.execute_reply":"2024-05-26T14:05:30.528741Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"polarity_report = classification_report(y_test_flat, y_pred_flat, digits=4, output_dict=True)\nprint(classification_report(y_test_flat, y_pred_flat, target_names=target_names, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:05:32.784389Z","iopub.execute_input":"2024-05-26T14:05:32.785082Z","iopub.status.idle":"2024-05-26T14:05:32.859834Z","shell.execute_reply.started":"2024-05-26T14:05:32.785045Z","shell.execute_reply":"2024-05-26T14:05:32.858606Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        None     0.9275    0.9707    0.9486      9498\n    positive     0.8517    0.8088    0.8297      1590\n    negative     0.7481    0.6366    0.6879      1409\n     neutral     0.5714    0.0479    0.0884       167\n\n    accuracy                         0.9011     12664\n   macro avg     0.7747    0.6160    0.6386     12664\nweighted avg     0.8933    0.9011    0.8933     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"aspect_polarity_test = []\naspect_polarity_pred = []\n\nfor row_test, row_pred in zip(y_test_argmax, y_pred):\n    for index, (col_test, col_pred) in enumerate(zip(row_test, row_pred)):\n        aspect_polarity_test.append(f'{categories[index]},{replacements[col_test]}')\n        aspect_polarity_pred.append(f'{categories[index]},{replacements[col_pred]}')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:01.963074Z","iopub.execute_input":"2024-05-26T14:06:01.963962Z","iopub.status.idle":"2024-05-26T14:06:02.040019Z","shell.execute_reply.started":"2024-05-26T14:06:01.963921Z","shell.execute_reply":"2024-05-26T14:06:02.038689Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"aspect_polarity_report = classification_report(aspect_polarity_test, aspect_polarity_pred, digits=4, zero_division=1, output_dict=True)\nprint(classification_report(aspect_polarity_test, aspect_polarity_pred, digits=4, zero_division=1))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:04.445490Z","iopub.execute_input":"2024-05-26T14:06:04.446055Z","iopub.status.idle":"2024-05-26T14:06:04.990759Z","shell.execute_reply.started":"2024-05-26T14:06:04.446009Z","shell.execute_reply":"2024-05-26T14:06:04.989592Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"                           precision    recall  f1-score   support\n\n            FACILITY,None     0.9856    0.9987    0.9921      3021\n        FACILITY,negative     0.9429    0.7174    0.8148       138\n         FACILITY,neutral     1.0000    0.0000    0.0000         2\n        FACILITY,positive     1.0000    0.0000    0.0000         5\n            LECTURER,None     0.7316    0.8151    0.7711       876\n        LECTURER,negative     0.7636    0.6941    0.7272       791\n         LECTURER,neutral     1.0000    0.0135    0.0267        74\n        LECTURER,positive     0.8619    0.8891    0.8753      1425\n              OTHERS,None     0.9548    0.9973    0.9756      3007\n          OTHERS,negative     0.4000    0.0312    0.0580        64\n           OTHERS,neutral     0.5385    0.1591    0.2456        44\n          OTHERS,positive     0.1429    0.0196    0.0345        51\n    TRAINING_PROGRAM,None     0.9012    0.9599    0.9296      2594\nTRAINING_PROGRAM,negative     0.6676    0.5938    0.6285       416\n TRAINING_PROGRAM,neutral     1.0000    0.0000    0.0000        47\nTRAINING_PROGRAM,positive     0.5455    0.1651    0.2535       109\n\n                 accuracy                         0.9011     12664\n                macro avg     0.7772    0.4409    0.4583     12664\n             weighted avg     0.8932    0.9011    0.8891     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"aspect_dict = aspect_report['macro avg']\naspect_dict['accuracy'] = aspect_report['accuracy']\n\npolarity_dict  = polarity_report['macro avg']\npolarity_dict['accuracy'] = polarity_report['accuracy']\n\naspect_polarity_dict = aspect_polarity_report['macro avg']\naspect_polarity_dict['accuracy'] = aspect_polarity_report['accuracy']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:20.218230Z","iopub.execute_input":"2024-05-26T14:06:20.219107Z","iopub.status.idle":"2024-05-26T14:06:20.227089Z","shell.execute_reply.started":"2024-05-26T14:06:20.219059Z","shell.execute_reply":"2024-05-26T14:06:20.225873Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"df_report = pd.DataFrame.from_dict([aspect_dict, polarity_dict, aspect_polarity_dict])\ndf_report.index = ['Aspect Detection', 'Polarity Detection', 'Aspect + Polarity']\ndf_report.drop('support', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:06:30.426845Z","iopub.execute_input":"2024-05-26T14:06:30.427238Z","iopub.status.idle":"2024-05-26T14:06:30.453822Z","shell.execute_reply.started":"2024-05-26T14:06:30.427206Z","shell.execute_reply":"2024-05-26T14:06:30.452868Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"                    precision    recall  f1-score  accuracy\nAspect Detection     0.847468  0.636503  0.692016  0.921115\nPolarity Detection   0.774670  0.616015  0.638642  0.901058\nAspect + Polarity    0.777240  0.440870  0.458276  0.901058","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1-score</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Aspect Detection</th>\n      <td>0.847468</td>\n      <td>0.636503</td>\n      <td>0.692016</td>\n      <td>0.921115</td>\n    </tr>\n    <tr>\n      <th>Polarity Detection</th>\n      <td>0.774670</td>\n      <td>0.616015</td>\n      <td>0.638642</td>\n      <td>0.901058</td>\n    </tr>\n    <tr>\n      <th>Aspect + Polarity</th>\n      <td>0.777240</td>\n      <td>0.440870</td>\n      <td>0.458276</td>\n      <td>0.901058</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"example_input = text_preprocess(input('Enter your sentence: '))\ntokenized_input = tokenizer(example_input, padding='max_length', truncation=True)\nfeatures = {x: [[tokenized_input[x]]] for x in tokenizer.model_input_names}\n\npred = predict(reloaded_model, tf.data.Dataset.from_tensor_slices(features))\nprint_acsa_pred(replacements, categories, pred[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:57:46.020371Z","iopub.execute_input":"2024-05-26T14:57:46.020827Z","iopub.status.idle":"2024-05-26T14:57:52.266145Z","shell.execute_reply.started":"2024-05-26T14:57:46.020792Z","shell.execute_reply":"2024-05-26T14:57:52.265043Z"},"trusted":true},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your sentence:  s·ª£ ma qu√°\n"}]}]}