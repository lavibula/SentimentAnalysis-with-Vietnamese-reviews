{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# from torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:26:05.151891Z","iopub.execute_input":"2024-06-02T14:26:05.152665Z","iopub.status.idle":"2024-06-02T14:26:11.320165Z","shell.execute_reply.started":"2024-06-02T14:26:05.152634Z","shell.execute_reply":"2024-06-02T14:26:11.319242Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Number of classes in MNIST\nnum_classes = 4\n\n# Number of training epochs (arbitrary)\nepochs = 5\n\n# Training parameters\nlearning_rate = 1e-6\nbatch_size = 16\ndisplay_step = 100\n\n# Model path\n# checkpoint = 'model.pth'\n\n# device: cuda\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:26:11.321617Z","iopub.execute_input":"2024-06-02T14:26:11.322146Z","iopub.status.idle":"2024-06-02T14:26:11.388801Z","shell.execute_reply.started":"2024-06-02T14:26:11.322107Z","shell.execute_reply":"2024-06-02T14:26:11.388041Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:45:33.239654Z","iopub.status.idle":"2024-06-02T14:45:33.240160Z","shell.execute_reply.started":"2024-06-02T14:45:33.239896Z","shell.execute_reply":"2024-06-02T14:45:33.239920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-02T14:26:17.490063Z","iopub.execute_input":"2024-06-02T14:26:17.490368Z","iopub.status.idle":"2024-06-02T14:26:19.048886Z","shell.execute_reply.started":"2024-06-02T14:26:17.490337Z","shell.execute_reply":"2024-06-02T14:26:19.048100Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Install the vncorenlp python wrapper\n!pip install vncorenlp\n\n# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)\n!mkdir -p vncorenlp/models/wordsegmenter\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp/\n!mv vi-vocab vncorenlp/models/wordsegmenter/\n!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:26:19.049951Z","iopub.execute_input":"2024-06-02T14:26:19.050388Z","iopub.status.idle":"2024-06-02T14:26:43.890783Z","shell.execute_reply.started":"2024-06-02T14:26:19.050362Z","shell.execute_reply":"2024-06-02T14:26:43.889563Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting vncorenlp\n  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vncorenlp) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (2024.2.2)\nBuilding wheels for collected packages: vncorenlp\n  Building wheel for vncorenlp (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645932 sha256=842f2a38230f339772786a74627145cce4c88996b654e1ec3a896388d478693e\n  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\nSuccessfully built vncorenlp\nInstalling collected packages: vncorenlp\nSuccessfully installed vncorenlp-1.0.3\n--2024-06-02 14:26:37--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 27412575 (26M) [application/octet-stream]\nSaving to: 'VnCoreNLP-1.1.1.jar'\n\nVnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   167MB/s    in 0.2s    \n\n2024-06-02 14:26:38 (167 MB/s) - 'VnCoreNLP-1.1.1.jar' saved [27412575/27412575]\n\n--2024-06-02 14:26:39--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 526544 (514K) [application/octet-stream]\nSaving to: 'vi-vocab'\n\nvi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.05s   \n\n2024-06-02 14:26:39 (10.5 MB/s) - 'vi-vocab' saved [526544/526544]\n\n--2024-06-02 14:26:40--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 128508 (125K) [text/plain]\nSaving to: 'wordsegmenter.rdr'\n\nwordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.03s   \n\n2024-06-02 14:26:40 (4.46 MB/s) - 'wordsegmenter.rdr' saved [128508/128508]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\"/kaggle/working/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n\ntext = \"Đại học Bách Khoa Hà Nội.\"\n\nword_segmented_text = rdrsegmenter.tokenize(text)\nprint(word_segmented_text)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:26:43.892363Z","iopub.execute_input":"2024-06-02T14:26:43.892714Z","iopub.status.idle":"2024-06-02T14:26:49.531099Z","shell.execute_reply.started":"2024-06-02T14:26:43.892680Z","shell.execute_reply":"2024-06-02T14:26:49.530203Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[['Đại_học', 'Bách_Khoa', 'Hà_Nội', '.']]\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ntrain_path = 'https://raw.githubusercontent.com/lavibula/SentimentAnalysis-with-Vietnamese-reviews/main/absa_data/train.csv'\ntest_path = 'https://raw.githubusercontent.com/lavibula/SentimentAnalysis-with-Vietnamese-reviews/main/absa_data/val.csv'\n\ntrain_set = pd.read_csv(train_path)\ntest_set = pd.read_csv(test_path)\n\ntrain_set['Sentence'] = train_set['Sentence'].apply(rdrsegmenter.tokenize).apply(lambda x: ' '.join(x[0]))\ntest_set['Sentence'] = test_set['Sentence'].apply(rdrsegmenter.tokenize).apply(lambda x: ' '.join(x[0]))\n\ntrain_set = pd.concat([train_set, test_set]).reset_index().drop(columns = 'index')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:26:49.532136Z","iopub.execute_input":"2024-06-02T14:26:49.532470Z","iopub.status.idle":"2024-06-02T14:27:17.585529Z","shell.execute_reply.started":"2024-06-02T14:26:49.532440Z","shell.execute_reply":"2024-06-02T14:27:17.584435Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train_set, test_set]).reset_index().drop(columns = 'index')\ndf","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:17.586850Z","iopub.execute_input":"2024-06-02T14:27:17.587162Z","iopub.status.idle":"2024-06-02T14:27:17.609859Z","shell.execute_reply.started":"2024-06-02T14:27:17.587135Z","shell.execute_reply":"2024-06-02T14:27:17.608923Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                Sentence  FACILITY  LECTURER  \\\n0                                slide giáo_trình đầy_đủ         0         0   \n1             nhiệt_tình giảng_dạy gần_gũi với sinh_viên         0         1   \n2                     đi học đầy_đủ full điểm chuyên_cần         0         0   \n3      chưa áp_dụng công_nghệ_thông_tin và các thiết_...         0         2   \n4      thầy giảng bài hay có nhiều bài_tập ví_dụ ngay...         0         1   \n...                                                  ...       ...       ...   \n14587                                hướng_dẫn lab mơ_hồ         0         2   \n14588  thầy cho chúng_em những bài_tập mang tính thực...         0         1   \n14589  thầy không dạy nhiều chủ_yếu cho sinh_viên tự ...         0         2   \n14590  em muốn đổi tên môn_học vì tên môn là lập_trìn...         0         0   \n14591  thầy vừa dạy vừa chat hoặc gọi điện_thoại thườ...         0         2   \n\n       OTHERS  TRAINING_PROGRAM  \n0           0                 1  \n1           0                 0  \n2           0                 2  \n3           0                 0  \n4           0                 0  \n...       ...               ...  \n14587       0                 0  \n14588       0                 0  \n14589       0                 0  \n14590       0                 2  \n14591       0                 0  \n\n[14592 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>FACILITY</th>\n      <th>LECTURER</th>\n      <th>OTHERS</th>\n      <th>TRAINING_PROGRAM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>slide giáo_trình đầy_đủ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nhiệt_tình giảng_dạy gần_gũi với sinh_viên</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>đi học đầy_đủ full điểm chuyên_cần</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chưa áp_dụng công_nghệ_thông_tin và các thiết_...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>thầy giảng bài hay có nhiều bài_tập ví_dụ ngay...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14587</th>\n      <td>hướng_dẫn lab mơ_hồ</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14588</th>\n      <td>thầy cho chúng_em những bài_tập mang tính thực...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14589</th>\n      <td>thầy không dạy nhiều chủ_yếu cho sinh_viên tự ...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14590</th>\n      <td>em muốn đổi tên môn_học vì tên môn là lập_trìn...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14591</th>\n      <td>thầy vừa dạy vừa chat hoặc gọi điện_thoại thườ...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>14592 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Sample data for illustration\n# data = train_set[['FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM']].copy()\ndata = df.copy()\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Select specific columns\nselected_columns = ['FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM']\ndata_to_encode = df[selected_columns]\n\n# Initialize OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n\n# Fit and transform the data\nencoded_data = encoder.fit_transform(data_to_encode)\n\n# Convert the result to a DataFrame for better readability\nencoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(selected_columns))\n\n# If you want to list the values\nencoded_values_list = encoded_df.values\n# print(encoded_values_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:17.611021Z","iopub.execute_input":"2024-06-02T14:27:17.611363Z","iopub.status.idle":"2024-06-02T14:27:17.653541Z","shell.execute_reply.started":"2024-06-02T14:27:17.611330Z","shell.execute_reply":"2024-06-02T14:27:17.652689Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"encoded_values_list","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:17.658591Z","iopub.execute_input":"2024-06-02T14:27:17.658870Z","iopub.status.idle":"2024-06-02T14:27:17.665486Z","shell.execute_reply.started":"2024-06-02T14:27:17.658845Z","shell.execute_reply":"2024-06-02T14:27:17.664474Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[1., 0., 0., ..., 1., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 1., 0.],\n       ...,\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 1., 0.],\n       [1., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer\n\nphobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:17.666558Z","iopub.execute_input":"2024-06-02T14:27:17.666868Z","iopub.status.idle":"2024-06-02T14:27:24.510066Z","shell.execute_reply.started":"2024-06-02T14:27:17.666843Z","shell.execute_reply":"2024-06-02T14:27:24.508994Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0c07a95b165425a8ee8655743c247ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ca9b1bd5e2848e5835865647f9542c6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed75e6a572f4156aabd583d46ebdb64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b23a3f64c7f43f182cfc76a1cdf0715"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2f9c3f734d44b7f9cfba75c9ea33b5a"}},"metadata":{}}]},{"cell_type":"code","source":"# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\nsentence = 'Chúng_tôi là những nghiên_cứu_viên .'  \n\ninput_ids = torch.tensor([tokenizer.encode(sentence)])\n\nwith torch.no_grad():\n    features = phobert(input_ids)  # Models outputs are now tuples","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:24.511288Z","iopub.execute_input":"2024-06-02T14:27:24.511757Z","iopub.status.idle":"2024-06-02T14:27:24.952558Z","shell.execute_reply.started":"2024-06-02T14:27:24.511730Z","shell.execute_reply":"2024-06-02T14:27:24.951374Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"features[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:24.954011Z","iopub.execute_input":"2024-06-02T14:27:24.954444Z","iopub.status.idle":"2024-06-02T14:27:24.960460Z","shell.execute_reply.started":"2024-06-02T14:27:24.954406Z","shell.execute_reply":"2024-06-02T14:27:24.959629Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 7, 768])"},"metadata":{}}]},{"cell_type":"code","source":"features[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:24.961568Z","iopub.execute_input":"2024-06-02T14:27:24.961836Z","iopub.status.idle":"2024-06-02T14:27:24.971887Z","shell.execute_reply.started":"2024-06-02T14:27:24.961813Z","shell.execute_reply":"2024-06-02T14:27:24.970973Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"torch.Size([7, 768])"},"metadata":{}}]},{"cell_type":"code","source":"features.last_hidden_state[:, 0, :].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:24.973076Z","iopub.execute_input":"2024-06-02T14:27:24.973364Z","iopub.status.idle":"2024-06-02T14:27:24.986394Z","shell.execute_reply.started":"2024-06-02T14:27:24.973339Z","shell.execute_reply":"2024-06-02T14:27:24.985574Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 768])"},"metadata":{}}]},{"cell_type":"code","source":"X = train_set['Sentence'][10]\nX","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:24.987389Z","iopub.execute_input":"2024-06-02T14:27:24.987656Z","iopub.status.idle":"2024-06-02T14:27:24.997086Z","shell.execute_reply.started":"2024-06-02T14:27:24.987633Z","shell.execute_reply":"2024-06-02T14:27:24.996238Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'thầy rất tận_tình và đi dạy rất đúng giờ'"},"metadata":{}}]},{"cell_type":"code","source":"encoded_values_list.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:24.998090Z","iopub.execute_input":"2024-06-02T14:27:24.998414Z","iopub.status.idle":"2024-06-02T14:27:25.007893Z","shell.execute_reply.started":"2024-06-02T14:27:24.998383Z","shell.execute_reply":"2024-06-02T14:27:25.007066Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(14592, 16)"},"metadata":{}}]},{"cell_type":"code","source":"# Tách features (câu đánh giá) và labels (cảm xúc)\nX = df['Sentence']\ny = encoded_values_list\n\n# # Chuyển đổi dữ liệu text sang vector đặc trưng bằng CountVectorizer\n# vectorizer = CountVectorizer()\n# X = vectorizer.fit_transform(X)\n\n# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tính toán embeddings cho một câu đánh giá\ndef compute_embeddings(sentence):\n    # Tokenize câu đánh giá và thêm token đặc biệt\n    # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n#     sentence = 'Chúng_tôi là những nghiên_cứu_viên .'  \n\n    input_ids = torch.tensor([tokenizer.encode(sentence)])\n\n    with torch.no_grad():\n        features = phobert(input_ids)  # Models outputs are now tuples\n    \n    # Lấy embeddings từ lớp cuối cùng (CLS token)\n    cls_embedding = features.last_hidden_state[:, 0, :]\n    \n    return cls_embedding\n\n# # Tính toán embeddings cho tất cả các câu đánh giá trong tập huấn luyện và tập kiểm tra\nX_train_embeddings = torch.cat([compute_embeddings(sentence) for sentence in X_train], dim=0)\nX_test_embeddings = torch.cat([compute_embeddings(sentence) for sentence in X_test], dim=0)\n\n# # Tính toán embeddings cho tất cả các câu đánh giá trong tập huấn luyện và tập kiểm tra\n# from tqdm import tqdm\n# X_train_embeddings = []\n# for sentence in tqdm(X_train[0:10]):\n#     X_train_embeddings.append(compute_embeddings(sentence))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:27:25.009105Z","iopub.execute_input":"2024-06-02T14:27:25.009430Z","iopub.status.idle":"2024-06-02T14:41:48.096009Z","shell.execute_reply.started":"2024-06-02T14:27:25.009401Z","shell.execute_reply":"2024-06-02T14:41:48.095125Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"y_train = y[:len(X_train_embeddings)]","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:41:48.099867Z","iopub.execute_input":"2024-06-02T14:41:48.100147Z","iopub.status.idle":"2024-06-02T14:41:48.104573Z","shell.execute_reply.started":"2024-06-02T14:41:48.100122Z","shell.execute_reply":"2024-06-02T14:41:48.103714Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"y_test = y[len(X_train_embeddings):]","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:41:48.107207Z","iopub.execute_input":"2024-06-02T14:41:48.107459Z","iopub.status.idle":"2024-06-02T14:41:48.124686Z","shell.execute_reply.started":"2024-06-02T14:41:48.107438Z","shell.execute_reply":"2024-06-02T14:41:48.123859Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_test_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:41:48.125641Z","iopub.execute_input":"2024-06-02T14:41:48.125926Z","iopub.status.idle":"2024-06-02T14:41:48.138085Z","shell.execute_reply.started":"2024-06-02T14:41:48.125903Z","shell.execute_reply":"2024-06-02T14:41:48.137307Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"torch.Size([2919, 768])"},"metadata":{}}]},{"cell_type":"code","source":"y_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:41:48.139123Z","iopub.execute_input":"2024-06-02T14:41:48.139381Z","iopub.status.idle":"2024-06-02T14:41:48.150531Z","shell.execute_reply.started":"2024-06-02T14:41:48.139359Z","shell.execute_reply":"2024-06-02T14:41:48.149715Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(2919, 16)"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch\ntrain_labels = torch.tensor(y_train)\ntrain_data = TensorDataset(X_train_embeddings, train_labels)\n# train_sampler = SequentialSampler(train_data)\n# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:41:48.151740Z","iopub.execute_input":"2024-06-02T14:41:48.152061Z","iopub.status.idle":"2024-06-02T14:41:48.168551Z","shell.execute_reply.started":"2024-06-02T14:41:48.152031Z","shell.execute_reply":"2024-06-02T14:41:48.167804Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"val_labels = torch.tensor(y_test)\nval_data = TensorDataset(X_test_embeddings, val_labels)\n# val_sampler = SequentialSampler(val_data)\n# val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\nval_dataloader = DataLoader(val_data, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:41:48.169670Z","iopub.execute_input":"2024-06-02T14:41:48.169925Z","iopub.status.idle":"2024-06-02T14:41:48.182913Z","shell.execute_reply.started":"2024-06-02T14:41:48.169903Z","shell.execute_reply":"2024-06-02T14:41:48.182055Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import wandb\n\n# Log in to wandb programmatically (if not already logged in via CLI)\nwandb.login(key=\"8b8539216f9db012b822d7152559a24eb9f66ac2\")\n\n# Initialize a new wandb run\nrun = wandb.init(project=\"ABSA-Dense-project\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:15:51.296785Z","iopub.execute_input":"2024-06-02T16:15:51.297281Z","iopub.status.idle":"2024-06-02T16:16:14.250021Z","shell.execute_reply.started":"2024-06-02T16:15:51.297233Z","shell.execute_reply":"2024-06-02T16:16:14.249045Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:bhauficg) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Eval/Accuracy</td><td>▁▂▃▄▆▇█</td></tr><tr><td>Eval/Loss</td><td>█▇▆▄▃▂▁</td></tr><tr><td>Train/Accuracy</td><td>▁▂▃▅▆▇█</td></tr><tr><td>Train/Loss</td><td>█▇▅▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Eval/Accuracy</td><td>0.62547</td></tr><tr><td>Eval/Loss</td><td>0.10976</td></tr><tr><td>Train/Accuracy</td><td>0.52377</td></tr><tr><td>Train/Loss</td><td>0.10993</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">serene-valley-18</strong> at: <a href='https://wandb.ai/hopmuc123456/ABSA-Dense-project/runs/bhauficg' target=\"_blank\">https://wandb.ai/hopmuc123456/ABSA-Dense-project/runs/bhauficg</a><br/> View project at: <a href='https://wandb.ai/hopmuc123456/ABSA-Dense-project' target=\"_blank\">https://wandb.ai/hopmuc123456/ABSA-Dense-project</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240602_161058-bhauficg/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:bhauficg). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240602_161551-5jyhjeen</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hopmuc123456/ABSA-Dense-project/runs/5jyhjeen' target=\"_blank\">decent-eon-19</a></strong> to <a href='https://wandb.ai/hopmuc123456/ABSA-Dense-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hopmuc123456/ABSA-Dense-project' target=\"_blank\">https://wandb.ai/hopmuc123456/ABSA-Dense-project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hopmuc123456/ABSA-Dense-project/runs/5jyhjeen' target=\"_blank\">https://wandb.ai/hopmuc123456/ABSA-Dense-project/runs/5jyhjeen</a>"},"metadata":{}}]},{"cell_type":"code","source":"# Define the ANN model\nclass ANN(nn.Module):\n    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n        super(ANN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.2)\n        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n        self.sigmoid = nn.Sigmoid()  # For multi-label classification\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.dropout1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        x = self.sigmoid(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:08.167462Z","iopub.execute_input":"2024-06-02T14:42:08.168522Z","iopub.status.idle":"2024-06-02T14:42:08.177787Z","shell.execute_reply.started":"2024-06-02T14:42:08.168470Z","shell.execute_reply":"2024-06-02T14:42:08.176815Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\ninput_dim = X_train_embeddings.shape[1]\nhidden_dim1 = 600\nhidden_dim2 = 300\noutput_dim = 16  # Assuming 16 output classes (4 labels, each with 4 classes)\n\n# Initialize the model\nmodel = ANN(input_dim, hidden_dim1, hidden_dim2, output_dim).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:16:14.251787Z","iopub.execute_input":"2024-06-02T16:16:14.252072Z","iopub.status.idle":"2024-06-02T16:16:14.264789Z","shell.execute_reply.started":"2024-06-02T16:16:14.252045Z","shell.execute_reply":"2024-06-02T16:16:14.263863Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass CapsuleLoss(nn.Module):\n\n    def __init__(self, smooth=0.1, lamda=0.6):\n        super(CapsuleLoss, self).__init__()\n        self.smooth = smooth\n        self.lamda = lamda\n\n    def forward(self, input, target):\n        one_hot = torch.zeros_like(input).to(input.device)\n        one_hot = one_hot.scatter(1, target.unsqueeze(-1), 1)\n        a = torch.max(torch.zeros_like(input).to(input.device), 1 - self.smooth - input)\n        b = torch.max(torch.zeros_like(input).to(input.device), input - self.smooth)\n        loss = one_hot * a * a + self.lamda * (1 - one_hot) * b * b\n        loss = loss.sum(dim=1, keepdim=False)\n        return loss.mean()\n\n# CrossEntropyLoss for Label Smoothing Regularization\nclass CrossEntropyLoss_LSR(nn.Module):\n    def __init__(self, para_LSR=0.2):\n        super(CrossEntropyLoss_LSR, self).__init__()\n        self.para_LSR = para_LSR\n        self.logSoftmax = nn.LogSoftmax(dim=-1)\n\n    def _toOneHot_smooth(self, label, batchsize, classes):\n        prob = self.para_LSR * 1.0 / classes\n        one_hot_label = torch.zeros(batchsize, classes) + prob\n        for i in range(batchsize):\n            index = label[i]\n            one_hot_label[i, index] += (1.0 - self.para_LSR)\n        return one_hot_label\n\n    def forward(self, pre, label, size_average=True):\n        b, c = pre.size()\n        one_hot_label = self._toOneHot_smooth(label, b, c).to(pre.device)\n        loss = torch.sum(-one_hot_label * self.logSoftmax(pre), dim=1)\n        if size_average:\n            return torch.mean(loss)\n        else:\n            return torch.sum(loss)\n\nclass SmoothCrossEntropy(nn.Module):\n\n    def __init__(self, smooth=0.08):\n        super(SmoothCrossEntropy, self).__init__()\n        self.kldiv = nn.KLDivLoss()\n        self.smooth = smooth\n\n    def forward(self, input, target):\n        one_hot = torch.zeros_like(input).to(input.device)\n        one_hot = one_hot.scatter(1, target.unsqueeze(-1), 1)\n        target = (1 - self.smooth) * one_hot + self.smooth / (input.size(1) - 1) * (1 - one_hot)\n        # target = target + torch.rand_like(target).to(target.device) * 0.001\n        input = input - input.max(dim=1, keepdim=True)[0]\n        loss = -target * F.log_softmax(input, dim=-1)\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:08.430616Z","iopub.execute_input":"2024-06-02T14:42:08.431312Z","iopub.status.idle":"2024-06-02T14:42:08.450539Z","shell.execute_reply.started":"2024-06-02T14:42:08.431271Z","shell.execute_reply":"2024-06-02T14:42:08.449344Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import random\nfrom tqdm import tqdm_notebook\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AdamW\n\ndevice = 'cuda'\nepochs = 9\n\nparam_optimizer = list(model.named_parameters())\nno_decay = []\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\n\ncriterion = CapsuleLoss()\noptimizer = AdamW(optimizer_grouped_parameters, lr=5e-8, correct_bias=False)\n\n\nfor epoch_i in range(0, epochs):\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    \n    model.train()\n    train_accuracy = 0\n    nb_train_steps = 0\n    train_f1 = 0\n    total_loss = 0\n    total_samples = 0\n    correct_samples = 0\n    \n    for step, batch in tqdm_notebook(enumerate(train_dataloader)):\n        b_input_ids = batch[0].to(device)\n#         b_input_mask = batch[1].to(device)\n        b_labels = batch[1].to(device)\n\n        model.zero_grad()\n        outputs = model(b_input_ids)\n\n        logits = outputs.clone()  # keep this tensor on the GPU\n        logits = logits.view(-1, 4, 4)\n        label_ids = b_labels.view(-1, 4, 4)\n        \n        # Convert one-hot encoded labels to class indices\n        label_ids_indices = torch.argmax(label_ids, dim=-1)\n\n        # Reshape logits and label_ids\n        batch_size, num_capsules, num_classes = logits.shape\n        logits_flat = logits.view(batch_size * num_capsules, num_classes)\n        label_ids_flat = label_ids_indices.view(batch_size * num_capsules)\n\n        # Calculate loss\n        loss = criterion(logits_flat, label_ids_flat)\n        total_loss += batch_size * loss.item()\n        total_samples += batch_size * num_classes\n\n        pred = logits_flat.argmax(dim=1)\n        correct_samples += (label_ids_flat == pred).long().sum().item()\n#         print(correct_samples)\n#         print(total_samples)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n#         break\n        \n    train_loss = total_loss / total_samples\n    train_accuracy = correct_samples / total_samples\n\n    print(f\" Accuracy: {train_accuracy:.4f}\")\n    print(f\" Average training loss: {train_loss:.4f}\")\n\n    print(\"Running Validation...\")\n    model.eval()\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    eval_f1 = 0\n    total_loss = 0\n    total_samples = 0\n    correct_samples = 0\n    \n    for batch in tqdm_notebook(val_dataloader):\n\n        batch = tuple(t.to(device) for t in batch)\n\n        b_input_ids, b_labels = batch\n\n        with torch.no_grad():\n            outputs = model(b_input_ids)\n            \n            logits = outputs.clone()  # keep this tensor on the GPU\n            logits = logits.view(-1, 4, 4)\n            label_ids = b_labels.view(-1, 4, 4)\n\n            # Convert one-hot encoded labels to class indices\n            label_ids_indices = torch.argmax(label_ids, dim=-1)\n\n            # Reshape logits and label_ids\n            batch_size, num_capsules, num_classes = logits.shape\n            logits_flat = logits.view(batch_size * num_capsules, num_classes)\n            label_ids_flat = label_ids_indices.view(batch_size * num_capsules)\n\n            # Calculate loss\n            loss = criterion(logits_flat, label_ids_flat)\n            total_loss += batch_size * loss.item()\n            total_samples += batch_size * num_classes\n\n            pred = logits_flat.argmax(dim=1)\n            correct_samples += (label_ids_flat == pred).long().sum().item()\n#             break\n            \n    eval_loss = total_loss / total_samples\n    eval_accuracy = correct_samples / total_samples\n\n    print(f\" Accuracy: {eval_accuracy:.4f}\")\n    print(f\" Average evaluating loss: {eval_loss:.4f}\")\n    \n    wandb.log({\"Train/Accuracy\": train_accuracy, \n               \"Train/Loss\": train_loss,\n               \"Eval/Accuracy\": eval_accuracy,\n               \"Eval/Loss\": eval_loss,\n              })\n\n    save_path = f\"state_dict_model_{epoch_i + 1}.pt\"\n    torch.save(model.state_dict(), save_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:16:14.266598Z","iopub.execute_input":"2024-06-02T16:16:14.267131Z","iopub.status.idle":"2024-06-02T16:16:37.042879Z","shell.execute_reply.started":"2024-06-02T16:16:14.267085Z","shell.execute_reply":"2024-06-02T16:16:37.041791Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"======== Epoch 1 / 9 ========\nTraining...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/tmp/ipykernel_34/2536642992.py:34: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for step, batch in tqdm_notebook(enumerate(train_dataloader)):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cd43d687b1457e9c6303ffdbbd568d"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.2766\n Average training loss: 0.1113\nRunning Validation...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2536642992.py:83: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm_notebook(val_dataloader):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c043c96d1641adbeb1afd8babcece1"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.2775\n Average evaluating loss: 0.1108\n======== Epoch 2 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"643a6212c8fc47d59bcbb3028cce91f0"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.3177\n Average training loss: 0.1106\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"174fb0044a0f4dcfa299b4b967bb31a9"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.3232\n Average evaluating loss: 0.1102\n======== Epoch 3 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a98975bb494af9a04727d54be7ad43"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.3670\n Average training loss: 0.1100\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1002272181441bf9254790b537fba02"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.4103\n Average evaluating loss: 0.1096\n======== Epoch 4 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d687ac13cd349dfbaa7a3c97cb3e78f"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.4188\n Average training loss: 0.1095\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5454c0dfae13458c9a26d8a3d5998143"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.5044\n Average evaluating loss: 0.1091\n======== Epoch 5 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb2c0c50c184f6b965ccb17a6e867f7"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.4695\n Average training loss: 0.1090\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f01807f8adbd480398aece8565b20ee0"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.5919\n Average evaluating loss: 0.1086\n======== Epoch 6 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdd604f55f504260bdc9186408e12f06"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.5264\n Average training loss: 0.1086\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5c5e2c0560d4739a02a98bd82e2f65a"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.6570\n Average evaluating loss: 0.1082\n======== Epoch 7 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1581a03c6f8041cda1e45488babd1445"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.5784\n Average training loss: 0.1081\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f94615f87f6f429c9fc3cf1f44098020"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.6955\n Average evaluating loss: 0.1077\n======== Epoch 8 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b349358d38d4857ba3fccb59e2fc94b"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.6225\n Average training loss: 0.1076\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df07d076646944a2b54d412e2a7c76c9"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.7121\n Average evaluating loss: 0.1072\n======== Epoch 9 / 9 ========\nTraining...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a18eb038a798461ead9e34b4b8b6cd1a"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.6569\n Average training loss: 0.1071\nRunning Validation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/183 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4128de9d4fd6478fa3d6bc058aaa3166"}},"metadata":{}},{"name":"stdout","text":" Accuracy: 0.7178\n Average evaluating loss: 0.1067\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\n\n# train_path = 'https://raw.githubusercontent.com/lavibula/SentimentAnalysis-with-Vietnamese-reviews/main/absa_data/train.csv'\ntest_path = 'https://raw.githubusercontent.com/lavibula/SentimentAnalysis-with-Vietnamese-reviews/main/absa_data/test.csv'\n\n# train_set = pd.read_csv(train_path)\ntest_set = pd.read_csv(test_path)\n\n# train_set['Sentence'] = train_set['Sentence'].apply(rdrsegmenter.tokenize).apply(lambda x: ' '.join(x[0]))\ntest_set['Sentence'] = test_set['Sentence'].apply(rdrsegmenter.tokenize).apply(lambda x: ' '.join(x[0]))\n\n# train_set = pd.concat([train_set, test_set]).reset_index().drop(columns = 'index')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:12.800009Z","iopub.execute_input":"2024-06-02T14:42:12.801601Z","iopub.status.idle":"2024-06-02T14:42:20.259582Z","shell.execute_reply.started":"2024-06-02T14:42:12.801560Z","shell.execute_reply":"2024-06-02T14:42:20.258477Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df = test_set.copy()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:20.260847Z","iopub.execute_input":"2024-06-02T14:42:20.261188Z","iopub.status.idle":"2024-06-02T14:42:20.267092Z","shell.execute_reply.started":"2024-06-02T14:42:20.261155Z","shell.execute_reply":"2024-06-02T14:42:20.266000Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:20.268146Z","iopub.execute_input":"2024-06-02T14:42:20.268453Z","iopub.status.idle":"2024-06-02T14:42:20.289117Z","shell.execute_reply.started":"2024-06-02T14:42:20.268428Z","shell.execute_reply":"2024-06-02T14:42:20.288169Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                                               Sentence  FACILITY  LECTURER  \\\n0                                nói tiếng anh lưu_loát         0         1   \n1                                giáo_viên rất vui_tính         0         1   \n2                                         cô max có_tâm         0         1   \n3                              giảng bài thu_hút dí_dỏm         0         1   \n4     giáo_viên không giảng_dạy kiến_thức hướng_dẫn ...         0         2   \n...                                                 ...       ...       ...   \n3161  các slide khó hiểu ngôn_ngữ trong slide phức_t...         0         2   \n3162                   giáo_viên giảng_dạy có tâm_huyết         0         1   \n3163                      chia_sẻ cho em nhiều điều hay         0         1   \n3164                                   em tiếp_thu chậm         0         2   \n3165  em có học ở một trung_tâm tiếng anh ở ngoài tr...         0         0   \n\n      OTHERS  TRAINING_PROGRAM  \n0          0                 0  \n1          0                 0  \n2          0                 0  \n3          0                 0  \n4          0                 0  \n...      ...               ...  \n3161       0                 0  \n3162       0                 0  \n3163       0                 0  \n3164       0                 0  \n3165       0                 3  \n\n[3166 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>FACILITY</th>\n      <th>LECTURER</th>\n      <th>OTHERS</th>\n      <th>TRAINING_PROGRAM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nói tiếng anh lưu_loát</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>giáo_viên rất vui_tính</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cô max có_tâm</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>giảng bài thu_hút dí_dỏm</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>giáo_viên không giảng_dạy kiến_thức hướng_dẫn ...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3161</th>\n      <td>các slide khó hiểu ngôn_ngữ trong slide phức_t...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3162</th>\n      <td>giáo_viên giảng_dạy có tâm_huyết</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3163</th>\n      <td>chia_sẻ cho em nhiều điều hay</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3164</th>\n      <td>em tiếp_thu chậm</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3165</th>\n      <td>em có học ở một trung_tâm tiếng anh ở ngoài tr...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>3166 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Sample data for illustration\n# data = train_set[['FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM']].copy()\ndata = df.copy()\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Select specific columns\nselected_columns = ['FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM']\ndata_to_encode = df[selected_columns]\n\n# Initialize OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n\n# Fit and transform the data\nencoded_data = encoder.fit_transform(data_to_encode)\n\n# Convert the result to a DataFrame for better readability\nencoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(selected_columns))\n\n# If you want to list the values\nencoded_values_list = encoded_df.values\n# print(encoded_values_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:20.292395Z","iopub.execute_input":"2024-06-02T14:42:20.292708Z","iopub.status.idle":"2024-06-02T14:42:20.309822Z","shell.execute_reply.started":"2024-06-02T14:42:20.292661Z","shell.execute_reply":"2024-06-02T14:42:20.308734Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"encoded_values_list.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:20.310931Z","iopub.execute_input":"2024-06-02T14:42:20.311196Z","iopub.status.idle":"2024-06-02T14:42:20.318754Z","shell.execute_reply.started":"2024-06-02T14:42:20.311167Z","shell.execute_reply":"2024-06-02T14:42:20.317857Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(3166, 16)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer\n\nphobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:20.319715Z","iopub.execute_input":"2024-06-02T14:42:20.319970Z","iopub.status.idle":"2024-06-02T14:42:21.207773Z","shell.execute_reply.started":"2024-06-02T14:42:20.319949Z","shell.execute_reply":"2024-06-02T14:42:21.206688Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\nsentence = 'Chúng_tôi là những nghiên_cứu_viên .'  \n\ninput_ids = torch.tensor([tokenizer.encode(sentence)])\n\nwith torch.no_grad():\n    features = phobert(input_ids)  # Models outputs are now tuples","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:21.209110Z","iopub.execute_input":"2024-06-02T14:42:21.209389Z","iopub.status.idle":"2024-06-02T14:42:21.277578Z","shell.execute_reply.started":"2024-06-02T14:42:21.209364Z","shell.execute_reply":"2024-06-02T14:42:21.276465Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Tách features (câu đánh giá) và labels (cảm xúc)\nX = df['Sentence']\ny = encoded_values_list\n\n# Tính toán embeddings cho một câu đánh giá\ndef compute_embeddings(sentence):\n    # Tokenize câu đánh giá và thêm token đặc biệt\n    # INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n#     sentence = 'Chúng_tôi là những nghiên_cứu_viên .'  \n\n    input_ids = torch.tensor([tokenizer.encode(sentence)])\n\n    with torch.no_grad():\n        features = phobert(input_ids)  # Models outputs are now tuples\n    \n    # Lấy embeddings từ lớp cuối cùng (CLS token)\n    cls_embedding = features.last_hidden_state[:, 0, :]\n    \n    return cls_embedding\n\n# # Tính toán embeddings cho tất cả các câu đánh giá trong tập huấn luyện và tập kiểm tra\ninputs = torch.cat([compute_embeddings(sentence) for sentence in X], dim=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:42:21.278955Z","iopub.execute_input":"2024-06-02T14:42:21.280445Z","iopub.status.idle":"2024-06-02T14:45:32.357924Z","shell.execute_reply.started":"2024-06-02T14:42:21.280409Z","shell.execute_reply":"2024-06-02T14:45:32.356839Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"inputs.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T14:45:32.359455Z","iopub.execute_input":"2024-06-02T14:45:32.359875Z","iopub.status.idle":"2024-06-02T14:45:32.367646Z","shell.execute_reply.started":"2024-06-02T14:45:32.359838Z","shell.execute_reply":"2024-06-02T14:45:32.366655Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"torch.Size([3166, 768])"},"metadata":{}}]},{"cell_type":"code","source":"device = 'cuda'\n\nmodel.eval()\n# eval_loss, eval_accuracy = 0, 0\n# nb_eval_steps, nb_eval_examples = 0, 0\n# eval_f1 = 0\n# total_loss = 0\n# total_samples = 0\n# correct_samples = 0\n\n# b_input_ids, b_input_mask = inputs['input_ids'], inputs['attention_mask']\nb_labels = torch.tensor(y)\n# b_input_ids = b_input_ids.to(device)\n# b_input_mask = b_input_mask.to(device)\nb_labels = b_labels.to(device)\ninputs = inputs.to(device)\nlogits = []\nfor i in range(len(b_labels)):\n    with torch.no_grad():\n        outputs = model(inputs[i])\n        logit = outputs.clone()  # keep this tensor on the GPU\n        logits.append(list(np.array(logit.cpu())))\n#         if i == 2:\n#             break","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:17:28.098900Z","iopub.execute_input":"2024-06-02T16:17:28.099301Z","iopub.status.idle":"2024-06-02T16:17:29.055865Z","shell.execute_reply.started":"2024-06-02T16:17:28.099268Z","shell.execute_reply":"2024-06-02T16:17:29.054655Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"# logits","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:07:16.489298Z","iopub.execute_input":"2024-06-02T15:07:16.489694Z","iopub.status.idle":"2024-06-02T15:07:16.494537Z","shell.execute_reply.started":"2024-06-02T15:07:16.489663Z","shell.execute_reply":"2024-06-02T15:07:16.493639Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def correct_sample(logits, b_labels):\n    logits = torch.tensor(logits).reshape(-1,4,4)\n    label_ids = b_labels.view(-1, 4, 4)\n\n    # Convert one-hot encoded labels to class indices\n    label_ids_indices = torch.argmax(label_ids, dim=-1).cpu()\n\n    # Reshape logits and label_ids\n    batch_size, num_capsules, num_classes = logits.shape\n    logits_flat = logits.view(batch_size * num_capsules, num_classes)\n    print(label_ids_indices)\n    label_ids_flat = label_ids_indices.view(batch_size * num_capsules)\n\n    # Calculate loss\n    # loss = criterion(logits_flat, label_ids_flat)\n    # total_loss += batch_size * loss.item()\n    # total_samples += batch_size * num_classes\n\n    pred = logits_flat.argmax(dim=1)\n    correct_samples = (label_ids_flat == pred).long().sum().item()\n    return correct_samples\n\ncorrect_sample(logits, b_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:20:44.528677Z","iopub.execute_input":"2024-06-02T16:20:44.529648Z","iopub.status.idle":"2024-06-02T16:20:44.579692Z","shell.execute_reply.started":"2024-06-02T16:20:44.529613Z","shell.execute_reply":"2024-06-02T16:20:44.578577Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"tensor([[0, 1, 0, 0],\n        [0, 1, 0, 0],\n        [0, 1, 0, 0],\n        ...,\n        [0, 1, 0, 0],\n        [0, 2, 0, 0],\n        [0, 0, 0, 3]])\n","output_type":"stream"},{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"9074"},"metadata":{}}]},{"cell_type":"code","source":"(len(b_labels) * 4)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T15:07:53.807329Z","iopub.execute_input":"2024-06-02T15:07:53.807697Z","iopub.status.idle":"2024-06-02T15:07:53.815492Z","shell.execute_reply.started":"2024-06-02T15:07:53.807665Z","shell.execute_reply":"2024-06-02T15:07:53.814379Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"12664"},"metadata":{}}]},{"cell_type":"code","source":"def return_preds_and_labels(logits, b_labels):\n    logits = torch.tensor(logits).reshape(-1,4,4)\n    label_ids = b_labels.view(-1, 4, 4)\n\n    # Convert one-hot encoded labels to class indices\n    label_ids_indices = torch.argmax(label_ids, dim=-1).cpu()\n\n    # Reshape logits and label_ids\n    batch_size, num_capsules, num_classes = logits.shape\n    logits_flat = logits.view(batch_size * num_capsules, num_classes)\n    label_ids_flat = label_ids_indices.view(batch_size * num_capsules)\n\n    # Calculate loss\n    # loss = criterion(logits_flat, label_ids_flat)\n    # total_loss += batch_size * loss.item()\n    # total_samples += batch_size * num_classes\n    pred = logits_flat.argmax(dim=1)\n    return pred, label_ids_flat\n\npred, label = return_preds_and_labels(logits, b_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:17:31.360960Z","iopub.execute_input":"2024-06-02T16:17:31.361631Z","iopub.status.idle":"2024-06-02T16:17:31.403851Z","shell.execute_reply.started":"2024-06-02T16:17:31.361599Z","shell.execute_reply":"2024-06-02T16:17:31.402649Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(label, pred))\n# 0: None, 1: 'Positive', 2: 'Negative', 3: 'Normal'","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:17:34.267069Z","iopub.execute_input":"2024-06-02T16:17:34.267445Z","iopub.status.idle":"2024-06-02T16:17:34.304463Z","shell.execute_reply.started":"2024-06-02T16:17:34.267414Z","shell.execute_reply":"2024-06-02T16:17:34.303579Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.91      0.90      0.91      9498\n           1       0.47      0.04      0.08      1590\n           2       0.22      0.29      0.25      1409\n           3       0.02      0.16      0.04       167\n\n    accuracy                           0.72     12664\n   macro avg       0.40      0.35      0.32     12664\nweighted avg       0.77      0.72      0.72     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# topic\n# 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'\nprint(classification_report(label.reshape(-1,4).argmax(dim = 1), pred.reshape(-1,4).argmax(dim = 1)))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T16:17:37.139481Z","iopub.execute_input":"2024-06-02T16:17:37.139878Z","iopub.status.idle":"2024-06-02T16:17:37.163096Z","shell.execute_reply.started":"2024-06-02T16:17:37.139849Z","shell.execute_reply":"2024-06-02T16:17:37.162173Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       145\n           1       0.73      1.00      0.84      2290\n           2       0.26      0.03      0.06       159\n           3       0.00      0.00      0.00       572\n\n    accuracy                           0.72      3166\n   macro avg       0.25      0.26      0.22      3166\nweighted avg       0.54      0.72      0.61      3166\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]}]}