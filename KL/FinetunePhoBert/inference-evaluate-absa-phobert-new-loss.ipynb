{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"accelerator":"GPU","language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:31:42.328829Z","iopub.execute_input":"2024-06-02T09:31:42.329294Z","iopub.status.idle":"2024-06-02T09:31:42.348052Z","shell.execute_reply.started":"2024-06-02T09:31:42.329253Z","shell.execute_reply":"2024-06-02T09:31:42.346872Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"_Biz3kNemdGC","execution":{"iopub.status.busy":"2024-06-02T09:31:42.350178Z","iopub.execute_input":"2024-06-02T09:31:42.350720Z","iopub.status.idle":"2024-06-02T09:31:56.074416Z","shell.execute_reply.started":"2024-06-02T09:31:42.350676Z","shell.execute_reply":"2024-06-02T09:31:56.073239Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install the vncorenlp python wrapper\n!pip install vncorenlp\n\n# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)\n!mkdir -p vncorenlp/models/wordsegmenter\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n!mv VnCoreNLP-1.1.1.jar vncorenlp/\n!mv vi-vocab vncorenlp/models/wordsegmenter/\n!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/","metadata":{"id":"ncGVmD7VmdCQ","execution":{"iopub.status.busy":"2024-06-02T09:31:56.076888Z","iopub.execute_input":"2024-06-02T09:31:56.077701Z","iopub.status.idle":"2024-06-02T09:32:16.862533Z","shell.execute_reply.started":"2024-06-02T09:31:56.077653Z","shell.execute_reply":"2024-06-02T09:32:16.861449Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: vncorenlp in /opt/conda/lib/python3.10/site-packages (1.0.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vncorenlp) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (2024.2.2)\n--2024-06-02 09:32:11--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 27412575 (26M) [application/octet-stream]\nSaving to: 'VnCoreNLP-1.1.1.jar'\n\nVnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   174MB/s    in 0.2s    \n\n2024-06-02 09:32:11 (174 MB/s) - 'VnCoreNLP-1.1.1.jar' saved [27412575/27412575]\n\n--2024-06-02 09:32:12--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 526544 (514K) [application/octet-stream]\nSaving to: 'vi-vocab'\n\nvi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.05s   \n\n2024-06-02 09:32:12 (10.3 MB/s) - 'vi-vocab' saved [526544/526544]\n\n--2024-06-02 09:32:13--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 128508 (125K) [text/plain]\nSaving to: 'wordsegmenter.rdr'\n\nwordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n\n2024-06-02 09:32:13 (5.24 MB/s) - 'wordsegmenter.rdr' saved [128508/128508]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget https://public.vinai.io/PhoBERT_base_transformers.tar.gz\n!tar -xzvf PhoBERT_base_transformers.tar.gz","metadata":{"id":"3ntjPg2xEHX8","execution":{"iopub.status.busy":"2024-06-02T09:32:16.864363Z","iopub.execute_input":"2024-06-02T09:32:16.864788Z","iopub.status.idle":"2024-06-02T09:32:27.642258Z","shell.execute_reply.started":"2024-06-02T09:32:16.864746Z","shell.execute_reply":"2024-06-02T09:32:27.641067Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"--2024-06-02 09:32:17--  https://public.vinai.io/PhoBERT_base_transformers.tar.gz\nResolving public.vinai.io (public.vinai.io)... 108.159.227.26, 108.159.227.61, 108.159.227.65, ...\nConnecting to public.vinai.io (public.vinai.io)|108.159.227.26|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 322405979 (307M) [application/x-tar]\nSaving to: 'PhoBERT_base_transformers.tar.gz.1'\n\nPhoBERT_base_transf 100%[===================>] 307.47M   256MB/s    in 1.2s    \n\n2024-06-02 09:32:19 (256 MB/s) - 'PhoBERT_base_transformers.tar.gz.1' saved [322405979/322405979]\n\nPhoBERT_base_transformers/\nPhoBERT_base_transformers/config.json\nPhoBERT_base_transformers/bpe.codes\nPhoBERT_base_transformers/model.bin\nPhoBERT_base_transformers/dict.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install vncorenlp\n","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:32:27.645463Z","iopub.execute_input":"2024-06-02T09:32:27.646190Z","iopub.status.idle":"2024-06-02T09:32:40.529712Z","shell.execute_reply.started":"2024-06-02T09:32:27.646147Z","shell.execute_reply":"2024-06-02T09:32:40.528439Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: vncorenlp in /opt/conda/lib/python3.10/site-packages (1.0.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vncorenlp) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vncorenlp) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from vncorenlp import VnCoreNLP\nrdrsegmenter = VnCoreNLP(\"/kaggle/working/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n\ntext = \"Đại học Bách Khoa Hà Nội.\"\n\nword_segmented_text = rdrsegmenter.tokenize(text)\nprint(word_segmented_text)","metadata":{"id":"ddYwNvwemdAv","execution":{"iopub.status.busy":"2024-06-02T09:32:40.531501Z","iopub.execute_input":"2024-06-02T09:32:40.531928Z","iopub.status.idle":"2024-06-02T09:32:45.815160Z","shell.execute_reply.started":"2024-06-02T09:32:40.531887Z","shell.execute_reply":"2024-06-02T09:32:45.814109Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[['Đại_học', 'Bách_Khoa', 'Hà_Nội', '.']]\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport pandas as pd\n\ntest_path = 'https://raw.githubusercontent.com/lavibula/SentimentAnalysis-with-Vietnamese-reviews/main/absa_data/test.csv'\ntest_set = pd.read_csv(test_path)\ntest_set['Sentence'] = test_set['Sentence'].apply(rdrsegmenter.tokenize).apply(lambda x: ' '.join(x[0]))\n\ndf = test_set.copy()\ndf","metadata":{"id":"i5jkpRBUmc1v","execution":{"iopub.status.busy":"2024-06-02T09:32:45.816312Z","iopub.execute_input":"2024-06-02T09:32:45.816711Z","iopub.status.idle":"2024-06-02T09:32:55.019880Z","shell.execute_reply.started":"2024-06-02T09:32:45.816681Z","shell.execute_reply":"2024-06-02T09:32:55.018767Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                               Sentence  FACILITY  LECTURER  \\\n0                                nói tiếng anh lưu_loát         0         1   \n1                                giáo_viên rất vui_tính         0         1   \n2                                         cô max có_tâm         0         1   \n3                              giảng bài thu_hút dí_dỏm         0         1   \n4     giáo_viên không giảng_dạy kiến_thức hướng_dẫn ...         0         2   \n...                                                 ...       ...       ...   \n3161  các slide khó hiểu ngôn_ngữ trong slide phức_t...         0         2   \n3162                   giáo_viên giảng_dạy có tâm_huyết         0         1   \n3163                      chia_sẻ cho em nhiều điều hay         0         1   \n3164                                   em tiếp_thu chậm         0         2   \n3165  em có học ở một trung_tâm tiếng anh ở ngoài tr...         0         0   \n\n      OTHERS  TRAINING_PROGRAM  \n0          0                 0  \n1          0                 0  \n2          0                 0  \n3          0                 0  \n4          0                 0  \n...      ...               ...  \n3161       0                 0  \n3162       0                 0  \n3163       0                 0  \n3164       0                 0  \n3165       0                 3  \n\n[3166 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>FACILITY</th>\n      <th>LECTURER</th>\n      <th>OTHERS</th>\n      <th>TRAINING_PROGRAM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nói tiếng anh lưu_loát</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>giáo_viên rất vui_tính</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cô max có_tâm</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>giảng bài thu_hút dí_dỏm</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>giáo_viên không giảng_dạy kiến_thức hướng_dẫn ...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3161</th>\n      <td>các slide khó hiểu ngôn_ngữ trong slide phức_t...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3162</th>\n      <td>giáo_viên giảng_dạy có tâm_huyết</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3163</th>\n      <td>chia_sẻ cho em nhiều điều hay</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3164</th>\n      <td>em tiếp_thu chậm</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3165</th>\n      <td>em có học ở một trung_tâm tiếng anh ở ngoài tr...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>3166 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"3166*4","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:49:52.730849Z","iopub.execute_input":"2024-06-02T09:49:52.731307Z","iopub.status.idle":"2024-06-02T09:49:52.739232Z","shell.execute_reply.started":"2024-06-02T09:49:52.731270Z","shell.execute_reply":"2024-06-02T09:49:52.738137Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"12664"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Sample data for illustration\n# data = train_set[['FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM']].copy()\ndata = df.copy()\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Select specific columns\nselected_columns = ['FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM']\ndata_to_encode = df[selected_columns]\n\n# Initialize OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n\n# Fit and transform the data\nencoded_data = encoder.fit_transform(data_to_encode)\n\n# Convert the result to a DataFrame for better readability\nencoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(selected_columns))\n\n# If you want to list the values\nencoded_values_list = encoded_df.values\n# print(encoded_values_list)","metadata":{"id":"H5cbp6Q0ROGV","execution":{"iopub.status.busy":"2024-06-02T09:32:55.021569Z","iopub.execute_input":"2024-06-02T09:32:55.022615Z","iopub.status.idle":"2024-06-02T09:32:55.592430Z","shell.execute_reply.started":"2024-06-02T09:32:55.022576Z","shell.execute_reply":"2024-06-02T09:32:55.591463Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"encoded_values_list.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:32:55.593754Z","iopub.execute_input":"2024-06-02T09:32:55.594155Z","iopub.status.idle":"2024-06-02T09:32:55.601096Z","shell.execute_reply.started":"2024-06-02T09:32:55.594113Z","shell.execute_reply":"2024-06-02T09:32:55.599897Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(3166, 16)"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport random\nimport json\nimport torch\nimport torch.nn as nn\nfrom functools import partial\nfrom collections import namedtuple\nfrom dataclasses import dataclass, field, asdict\nfrom huggingface_hub import HfApi\n\n# import evaluate\nimport numpy as np\n# from datasets import load_dataset\nfrom transformers import Trainer\nfrom transformers import DataCollatorWithPadding\nfrom transformers import AutoTokenizer, TrainingArguments","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:40:22.123391Z","iopub.execute_input":"2024-06-02T09:40:22.123943Z","iopub.status.idle":"2024-06-02T09:40:28.906993Z","shell.execute_reply.started":"2024-06-02T09:40:22.123893Z","shell.execute_reply":"2024-06-02T09:40:28.906133Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-06-02 09:40:26.279220: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-02 09:40:26.279284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-02 09:40:26.281031: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd\n\n# train_path = 'https://raw.githubusercontent.com/lavibula/SentimentAnalysis-with-Vietnamese-reviews/main/absa_data/train.csv'\n# test_path = 'https://raw.githubusercontent.com/lavibula/SentimentAnalysis-with-Vietnamese-reviews/main/absa_data/test.csv'\n\n# train_set = pd.read_csv(train_path)\n# test_set = pd.read_csv(test_path)\n\ntest_id, test_text, test_labels = [], [], []\n\ntest_id = [('test_' + str(i)) for i in range(df.shape[0])]\ntest_text = list(df['Sentence'].values)\ntest_labels = encoded_values_list\ntest_labels = torch.tensor(test_labels)\n","metadata":{"id":"BGrBOrNXmcyL","execution":{"iopub.status.busy":"2024-06-02T09:40:33.836026Z","iopub.execute_input":"2024-06-02T09:40:33.837093Z","iopub.status.idle":"2024-06-02T09:40:33.845370Z","shell.execute_reply.started":"2024-06-02T09:40:33.837039Z","shell.execute_reply":"2024-06-02T09:40:33.844149Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer\n\nMODEL_NAME = \"vinai/phobert-base-v2\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Example input data\ntexts = test_text.copy()\n\n# Tokenize input data\ninputs = tokenizer(\n    texts,\n    max_length = 256, \n    padding='max_length',\n    truncation=True,\n    return_tensors=\"pt\"  # Return PyTorch tensors\n)\n    \n\n# Print the tokenized inputs\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:40:33.852220Z","iopub.execute_input":"2024-06-02T09:40:33.852512Z","iopub.status.idle":"2024-06-02T09:40:35.772174Z","shell.execute_reply.started":"2024-06-02T09:40:33.852486Z","shell.execute_reply":"2024-06-02T09:40:35.771046Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[    0,    96,   355,  ...,     1,     1,     1],\n        [    0,   960,    59,  ...,     1,     1,     1],\n        [    0,   106, 23371,  ...,     1,     1,     1],\n        ...,\n        [    0,   288,    13,  ...,     1,     1,     1],\n        [    0,   193,  4747,  ...,     1,     1,     1],\n        [    0,   193,    10,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs['input_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:47:12.379387Z","iopub.execute_input":"2024-06-02T09:47:12.379755Z","iopub.status.idle":"2024-06-02T09:47:12.386861Z","shell.execute_reply.started":"2024-06-02T09:47:12.379725Z","shell.execute_reply":"2024-06-02T09:47:12.385847Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"torch.Size([3166, 256])"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification, RobertaConfig, AdamW\n\nconfig = RobertaConfig.from_pretrained(\n    \"/kaggle/working/PhoBERT_base_transformers/config.json\", from_tf=False, num_labels = 16, output_hidden_states=False,\n)","metadata":{"id":"tnWqjZ9BmccW","execution":{"iopub.status.busy":"2024-06-02T09:40:35.773945Z","iopub.execute_input":"2024-06-02T09:40:35.774263Z","iopub.status.idle":"2024-06-02T09:40:35.786276Z","shell.execute_reply.started":"2024-06-02T09:40:35.774235Z","shell.execute_reply":"2024-06-02T09:40:35.785366Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Specify a path\nPATH = '/kaggle/input/absa-phobert-sehilnlf/state_dict_model_10.pt'","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:40:35.787538Z","iopub.execute_input":"2024-06-02T09:40:35.787853Z","iopub.status.idle":"2024-06-02T09:40:35.793991Z","shell.execute_reply.started":"2024-06-02T09:40:35.787818Z","shell.execute_reply":"2024-06-02T09:40:35.793060Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Load\nmodel = RobertaForSequenceClassification.from_pretrained(\n    \"/kaggle/working/PhoBERT_base_transformers/model.bin\",\n    config=config\n)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(DEVICE)\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:40:35.795933Z","iopub.execute_input":"2024-06-02T09:40:35.796245Z","iopub.status.idle":"2024-06-02T09:40:37.278392Z","shell.execute_reply.started":"2024-06-02T09:40:35.796212Z","shell.execute_reply":"2024-06-02T09:40:37.277396Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/working/PhoBERT_base_transformers/model.bin and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(64001, 768, padding_idx=0)\n      (position_embeddings): Embedding(258, 768, padding_idx=0)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=16, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## EVALUATE","metadata":{}},{"cell_type":"code","source":"device = 'cuda'\n\nmodel.eval()\n# eval_loss, eval_accuracy = 0, 0\n# nb_eval_steps, nb_eval_examples = 0, 0\n# eval_f1 = 0\n# total_loss = 0\n# total_samples = 0\n# correct_samples = 0\n\nb_input_ids, b_input_mask = inputs['input_ids'], inputs['attention_mask']\nb_labels = test_labels\nb_input_ids = b_input_ids.to(device)\nb_input_mask = b_input_mask.to(device)\nb_labels = b_labels.to(device)\n\nlogits = []\nfor i in range(len(b_labels)):\n    with torch.no_grad():\n        outputs = model(b_input_ids[i].reshape(1,-1),\n                        token_type_ids=None,\n                        attention_mask=b_input_mask[i].reshape(1,-1))\n\n        logit = outputs.logits  # keep this tensor on the GPU\n        logits.append(list(np.array(logit[0].cpu())))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:49:25.685369Z","iopub.execute_input":"2024-06-02T09:49:25.686154Z","iopub.status.idle":"2024-06-02T09:49:25.696526Z","shell.execute_reply.started":"2024-06-02T09:49:25.686033Z","shell.execute_reply":"2024-06-02T09:49:25.694798Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"len(logits)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:47:39.160346Z","iopub.execute_input":"2024-06-02T09:47:39.161053Z","iopub.status.idle":"2024-06-02T09:47:39.167214Z","shell.execute_reply.started":"2024-06-02T09:47:39.161018Z","shell.execute_reply":"2024-06-02T09:47:39.166071Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"code","source":"def correct_sample(logits, b_labels):\n    logits = torch.tensor(logits).reshape(-1,4,4)\n    label_ids = b_labels.view(-1, 4, 4)\n\n    # Convert one-hot encoded labels to class indices\n    label_ids_indices = torch.argmax(label_ids, dim=-1).cpu()\n\n    # Reshape logits and label_ids\n    batch_size, num_capsules, num_classes = logits.shape\n    logits_flat = logits.view(batch_size * num_capsules, num_classes)\n    label_ids_flat = label_ids_indices.view(batch_size * num_capsules)\n\n    # Calculate loss\n    # loss = criterion(logits_flat, label_ids_flat)\n    # total_loss += batch_size * loss.item()\n    # total_samples += batch_size * num_classes\n\n    pred = logits_flat.argmax(dim=1)\n    correct_samples = (label_ids_flat == pred).long().sum().item()\n    return correct_samples\n\ncorrect_sample(logits, b_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.011112Z","iopub.execute_input":"2024-06-02T09:41:44.011433Z","iopub.status.idle":"2024-06-02T09:41:44.066729Z","shell.execute_reply.started":"2024-06-02T09:41:44.011404Z","shell.execute_reply":"2024-06-02T09:41:44.065736Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"11790"},"metadata":{}}]},{"cell_type":"code","source":"(len(b_labels) * 4)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:50:14.569174Z","iopub.execute_input":"2024-06-02T09:50:14.569871Z","iopub.status.idle":"2024-06-02T09:50:14.576217Z","shell.execute_reply.started":"2024-06-02T09:50:14.569836Z","shell.execute_reply":"2024-06-02T09:50:14.575276Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"12664"},"metadata":{}}]},{"cell_type":"code","source":"def return_preds_and_labels(logits, b_labels):\n    logits = torch.tensor(logits).reshape(-1,4,4)\n    label_ids = b_labels.view(-1, 4, 4)\n\n    # Convert one-hot encoded labels to class indices\n    label_ids_indices = torch.argmax(label_ids, dim=-1).cpu()\n\n    # Reshape logits and label_ids\n    batch_size, num_capsules, num_classes = logits.shape\n    logits_flat = logits.view(batch_size * num_capsules, num_classes)\n    label_ids_flat = label_ids_indices.view(batch_size * num_capsules)\n\n    # Calculate loss\n    # loss = criterion(logits_flat, label_ids_flat)\n    # total_loss += batch_size * loss.item()\n    # total_samples += batch_size * num_classes\n    pred = logits_flat.argmax(dim=1)\n    return pred, label_ids_flat\n\npred, label = return_preds_and_labels(logits, b_labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.118067Z","iopub.execute_input":"2024-06-02T09:41:44.118445Z","iopub.status.idle":"2024-06-02T09:41:44.165268Z","shell.execute_reply.started":"2024-06-02T09:41:44.118417Z","shell.execute_reply":"2024-06-02T09:41:44.164097Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"label.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:46:12.605199Z","iopub.execute_input":"2024-06-02T09:46:12.605567Z","iopub.status.idle":"2024-06-02T09:46:12.611711Z","shell.execute_reply.started":"2024-06-02T09:46:12.605538Z","shell.execute_reply":"2024-06-02T09:46:12.610696Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"torch.Size([12664])"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(label, pred))\n# 0: None, 1: 'Positive', 2: 'Negative', 3: 'Normal'","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.166658Z","iopub.execute_input":"2024-06-02T09:41:44.167065Z","iopub.status.idle":"2024-06-02T09:41:44.209209Z","shell.execute_reply.started":"2024-06-02T09:41:44.167030Z","shell.execute_reply":"2024-06-02T09:41:44.208013Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.96      0.97      0.97      9498\n           1       0.88      0.88      0.88      1590\n           2       0.80      0.81      0.81      1409\n           3       0.46      0.04      0.07       167\n\n    accuracy                           0.93     12664\n   macro avg       0.78      0.67      0.68     12664\nweighted avg       0.93      0.93      0.93     12664\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# topic\n# 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'\nprint(classification_report(label.reshape(-1,4).argmax(dim = 1), pred.reshape(-1,4).argmax(dim = 1)))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:55:58.127366Z","iopub.execute_input":"2024-06-02T09:55:58.127803Z","iopub.status.idle":"2024-06-02T09:55:58.146682Z","shell.execute_reply.started":"2024-06-02T09:55:58.127761Z","shell.execute_reply":"2024-06-02T09:55:58.145809Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.45      0.95      0.61       145\n           1       0.94      0.95      0.94      2290\n           2       0.79      0.07      0.13       159\n           3       0.82      0.78      0.80       572\n\n    accuracy                           0.87      3166\n   macro avg       0.75      0.69      0.62      3166\nweighted avg       0.89      0.87      0.86      3166\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## INFERENCE","metadata":{}},{"cell_type":"code","source":"def inference(texts):\n    # segmenter\n    word_segmented_text = rdrsegmenter.tokenize(text)\n\n    # Tokenize input data\n    inputs = tokenizer(\n        texts,\n        max_length = 256, \n        padding='max_length',\n        truncation=True,\n        return_tensors=\"pt\"  # Return PyTorch tensors\n    )\n\n    inputs.to(DEVICE)\n    \n    with torch.no_grad():\n        outputs = model(inputs['input_ids'],\n                    token_type_ids=None,\n                    attention_mask=inputs['attention_mask'])\n\n    logits = outputs.logits\n    logits = logits.view(-1, 4, 4)\n    logits = torch.argmax(logits, dim=-1)[0]\n    \n    print(logits)\n    topic = ['FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM']\n    sentiment = ['None', 'Positive', 'Negative', 'Normal']\n    for i in range(len(logits)):\n        if logits[i] == 0:\n            continue\n        print('Topic:', topic[i])\n        print('Sentiment:', sentiment[logits[i]])\n\n        break\n        \ninference([\"giao bài_tập quá nhiều\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.210755Z","iopub.execute_input":"2024-06-02T09:41:44.211196Z","iopub.status.idle":"2024-06-02T09:41:44.242593Z","shell.execute_reply.started":"2024-06-02T09:41:44.211155Z","shell.execute_reply":"2024-06-02T09:41:44.241597Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"tensor([0, 2, 0, 0], device='cuda:0')\nTopic: LECTURER\nSentiment: Negative\n","output_type":"stream"}]},{"cell_type":"code","source":"logits = torch.tensor([0, 2, 0, 0], device='cuda:0')\nlogits","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.243984Z","iopub.execute_input":"2024-06-02T09:41:44.244884Z","iopub.status.idle":"2024-06-02T09:41:44.254552Z","shell.execute_reply.started":"2024-06-02T09:41:44.244839Z","shell.execute_reply":"2024-06-02T09:41:44.253522Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([0, 2, 0, 0], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"(torch.tensor([0, 2, 0, 0], device='cuda:0') == torch.tensor([0, 2, 0, 0], device='cuda:0'))","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.255937Z","iopub.execute_input":"2024-06-02T09:41:44.256423Z","iopub.status.idle":"2024-06-02T09:41:44.265425Z","shell.execute_reply.started":"2024-06-02T09:41:44.256385Z","shell.execute_reply":"2024-06-02T09:41:44.264368Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"tensor([True, True, True, True], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"inference([\"chưa giỏi chuyên_môn cho lắm\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.266591Z","iopub.execute_input":"2024-06-02T09:41:44.267213Z","iopub.status.idle":"2024-06-02T09:41:44.290519Z","shell.execute_reply.started":"2024-06-02T09:41:44.267185Z","shell.execute_reply":"2024-06-02T09:41:44.289595Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"tensor([0, 2, 0, 0], device='cuda:0')\nTopic: LECTURER\nSentiment: Negative\n","output_type":"stream"}]},{"cell_type":"code","source":"inference([\"chưa áp dụng công nghệ thông tin\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.293981Z","iopub.execute_input":"2024-06-02T09:41:44.294404Z","iopub.status.idle":"2024-06-02T09:41:44.318509Z","shell.execute_reply.started":"2024-06-02T09:41:44.294361Z","shell.execute_reply":"2024-06-02T09:41:44.317552Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"tensor([0, 2, 0, 0], device='cuda:0')\nTopic: LECTURER\nSentiment: Negative\n","output_type":"stream"}]},{"cell_type":"code","source":"inference([\"giáo_viên rất vui_tính\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.320003Z","iopub.execute_input":"2024-06-02T09:41:44.320432Z","iopub.status.idle":"2024-06-02T09:41:44.344847Z","shell.execute_reply.started":"2024-06-02T09:41:44.320394Z","shell.execute_reply":"2024-06-02T09:41:44.343777Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"tensor([0, 1, 0, 0], device='cuda:0')\nTopic: LECTURER\nSentiment: Positive\n","output_type":"stream"}]},{"cell_type":"code","source":"inference([\"có đôi_lúc nói hơi nhanh làm sinh_viên không theo kịp\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.346050Z","iopub.execute_input":"2024-06-02T09:41:44.346328Z","iopub.status.idle":"2024-06-02T09:41:44.370299Z","shell.execute_reply.started":"2024-06-02T09:41:44.346303Z","shell.execute_reply":"2024-06-02T09:41:44.369371Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"tensor([0, 2, 0, 0], device='cuda:0')\nTopic: LECTURER\nSentiment: Negative\n","output_type":"stream"}]},{"cell_type":"code","source":"inference([\"giáo trình cũ không có gì mới\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-02T09:41:44.371574Z","iopub.execute_input":"2024-06-02T09:41:44.371948Z","iopub.status.idle":"2024-06-02T09:41:44.397008Z","shell.execute_reply.started":"2024-06-02T09:41:44.371913Z","shell.execute_reply":"2024-06-02T09:41:44.396043Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([0, 0, 0, 2], device='cuda:0')\nTopic: TRAINING_PROGRAM\nSentiment: Negative\n","output_type":"stream"}]}]}