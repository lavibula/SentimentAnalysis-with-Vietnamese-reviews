{"cells":[{"cell_type":"markdown","metadata":{},"source":["# DATA"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.309139Z","iopub.status.busy":"2024-05-26T10:53:12.308062Z","iopub.status.idle":"2024-05-26T10:53:12.313956Z","shell.execute_reply":"2024-05-26T10:53:12.312955Z","shell.execute_reply.started":"2024-05-26T10:53:12.309091Z"},"trusted":true},"outputs":[],"source":["import warnings\n","\n","# Ignore all warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.353415Z","iopub.status.busy":"2024-05-26T10:53:12.353100Z","iopub.status.idle":"2024-05-26T10:53:12.483453Z","shell.execute_reply":"2024-05-26T10:53:12.482435Z","shell.execute_reply.started":"2024-05-26T10:53:12.353389Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   Topic  Sentiment                                           Sentence\n","0      1          2                           slide giáo trình đầy đủ \n","1      0          2       nhiệt tình giảng dạy  gần gũi với sinh viên \n","2      1          0                đi học đầy đủ full điểm chuyên cần \n","3      0          0  chưa áp dụng công nghệ thông tin và các thiết ...\n","4      0          2  thầy giảng bài hay  có nhiều bài tập ví dụ nga...\n","              Topic Sentiment  \\\n","0  TRAINING_PROGRAM  POSITIVE   \n","1          LECTURER  POSITIVE   \n","2  TRAINING_PROGRAM  NEGATIVE   \n","3          LECTURER  NEGATIVE   \n","4          LECTURER  POSITIVE   \n","\n","                                            Sentence  \n","0                           slide giáo trình đầy đủ   \n","1       nhiệt tình giảng dạy  gần gũi với sinh viên   \n","2                đi học đầy đủ full điểm chuyên cần   \n","3  chưa áp dụng công nghệ thông tin và các thiết ...  \n","4  thầy giảng bài hay  có nhiều bài tập ví dụ nga...  \n"]}],"source":["import pandas as pd\n","import re\n","\n","# Define the file paths\n","topics_file ='/kaggle/input/uit-vsfc/train/topics.txt'\n","sentiments_file ='/kaggle/input/uit-vsfc/train/sentiments.txt'\n","sents_file ='/kaggle/input/uit-vsfc/train/sents.txt'\n","\n","# Read the files\n","topics = pd.read_csv(topics_file, header=None, names=['Topic'])\n","sentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n","\n","# Read the sentences file line by line\n","with open(sents_file,'r', encoding='utf-8') as file:\n","    sentences = file.readlines()\n","\n","# Remove newline characters and punctuation from sentences, and split by whitespace\n","sentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n","\n","# Create a DataFrame for sentences\n","sentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n","\n","# Combine the DataFrames along the columns\n","combined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n","\n","# Display the first few rows of the combined DataFrame\n","print(combined_df.head())\n","\n","# Replace values in'Topic'column\n","topic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\n","combined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n","\n","# Replace values in'Sentiment'column\n","sentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\n","combined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n","\n","# Display the modified DataFrame\n","print(combined_df.head())"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.486212Z","iopub.status.busy":"2024-05-26T10:53:12.485375Z","iopub.status.idle":"2024-05-26T10:53:12.503773Z","shell.execute_reply":"2024-05-26T10:53:12.502858Z","shell.execute_reply.started":"2024-05-26T10:53:12.486176Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic</th>\n","      <th>Sentiment</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>POSITIVE</td>\n","      <td>slide giáo trình đầy đủ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>nhiệt tình giảng dạy  gần gũi với sinh viên</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>NEGATIVE</td>\n","      <td>đi học đầy đủ full điểm chuyên cần</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>chưa áp dụng công nghệ thông tin và các thiết ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>thầy giảng bài hay  có nhiều bài tập ví dụ nga...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11421</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>NEGATIVE</td>\n","      <td>chỉ vì môn game mà em học hai lần mà không qua...</td>\n","    </tr>\n","    <tr>\n","      <th>11422</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>em cảm ơn cô nhiều</td>\n","    </tr>\n","    <tr>\n","      <th>11423</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giao bài tập quá nhiều</td>\n","    </tr>\n","    <tr>\n","      <th>11424</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giáo viên dạy dễ hiểu  nhiệt tình</td>\n","    </tr>\n","    <tr>\n","      <th>11425</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>gói gọn doubledot hay  tận tình  phù hợp với m...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11426 rows × 3 columns</p>\n","</div>"],"text/plain":["                  Topic Sentiment  \\\n","0      TRAINING_PROGRAM  POSITIVE   \n","1              LECTURER  POSITIVE   \n","2      TRAINING_PROGRAM  NEGATIVE   \n","3              LECTURER  NEGATIVE   \n","4              LECTURER  POSITIVE   \n","...                 ...       ...   \n","11421  TRAINING_PROGRAM  NEGATIVE   \n","11422          LECTURER  POSITIVE   \n","11423          LECTURER  NEGATIVE   \n","11424          LECTURER  POSITIVE   \n","11425          LECTURER  POSITIVE   \n","\n","                                                Sentence  \n","0                               slide giáo trình đầy đủ   \n","1           nhiệt tình giảng dạy  gần gũi với sinh viên   \n","2                    đi học đầy đủ full điểm chuyên cần   \n","3      chưa áp dụng công nghệ thông tin và các thiết ...  \n","4      thầy giảng bài hay  có nhiều bài tập ví dụ nga...  \n","...                                                  ...  \n","11421  chỉ vì môn game mà em học hai lần mà không qua...  \n","11422                                em cảm ơn cô nhiều   \n","11423                            giao bài tập quá nhiều   \n","11424                 giáo viên dạy dễ hiểu  nhiệt tình   \n","11425  gói gọn doubledot hay  tận tình  phù hợp với m...  \n","\n","[11426 rows x 3 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["combined_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.526585Z","iopub.status.busy":"2024-05-26T10:53:12.526280Z","iopub.status.idle":"2024-05-26T10:53:12.594208Z","shell.execute_reply":"2024-05-26T10:53:12.593361Z","shell.execute_reply.started":"2024-05-26T10:53:12.526561Z"},"trusted":true},"outputs":[],"source":["combined_df.to_csv('train.csv', index=False)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.600350Z","iopub.status.busy":"2024-05-26T10:53:12.600006Z","iopub.status.idle":"2024-05-26T10:53:12.611420Z","shell.execute_reply":"2024-05-26T10:53:12.610346Z","shell.execute_reply.started":"2024-05-26T10:53:12.600322Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic</th>\n","      <th>Sentiment</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>POSITIVE</td>\n","      <td>slide giáo trình đầy đủ</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>nhiệt tình giảng dạy  gần gũi với sinh viên</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>NEGATIVE</td>\n","      <td>đi học đầy đủ full điểm chuyên cần</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>chưa áp dụng công nghệ thông tin và các thiết ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>thầy giảng bài hay  có nhiều bài tập ví dụ nga...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              Topic Sentiment  \\\n","0  TRAINING_PROGRAM  POSITIVE   \n","1          LECTURER  POSITIVE   \n","2  TRAINING_PROGRAM  NEGATIVE   \n","3          LECTURER  NEGATIVE   \n","4          LECTURER  POSITIVE   \n","\n","                                            Sentence  \n","0                           slide giáo trình đầy đủ   \n","1       nhiệt tình giảng dạy  gần gũi với sinh viên   \n","2                đi học đầy đủ full điểm chuyên cần   \n","3  chưa áp dụng công nghệ thông tin và các thiết ...  \n","4  thầy giảng bài hay  có nhiều bài tập ví dụ nga...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["combined_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Valid"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.672155Z","iopub.status.busy":"2024-05-26T10:53:12.671764Z","iopub.status.idle":"2024-05-26T10:53:12.726928Z","shell.execute_reply":"2024-05-26T10:53:12.725752Z","shell.execute_reply.started":"2024-05-26T10:53:12.672125Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   Topic  Sentiment                                           Sentence\n","0      1          0                            giáo trình chưa cụ thể \n","1      0          0                                    giảng buồn ngủ \n","2      0          2                       giáo viên vui tính  tận tâm \n","3      0          0  giảng viên nên giao bài tập nhiều hơn  chia nh...\n","4      0          0  giảng viên cần giảng bài chi tiết hơn  đi sâu ...\n","              Topic Sentiment  \\\n","0  TRAINING_PROGRAM  NEGATIVE   \n","1          LECTURER  NEGATIVE   \n","2          LECTURER  POSITIVE   \n","3          LECTURER  NEGATIVE   \n","4          LECTURER  NEGATIVE   \n","\n","                                            Sentence  \n","0                            giáo trình chưa cụ thể   \n","1                                    giảng buồn ngủ   \n","2                       giáo viên vui tính  tận tâm   \n","3  giảng viên nên giao bài tập nhiều hơn  chia nh...  \n","4  giảng viên cần giảng bài chi tiết hơn  đi sâu ...  \n"]}],"source":["import pandas as pd\n","import re\n","\n","# Define the file paths\n","topics_file ='/kaggle/input/uit-vsfc/dev/topics.txt'\n","sentiments_file ='/kaggle/input/uit-vsfc/dev/sentiments.txt'\n","sents_file ='/kaggle/input/uit-vsfc/dev/sents.txt'\n","\n","# Read the files\n","topics = pd.read_csv(topics_file, header=None, names=['Topic'])\n","sentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n","\n","# Read the sentences file line by line\n","with open(sents_file,'r', encoding='utf-8') as file:\n","    sentences = file.readlines()\n","\n","# Remove newline characters and punctuation from sentences, and split by whitespace\n","sentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n","\n","# Create a DataFrame for sentences\n","sentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n","\n","# Combine the DataFrames along the columns\n","combined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n","\n","# Display the first few rows of the combined DataFrame\n","print(combined_df.head())\n","\n","# Replace values in'Topic'column\n","topic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\n","combined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n","\n","# Replace values in'Sentiment'column\n","sentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\n","combined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n","\n","# Display the modified DataFrame\n","print(combined_df.head())"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.732233Z","iopub.status.busy":"2024-05-26T10:53:12.731851Z","iopub.status.idle":"2024-05-26T10:53:12.745549Z","shell.execute_reply":"2024-05-26T10:53:12.744535Z","shell.execute_reply.started":"2024-05-26T10:53:12.732203Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic</th>\n","      <th>Sentiment</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>NEGATIVE</td>\n","      <td>giáo trình chưa cụ thể</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giảng buồn ngủ</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giáo viên vui tính  tận tâm</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giảng viên nên giao bài tập nhiều hơn  chia nh...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giảng viên cần giảng bài chi tiết hơn  đi sâu ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1578</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>hướng dẫn lab mơ hồ</td>\n","    </tr>\n","    <tr>\n","      <th>1579</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>thầy cho chúng em những bài tập mang tính thực...</td>\n","    </tr>\n","    <tr>\n","      <th>1580</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>thầy không dạy nhiều chủ yếu cho sinh viên tự ...</td>\n","    </tr>\n","    <tr>\n","      <th>1581</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>NEGATIVE</td>\n","      <td>em muốn đổi tên môn học vì tên môn là lập trìn...</td>\n","    </tr>\n","    <tr>\n","      <th>1582</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>thầy vừa dạy vừa chat hoặc gọi điện thoại thườ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1583 rows × 3 columns</p>\n","</div>"],"text/plain":["                 Topic Sentiment  \\\n","0     TRAINING_PROGRAM  NEGATIVE   \n","1             LECTURER  NEGATIVE   \n","2             LECTURER  POSITIVE   \n","3             LECTURER  NEGATIVE   \n","4             LECTURER  NEGATIVE   \n","...                ...       ...   \n","1578          LECTURER  NEGATIVE   \n","1579          LECTURER  POSITIVE   \n","1580          LECTURER  NEGATIVE   \n","1581  TRAINING_PROGRAM  NEGATIVE   \n","1582          LECTURER  NEGATIVE   \n","\n","                                               Sentence  \n","0                               giáo trình chưa cụ thể   \n","1                                       giảng buồn ngủ   \n","2                          giáo viên vui tính  tận tâm   \n","3     giảng viên nên giao bài tập nhiều hơn  chia nh...  \n","4     giảng viên cần giảng bài chi tiết hơn  đi sâu ...  \n","...                                                 ...  \n","1578                               hướng dẫn lab mơ hồ   \n","1579  thầy cho chúng em những bài tập mang tính thực...  \n","1580  thầy không dạy nhiều chủ yếu cho sinh viên tự ...  \n","1581  em muốn đổi tên môn học vì tên môn là lập trìn...  \n","1582  thầy vừa dạy vừa chat hoặc gọi điện thoại thườ...  \n","\n","[1583 rows x 3 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["combined_df"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.799562Z","iopub.status.busy":"2024-05-26T10:53:12.799254Z","iopub.status.idle":"2024-05-26T10:53:12.813095Z","shell.execute_reply":"2024-05-26T10:53:12.812075Z","shell.execute_reply.started":"2024-05-26T10:53:12.799536Z"},"trusted":true},"outputs":[],"source":["combined_df.to_csv('dev.csv', index=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.866810Z","iopub.status.busy":"2024-05-26T10:53:12.866507Z","iopub.status.idle":"2024-05-26T10:53:12.876620Z","shell.execute_reply":"2024-05-26T10:53:12.875649Z","shell.execute_reply.started":"2024-05-26T10:53:12.866786Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic</th>\n","      <th>Sentiment</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>NEGATIVE</td>\n","      <td>giáo trình chưa cụ thể</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giảng buồn ngủ</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giáo viên vui tính  tận tâm</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giảng viên nên giao bài tập nhiều hơn  chia nh...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giảng viên cần giảng bài chi tiết hơn  đi sâu ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              Topic Sentiment  \\\n","0  TRAINING_PROGRAM  NEGATIVE   \n","1          LECTURER  NEGATIVE   \n","2          LECTURER  POSITIVE   \n","3          LECTURER  NEGATIVE   \n","4          LECTURER  NEGATIVE   \n","\n","                                            Sentence  \n","0                            giáo trình chưa cụ thể   \n","1                                    giảng buồn ngủ   \n","2                       giáo viên vui tính  tận tâm   \n","3  giảng viên nên giao bài tập nhiều hơn  chia nh...  \n","4  giảng viên cần giảng bài chi tiết hơn  đi sâu ...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["combined_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Test"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.933645Z","iopub.status.busy":"2024-05-26T10:53:12.933318Z","iopub.status.idle":"2024-05-26T10:53:12.997497Z","shell.execute_reply":"2024-05-26T10:53:12.996363Z","shell.execute_reply.started":"2024-05-26T10:53:12.933618Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   Topic  Sentiment                                           Sentence\n","0      0          2                            nói tiếng anh lưu loát \n","1      0          2                            giáo viên rất vui tính \n","2      0          2                                     cô max có tâm \n","3      0          2                         giảng bài thu hút  dí dỏm \n","4      0          0  giáo viên không giảng dạy kiến thức  hướng dẫn...\n","      Topic Sentiment                                           Sentence\n","0  LECTURER  POSITIVE                            nói tiếng anh lưu loát \n","1  LECTURER  POSITIVE                            giáo viên rất vui tính \n","2  LECTURER  POSITIVE                                     cô max có tâm \n","3  LECTURER  POSITIVE                         giảng bài thu hút  dí dỏm \n","4  LECTURER  NEGATIVE  giáo viên không giảng dạy kiến thức  hướng dẫn...\n"]}],"source":["import pandas as pd\n","import re\n","\n","# Define the file paths\n","topics_file ='/kaggle/input/uit-vsfc/test/topics.txt'\n","sentiments_file ='/kaggle/input/uit-vsfc/test/sentiments.txt'\n","sents_file ='/kaggle/input/uit-vsfc/test/sents.txt'\n","\n","# Read the files\n","topics = pd.read_csv(topics_file, header=None, names=['Topic'])\n","sentiments = pd.read_csv(sentiments_file, header=None, names=['Sentiment'])\n","\n","# Read the sentences file line by line\n","with open(sents_file,'r', encoding='utf-8') as file:\n","    sentences = file.readlines()\n","\n","# Remove newline characters and punctuation from sentences, and split by whitespace\n","sentences = [re.sub(r'[^\\w\\s]','', sentence.strip().lower()) for sentence in sentences]\n","\n","# Create a DataFrame for sentences\n","sentences_df = pd.DataFrame(sentences, columns=['Sentence'])\n","\n","# Combine the DataFrames along the columns\n","combined_df = pd.concat([topics, sentiments, sentences_df], axis=1)\n","\n","# Display the first few rows of the combined DataFrame\n","print(combined_df.head())\n","\n","# Replace values in'Topic'column\n","topic_mapping = {0:'LECTURER', 1:'TRAINING_PROGRAM', 2:'FACILITY', 3:'OTHERS'}\n","combined_df['Topic'] = combined_df['Topic'].replace(topic_mapping)\n","\n","# Replace values in'Sentiment'column\n","sentiment_mapping = {0:'NEGATIVE', 1:'NEUTRAL', 2:'POSITIVE'}\n","combined_df['Sentiment'] = combined_df['Sentiment'].replace(sentiment_mapping)\n","\n","# Display the modified DataFrame\n","print(combined_df.head())"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:12.999452Z","iopub.status.busy":"2024-05-26T10:53:12.999115Z","iopub.status.idle":"2024-05-26T10:53:13.012341Z","shell.execute_reply":"2024-05-26T10:53:13.011173Z","shell.execute_reply.started":"2024-05-26T10:53:12.999424Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic</th>\n","      <th>Sentiment</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>nói tiếng anh lưu loát</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giáo viên rất vui tính</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>cô max có tâm</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giảng bài thu hút  dí dỏm</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giáo viên không giảng dạy kiến thức  hướng dẫn...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3161</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>các slide khó hiểu  ngôn ngữ trong slide phức ...</td>\n","    </tr>\n","    <tr>\n","      <th>3162</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giáo viên giảng dạy có tâm huyết</td>\n","    </tr>\n","    <tr>\n","      <th>3163</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>chia sẻ cho em nhiều điều hay</td>\n","    </tr>\n","    <tr>\n","      <th>3164</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>em tiếp thu chậm</td>\n","    </tr>\n","    <tr>\n","      <th>3165</th>\n","      <td>TRAINING_PROGRAM</td>\n","      <td>NEUTRAL</td>\n","      <td>em có học ở một trung tâm tiếng anh ở ngoài tr...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3166 rows × 3 columns</p>\n","</div>"],"text/plain":["                 Topic Sentiment  \\\n","0             LECTURER  POSITIVE   \n","1             LECTURER  POSITIVE   \n","2             LECTURER  POSITIVE   \n","3             LECTURER  POSITIVE   \n","4             LECTURER  NEGATIVE   \n","...                ...       ...   \n","3161          LECTURER  NEGATIVE   \n","3162          LECTURER  POSITIVE   \n","3163          LECTURER  POSITIVE   \n","3164          LECTURER  NEGATIVE   \n","3165  TRAINING_PROGRAM   NEUTRAL   \n","\n","                                               Sentence  \n","0                               nói tiếng anh lưu loát   \n","1                               giáo viên rất vui tính   \n","2                                        cô max có tâm   \n","3                            giảng bài thu hút  dí dỏm   \n","4     giáo viên không giảng dạy kiến thức  hướng dẫn...  \n","...                                                 ...  \n","3161  các slide khó hiểu  ngôn ngữ trong slide phức ...  \n","3162                  giáo viên giảng dạy có tâm huyết   \n","3163                     chia sẻ cho em nhiều điều hay   \n","3164                                  em tiếp thu chậm   \n","3165  em có học ở một trung tâm tiếng anh ở ngoài tr...  \n","\n","[3166 rows x 3 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["combined_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:13.082326Z","iopub.status.busy":"2024-05-26T10:53:13.081996Z","iopub.status.idle":"2024-05-26T10:53:13.106416Z","shell.execute_reply":"2024-05-26T10:53:13.105414Z","shell.execute_reply.started":"2024-05-26T10:53:13.082298Z"},"trusted":true},"outputs":[],"source":["combined_df.to_csv('test.csv', index=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:13.409567Z","iopub.status.busy":"2024-05-26T10:53:13.408559Z","iopub.status.idle":"2024-05-26T10:53:13.420776Z","shell.execute_reply":"2024-05-26T10:53:13.419739Z","shell.execute_reply.started":"2024-05-26T10:53:13.409517Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Topic</th>\n","      <th>Sentiment</th>\n","      <th>Sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>nói tiếng anh lưu loát</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giáo viên rất vui tính</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>cô max có tâm</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LECTURER</td>\n","      <td>POSITIVE</td>\n","      <td>giảng bài thu hút  dí dỏm</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LECTURER</td>\n","      <td>NEGATIVE</td>\n","      <td>giáo viên không giảng dạy kiến thức  hướng dẫn...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Topic Sentiment                                           Sentence\n","0  LECTURER  POSITIVE                            nói tiếng anh lưu loát \n","1  LECTURER  POSITIVE                            giáo viên rất vui tính \n","2  LECTURER  POSITIVE                                     cô max có tâm \n","3  LECTURER  POSITIVE                         giảng bài thu hút  dí dỏm \n","4  LECTURER  NEGATIVE  giáo viên không giảng dạy kiến thức  hướng dẫn..."]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["combined_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Load dataset\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:13.700704Z","iopub.status.busy":"2024-05-26T10:53:13.699866Z","iopub.status.idle":"2024-05-26T10:53:13.761596Z","shell.execute_reply":"2024-05-26T10:53:13.760454Z","shell.execute_reply.started":"2024-05-26T10:53:13.700667Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'Sentence', 'Sentiment', 'Topic'}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Đường dẫn tới các file\n","train_file ='/kaggle/working/train.csv'\n","val_file ='/kaggle/working/dev.csv'\n","test_file ='/kaggle/working/test.csv'\n","\n","# Đọc các file CSV\n","train_df = pd.read_csv(train_file)\n","val_df = pd.read_csv(val_file)\n","test_df = pd.read_csv(test_file)\n","\n","# Lấy tất cả các tên cột từ cả ba DataFrame\n","all_columns = set(train_df.columns).union(set(val_df.columns)).union(set(test_df.columns))\n","\n","all_columns"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocess data"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:13.769700Z","iopub.status.busy":"2024-05-26T10:53:13.769401Z","iopub.status.idle":"2024-05-26T10:53:30.776182Z","shell.execute_reply":"2024-05-26T10:53:30.774569Z","shell.execute_reply.started":"2024-05-26T10:53:13.769674Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting underthesea\n","  Downloading underthesea-6.8.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\n","Collecting python-crfsuite>=0.9.6 (from underthesea)\n","  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.31.0)\n","Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.4.0)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)\n","Collecting underthesea-core==1.0.4 (from underthesea)\n","  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)\n","Downloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\n","Successfully installed python-crfsuite-0.9.10 underthesea-6.8.0 underthesea-core-1.0.4\n"]}],"source":["!pip install underthesea"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:30.778716Z","iopub.status.busy":"2024-05-26T10:53:30.778404Z","iopub.status.idle":"2024-05-26T10:53:45.158518Z","shell.execute_reply":"2024-05-26T10:53:45.157347Z","shell.execute_reply.started":"2024-05-26T10:53:30.778685Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyvi\n","  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pyvi) (1.2.2)\n","Collecting sklearn-crfsuite (from pyvi)\n","  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.26.4)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pyvi) (3.2.0)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.10)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n","Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n","Requirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.10/site-packages (from sklearn-crfsuite->pyvi) (4.66.1)\n","Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n","Installing collected packages: sklearn-crfsuite, pyvi\n","Successfully installed pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"]}],"source":["!pip install pyvi"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:45.160314Z","iopub.status.busy":"2024-05-26T10:53:45.159961Z","iopub.status.idle":"2024-05-26T10:53:46.788861Z","shell.execute_reply":"2024-05-26T10:53:46.787863Z","shell.execute_reply.started":"2024-05-26T10:53:45.160279Z"},"trusted":true},"outputs":[],"source":["import regex as re\n","import string\n","import emoji\n","\n","from nltk import flatten\n","import unicodedata\n","from underthesea import word_tokenize\n","from pyvi import ViTokenizer"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:46.791807Z","iopub.status.busy":"2024-05-26T10:53:46.791485Z","iopub.status.idle":"2024-05-26T10:53:46.817763Z","shell.execute_reply":"2024-05-26T10:53:46.816926Z","shell.execute_reply.started":"2024-05-26T10:53:46.791778Z"},"trusted":true},"outputs":[],"source":["# 1. Loại bỏ các thẻ HTML\n","def remove_HTML(text):\n","    clean = re.compile('<.*?>')\n","    return re.sub(clean,'', text)\n","\n","# 2. Chuyển đổi các ký tự Unicode về dạng chuẩn\n","def convert_unicode(text):\n","    return unicodedata.normalize('NFC', text)\n","\n","# 3. Loại bỏ các ký tự kéo dài\n","def remove_elongated_chars(text):\n","    replacements = {\n","       'a':'àáảãạăằắẳẵặâầấẩẫậ',\n","       'e':'èéẻẽẹêềếểễệ',\n","       'i':'ìíỉĩị',\n","       'o':'òóỏõọôồốổỗộơờớởỡợ',\n","       'u':'ùúủũụưừứửữự',\n","       'y':'ỳýỷỹỵ',\n","       'd':'đ',\n","       'A':'ÀÁẢÃẠĂẰẮẲẴẶÂẦẤẨẪẬ',\n","       'E':'ÈÉẺẼẸÊỀẾỂỄỆ',\n","       'I':'ÌÍỈĨỊ',\n","       'O':'ÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢ',\n","       'U':'ÙÚỦŨỤƯỪỨỬỮỰ',\n","       'Y':'ỲÝỶỸỴ',\n","       'D':'Đ'\n","    }\n","    \n","    for char, replacements_str in replacements.items():\n","        pattern = rf\"({char})\\1+\"\n","        text = re.sub(pattern, char, text)\n","    \n","    pattern = r\"(\\w)\\1+\"\n","    text = re.sub(pattern, r'\\1', text)\n","    return text\n","\n","# 4. Xử lý các từ phủ định\n","def handle_negation(text):\n","    not_words = {\"không\",'không hề', \"chẳng\", \"chưa\", \"không phải\", \"chả\", \"mất\",\n","                 \"thiếu\", \"đếch\", \"đéo\", \"kém\", \"nỏ\", \"not\",\n","                 \"bớt\", \"không bao giờ\", \"chưa bao giờ\"}\n","    not_words = sorted(not_words, key=len, reverse=True)\n","    pattern = r'\\b(?:'+'|'.join(re.escape(word) for word in not_words) + r')\\b'\n","    text = re.sub(pattern,'không', text, flags=re.IGNORECASE)\n","    return text\n","\n","# 5. Chuẩn hóa các từ viết tắt thường gặp\n","def normalize_acronyms(text):\n","    acronyms = {\n","       'ô kêi':'ok','okie':'ok','o kê':'ok',\n","       'okey':'ok','ôkê':'ok','oki':'ok','oke': 'ok','okay':'ok','okê':'ok',\n","       'tks': u'cám ơn','thks': u'cám ơn','thanks': u'cám ơn','ths': u'cám ơn','thank': u'cám ơn',\n","       '⭐':'star','*':'star','🌟':'star',\n","       'kg': u'không','not': u'không', u'kg': u'không','\"k': u'không','kh':u'không','kô':u'không','hok':u'không','kp': u'không phải',u'kô': u'không','\"ko': u'không', u'ko': u'không', u'k': u'không','khong': u'không', u'hok': u'không',\n","       'cute': u'dễ thương','vs': u'với','wa':'quá','wá': u'quá','j': u'gì','“':'',\n","       'sz': u'cỡ','size': u'cỡ', u'đx': u'được','dk': u'được','dc': u'được','đk': u'được',\n","       'đc': u'được','authentic': u'chuẩn chính hãng',u'aut': u'chuẩn chính hãng', u'auth': u'chuẩn chính hãng','store': u'cửa hàng',\n","       'shop': u'cửa hàng','sp': u'sản phẩm','gud': u'tốt','god': u'tốt','wel done':'tốt','good': u'tốt','gút': u'tốt',\n","       'sấu': u'xấu','gut': u'tốt', u'tot': u'tốt', u'nice': u'tốt','perfect':'rất tốt','bt': u'bình thường',\n","       'time': u'thời gian','qá': u'quá', u'ship': u'giao hàng', u'm': u'mình', u'mik': u'mình',\n","       'ể':'ể','product':'sản phẩm','quality':'chất lượng','chat':'chất','excelent':'hoàn hảo','bad':'tệ','fresh':'tươi','sad':'tệ',\n","       'date': u'hạn sử dụng','hsd': u'hạn sử dụng','quickly': u'nhanh','quick': u'nhanh','fast': u'nhanh','delivery': u'giao hàng',u'síp': u'giao hàng',\n","       'beautiful': u'đẹp tuyệt vời', u'tl': u'trả lời', u'r': u'rồi', u'shopE': u'cửa hàng',u'order': u'đặt hàng',\n","       'chất lg': u'chất lượng',u'sd': u'sử dụng',u'dt': u'điện thoại',u'nt': u'nhắn tin',u'tl': u'trả lời',u'sài': u'xài',u'bjo':u'bao giờ',\n","       'thick': u'thích','thik': u'thích', u'sop': u'cửa hàng', u'shop': u'cửa hàng', \n","       'fb':'facebook','face':'facebook','very': u'rất',u'quả ng':u'quảng ',\n","       'dep': u'đẹp',u'xau': u'xấu','delicious': u'ngon', u'hàg': u'hàng', u'qủa': u'quả',\n","       'iu': u'yêu','fake': u'giả mạo','trl':'trả lời',\n","       'por': u'tệ','poor': u'tệ','ib':u'nhắn tin','rep':u'trả lời',u'fback':'feedback','fedback':'feedback',\n","       'max': u'cực kỳ',\n","       'full':'đầy đủ', 'ful':'đầy đủ'\n","    }\n","    words = text.split()\n","    normalized_text =' '.join([acronyms.get(word.lower(), word) for word in words])\n","    return normalized_text\n","\n","# 6. Phân đoạn từ cho tiếng Việt\n","def word_segmentation(text):\n","    return word_tokenize(text, format=\"text\")\n","\n","# 7. Loại bỏ các ký tự không cần thiết khỏi văn bản\n","def remove_unnecessary_characters(text):\n","    text = re.sub(r'\\s+', ' ', text)  # Loại bỏ khoảng trắng thừa\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Loại bỏ các ký tự đặc biệt\n","    return text.strip()\n","\n","# 8. Kết hợp các hàm để tiền xử lý văn bản\n","def text_preprocess(text):\n","    text = remove_HTML(text)\n","    text = normalize_acronyms(text)\n","    text = convert_unicode(text)\n","    text = remove_elongated_chars(text)\n","    text = handle_negation(text)\n","    #text = word_segmentation(text)\n","    text = remove_unnecessary_characters(text)\n","    return text"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:46.819203Z","iopub.status.busy":"2024-05-26T10:53:46.818828Z","iopub.status.idle":"2024-05-26T10:53:46.842678Z","shell.execute_reply":"2024-05-26T10:53:46.841653Z","shell.execute_reply.started":"2024-05-26T10:53:46.819168Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["slide giáo trình đầy đủ\n"]}],"source":["text = \"slide giáo trình đầy đủ\"\n","print(text_preprocess(text))"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:53:46.844781Z","iopub.status.busy":"2024-05-26T10:53:46.844095Z","iopub.status.idle":"2024-05-26T10:55:15.461519Z","shell.execute_reply":"2024-05-26T10:55:15.460460Z","shell.execute_reply.started":"2024-05-26T10:53:46.844745Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tf-models-official\n","  Downloading tf_models_official-2.16.0-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (3.0.8)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.5.0)\n","Collecting gin-config (from tf-models-official)\n","  Downloading gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.126.0)\n","Collecting immutabledict (from tf-models-official)\n","  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n","Requirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.6.12)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (3.7.5)\n","Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.26.4)\n","Requirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.1.3)\n","Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.9.0.80)\n","Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (2.1.4)\n","Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (5.9.3)\n","Requirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (9.0.0)\n","Collecting pycocotools (from tf-models-official)\n","  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n","Requirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (6.0.1)\n","Collecting sacrebleu (from tf-models-official)\n","  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.11.4)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.2.0)\n","Collecting seqeval (from tf-models-official)\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (1.16.0)\n","Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (4.9.4)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official) (0.16.1)\n","Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official)\n","  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n","Collecting tensorflow-text~=2.16.1 (from tf-models-official)\n","  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n","Collecting tensorflow~=2.16.1 (from tf-models-official)\n","  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n","Collecting tf-keras>=2.16.0 (from tf-models-official)\n","  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting tf-slim>=1.1.0 (from tf-models-official)\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.21.0)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.26.1)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.2.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n","Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2024.2.2)\n","Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.9.0.post0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (2.31.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (4.66.1)\n","Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (8.0.4)\n","Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (1.26.18)\n","Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official) (6.1.0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official) (2023.4)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.10.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (16.0.6)\n","Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.16.1->tf-models-official)\n","  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (21.3)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.20.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (69.0.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (4.9.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (1.51.1)\n","Collecting tensorboard<2.17,>=2.16 (from tensorflow~=2.16.1->tf-models-official)\n","  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (3.2.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.16.1->tf-models-official) (0.35.0)\n","Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official) (3.1.1)\n","Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.5.1)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (0.3.0)\n","Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official) (4.9)\n","Collecting portalocker (from sacrebleu->tf-models-official)\n","  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (2023.12.25)\n","Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.9.0)\n","Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (0.4.6)\n","Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official) (5.2.1)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official) (1.2.2)\n","Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (8.1.7)\n","Requirement already satisfied: etils>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (1.6.0)\n","Requirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\n","Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.14.0)\n","Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n","Requirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official) (0.5.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.16.1->tf-models-official) (0.42.0)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (2024.2.0)\n","Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (6.1.1)\n","Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official) (3.17.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.62.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official) (4.2.4)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (13.7.0)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.0.8)\n","Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.6)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (3.0.2)\n","Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official) (0.5.1)\n","Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow~=2.16.1->tf-models-official) (2.1.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow~=2.16.1->tf-models-official) (0.1.2)\n","Downloading tf_models_official-2.16.0-py2.py3-none-any.whl (2.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n","Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=0188b2cd398c4b27cd09f4b63a4c4246a0a18e17277577a4ab32950be32e633b\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: gin-config, tf-slim, tensorflow-model-optimization, portalocker, ml-dtypes, immutabledict, tensorboard, sacrebleu, seqeval, pycocotools, tensorflow, tf-keras, tensorflow-text, tf-models-official\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.2.0\n","    Uninstalling ml-dtypes-0.2.0:\n","      Successfully uninstalled ml-dtypes-0.2.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.1\n","    Uninstalling tensorboard-2.15.1:\n","      Successfully uninstalled tensorboard-2.15.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","  Attempting uninstall: tf-keras\n","    Found existing installation: tf_keras 2.15.1\n","    Uninstalling tf_keras-2.15.1:\n","      Successfully uninstalled tf_keras-2.15.1\n","  Attempting uninstall: tensorflow-text\n","    Found existing installation: tensorflow-text 2.15.0\n","    Uninstalling tensorflow-text-2.15.0:\n","      Successfully uninstalled tensorflow-text-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","keras-nlp 0.9.3 requires keras-core, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gin-config-0.5.0 immutabledict-4.2.0 ml-dtypes-0.3.2 portalocker-2.8.2 pycocotools-2.0.7 sacrebleu-2.4.2 seqeval-1.2.2 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-model-optimization-0.8.0 tensorflow-text-2.16.1 tf-keras-2.16.0 tf-models-official-2.16.0 tf-slim-1.1.0\n"]}],"source":["!pip install tf-models-official\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:15.463297Z","iopub.status.busy":"2024-05-26T10:55:15.462954Z","iopub.status.idle":"2024-05-26T10:55:25.914052Z","shell.execute_reply":"2024-05-26T10:55:25.912946Z","shell.execute_reply.started":"2024-05-26T10:55:15.463261Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import tensorflow as tf\n","from transformers import TFAutoModel, AutoTokenizer\n","from tensorflow.keras.layers import Input, Dense, Dropout, concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import plot_model\n","from official.nlp import optimization\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:25.916387Z","iopub.status.busy":"2024-05-26T10:55:25.915538Z","iopub.status.idle":"2024-05-26T10:55:27.200977Z","shell.execute_reply":"2024-05-26T10:55:27.199957Z","shell.execute_reply.started":"2024-05-26T10:55:25.916348Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae9b5928bcbc4c4fa66f258d961ee21f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f9e49d3a1d448f2afe4b333e9f439a3","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5b95071991b4b5e95d8dcb0ca6de2fb","version_major":2,"version_minor":0},"text/plain":["bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ce5da06b93a4bfa985aa0ef943f07cf","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'vinai/phobert-base': 256, 'vinai/phobert-large': 256}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Define pre-trained model and tokenizer\n","PRETRAINED_MODEL = 'vinai/phobert-base'  # Choose your preferred Vietnamese BERT model\n","tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n","tokenizer.max_model_input_sizes"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:27.202532Z","iopub.status.busy":"2024-05-26T10:55:27.202248Z","iopub.status.idle":"2024-05-26T10:55:27.207253Z","shell.execute_reply":"2024-05-26T10:55:27.206174Z","shell.execute_reply.started":"2024-05-26T10:55:27.202508Z"},"trusted":true},"outputs":[],"source":["# Prepare data for TensorFlow\n","MAX_SEQUENCE_LENGTH = tokenizer.model_max_length\n","BATCH_SIZE = 16\n","EPOCHS = 10\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:27.212495Z","iopub.status.busy":"2024-05-26T10:55:27.212193Z","iopub.status.idle":"2024-05-26T10:55:27.222056Z","shell.execute_reply":"2024-05-26T10:55:27.221116Z","shell.execute_reply.started":"2024-05-26T10:55:27.212459Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"# Define labels\\n\\n# Perform one-hot encoding on 'Topic' column\\none_hot_encoded = pd.get_dummies(train_df['Topic'])\\none_hot_encoded = one_hot_encoded.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\ntrain_df = pd.concat([train_df, one_hot_encoded], axis=1)\\n\\n# Drop the original 'result' column\\ntrain_df.drop(columns=['Topic'], inplace=True)\\n\\n# Perform one-hot encoding on 'result' column\\none_hot_encoded1 = pd.get_dummies(val_df['Topic'])\\none_hot_encoded1 = one_hot_encoded1.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\nval_df = pd.concat([val_df, one_hot_encoded1], axis=1)\\n\\n# Drop the original 'result' column\\nval_df.drop(columns=['Topic'], inplace=True)\\n\\n# Perform one-hot encoding on 'result' column\\none_hot_encoded2 = pd.get_dummies(test_df['Topic'])\\none_hot_encoded2 = one_hot_encoded2.astype(int)\\n# Concatenate the one-hot encoded DataFrame with the original DataFrame\\ntest_df = pd.concat([test_df, one_hot_encoded2], axis=1)\\n\\n# Drop the original 'result' column\\ntest_df.drop(columns=['Topic'], inplace=True)\\n\\n\""]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"# Define labels\n","\n","# Perform one-hot encoding on 'Topic' column\n","one_hot_encoded = pd.get_dummies(train_df['Topic'])\n","one_hot_encoded = one_hot_encoded.astype(int)\n","# Concatenate the one-hot encoded DataFrame with the original DataFrame\n","train_df = pd.concat([train_df, one_hot_encoded], axis=1)\n","\n","# Drop the original 'result' column\n","train_df.drop(columns=['Topic'], inplace=True)\n","\n","# Perform one-hot encoding on 'result' column\n","one_hot_encoded1 = pd.get_dummies(val_df['Topic'])\n","one_hot_encoded1 = one_hot_encoded1.astype(int)\n","# Concatenate the one-hot encoded DataFrame with the original DataFrame\n","val_df = pd.concat([val_df, one_hot_encoded1], axis=1)\n","\n","# Drop the original 'result' column\n","val_df.drop(columns=['Topic'], inplace=True)\n","\n","# Perform one-hot encoding on 'result' column\n","one_hot_encoded2 = pd.get_dummies(test_df['Topic'])\n","one_hot_encoded2 = one_hot_encoded2.astype(int)\n","# Concatenate the one-hot encoded DataFrame with the original DataFrame\n","test_df = pd.concat([test_df, one_hot_encoded2], axis=1)\n","\n","# Drop the original 'result' column\n","test_df.drop(columns=['Topic'], inplace=True)\n","\n","\"\"\""]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:27.223553Z","iopub.status.busy":"2024-05-26T10:55:27.223204Z","iopub.status.idle":"2024-05-26T10:55:27.262333Z","shell.execute_reply":"2024-05-26T10:55:27.261515Z","shell.execute_reply.started":"2024-05-26T10:55:27.223521Z"},"trusted":true},"outputs":[],"source":["# Define the mapping for sentiment\n","sentiment_mapping = {'POSITIVE': 1, 'NEGATIVE': 2, 'NEUTRAL': 3}\n","\n","# Function to perform the customized one-hot encoding\n","def custom_one_hot_encoding(df):\n","    # One-hot encode the 'Topic' column\n","    one_hot_encoded = pd.get_dummies(df['Topic'])\n","    # Map the 'Sentiment' values to the specified values\n","    one_hot_encoded = one_hot_encoded * df['Sentiment'].map(sentiment_mapping).values[:, None]\n","    # Concatenate the one-hot encoded DataFrame with the original DataFrame\n","    df = pd.concat([df, one_hot_encoded], axis=1)\n","    # Drop the original 'Topic' column\n","    df.drop(columns=['Topic'], inplace=True)\n","    return df\n","\n","# Apply the function to each DataFrame\n","train_df = custom_one_hot_encoding(train_df)\n","val_df = custom_one_hot_encoding(val_df)\n","test_df = custom_one_hot_encoding(test_df)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:27.263819Z","iopub.status.busy":"2024-05-26T10:55:27.263495Z","iopub.status.idle":"2024-05-26T10:55:27.271464Z","shell.execute_reply":"2024-05-26T10:55:27.270442Z","shell.execute_reply.started":"2024-05-26T10:55:27.263791Z"},"trusted":true},"outputs":[],"source":["train_df = train_df.drop(['Sentiment'], axis = 1)\n","val_df = val_df.drop(['Sentiment'], axis = 1)\n","test_df = test_df.drop(['Sentiment'], axis = 1)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:27.273030Z","iopub.status.busy":"2024-05-26T10:55:27.272675Z","iopub.status.idle":"2024-05-26T10:55:27.284327Z","shell.execute_reply":"2024-05-26T10:55:27.283228Z","shell.execute_reply.started":"2024-05-26T10:55:27.272993Z"},"trusted":true},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:27.286065Z","iopub.status.busy":"2024-05-26T10:55:27.285652Z","iopub.status.idle":"2024-05-26T10:55:29.887633Z","shell.execute_reply":"2024-05-26T10:55:29.886576Z","shell.execute_reply.started":"2024-05-26T10:55:27.286031Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train outputs: (11426, 4, 4)\n","Validate outputs: (1583, 4, 4)\n","Test outputs: (3166, 4, 4)\n"]},{"data":{"text/plain":["array([[1, 0, 0, 0],\n","       [1, 0, 0, 0],\n","       [1, 0, 0, 0],\n","       [0, 1, 0, 0]], dtype=uint8)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["def make_outputs(df):\n","    outputs = []\n","    for row in range(len(df)):\n","        row_one_hot = []\n","        for col in range(1, len(df.columns)):\n","            sentiment = df.iloc[row, col]\n","            if sentiment == 0: one_hot = [1, 0, 0, 0] #None\n","            elif sentiment == 1: one_hot = [0, 1, 0, 0] # Pos\n","            elif sentiment == 2: one_hot = [0, 0, 1, 0] # Neg\n","            elif sentiment == 3: one_hot = [0, 0, 0, 1] # Neu\n","            row_one_hot.append(one_hot)\n","        outputs.append(row_one_hot)\n","    return np.array(outputs, dtype='uint8')\n","y_train = make_outputs(train_df)\n","y_val = make_outputs(val_df)\n","y_test = make_outputs(test_df)\n","\n","print('Train outputs:', y_train.shape)\n","print('Validate outputs:', y_val.shape)\n","print('Test outputs:', y_test.shape)\n","y_train[0]"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:29.890049Z","iopub.status.busy":"2024-05-26T10:55:29.889265Z","iopub.status.idle":"2024-05-26T10:55:29.982726Z","shell.execute_reply":"2024-05-26T10:55:29.981732Z","shell.execute_reply.started":"2024-05-26T10:55:29.890007Z"},"trusted":true},"outputs":[],"source":["train_df.to_csv('train_tokenize.csv', index = False)\n","val_df.to_csv('val_tokenize.csv', index = False)\n","test_df.to_csv('test_tokenize.csv', index = False)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:29.984445Z","iopub.status.busy":"2024-05-26T10:55:29.984072Z","iopub.status.idle":"2024-05-26T10:55:29.989898Z","shell.execute_reply":"2024-05-26T10:55:29.988931Z","shell.execute_reply.started":"2024-05-26T10:55:29.984409Z"},"trusted":true},"outputs":[],"source":["TRAIN_PATH = \"/kaggle/working/train_tokenize.csv\"\n","VAL_PATH = \"/kaggle/working/val_tokenize.csv\"\n","TEST_PATH = \"/kaggle/working/test_tokenize.csv\""]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:29.992193Z","iopub.status.busy":"2024-05-26T10:55:29.991439Z","iopub.status.idle":"2024-05-26T10:55:30.310416Z","shell.execute_reply":"2024-05-26T10:55:30.309366Z","shell.execute_reply.started":"2024-05-26T10:55:29.992156Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79e340ffd06f41269407ae09dd89cdba","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abee33aae9fc4ea68de4ccb5cd5d9380","version_major":2,"version_minor":0},"text/plain":["Generating val split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b7c006ffa2746e7b4ccb16c250ed460","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n","        num_rows: 11426\n","    })\n","    val: Dataset({\n","        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n","        num_rows: 1583\n","    })\n","    test: Dataset({\n","        features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM'],\n","        num_rows: 3166\n","    })\n","})"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","raw_datasets = load_dataset('csv', data_files={'train': TRAIN_PATH, 'val': VAL_PATH, 'test': TEST_PATH})\n","raw_datasets"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:30.312053Z","iopub.status.busy":"2024-05-26T10:55:30.311731Z","iopub.status.idle":"2024-05-26T10:55:30.323132Z","shell.execute_reply":"2024-05-26T10:55:30.322145Z","shell.execute_reply.started":"2024-05-26T10:55:30.312026Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input_ids of sentence 1484: {'Sentence': 'thầy tận tâm với học sinh  nhiệt tình với học sinh ', 'FACILITY': 0, 'LECTURER': 1, 'OTHERS': 0, 'TRAINING_PROGRAM': 0}\n"]}],"source":["print('input_ids of sentence 1484:', raw_datasets['train'][1484])"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:30.324737Z","iopub.status.busy":"2024-05-26T10:55:30.324350Z","iopub.status.idle":"2024-05-26T10:55:30.332745Z","shell.execute_reply":"2024-05-26T10:55:30.331837Z","shell.execute_reply.started":"2024-05-26T10:55:30.324700Z"},"trusted":true},"outputs":[],"source":["def tokenize_function(dataset):\n","    clean_texts = list(map(text_preprocess, dataset['Sentence']))\n","    return tokenizer(clean_texts, max_length=tokenizer.model_max_length, padding='max_length', truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:30.334704Z","iopub.status.busy":"2024-05-26T10:55:30.334310Z","iopub.status.idle":"2024-05-26T10:55:50.686264Z","shell.execute_reply":"2024-05-26T10:55:50.685226Z","shell.execute_reply.started":"2024-05-26T10:55:30.334669Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58c7c5a75e2044a296deaaa20891f115","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/11426 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00acd5650b9b4ceeb4e4b039b1356220","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1583 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62284d09817c4515b92dfd8d74fa8961","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/3166 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["input_ids of sentence 1484: [0, 1249, 1855, 2652, 15, 222, 418, 2515, 939, 15, 222, 418, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","print('input_ids of sentence 1484:', tokenized_datasets['train'][1484]['input_ids'])"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:50.688489Z","iopub.status.busy":"2024-05-26T10:55:50.687773Z","iopub.status.idle":"2024-05-26T10:55:50.698161Z","shell.execute_reply":"2024-05-26T10:55:50.697266Z","shell.execute_reply.started":"2024-05-26T10:55:50.688442Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Decode: <s> thầy tận tâm với học sinh nhiệt tình với học sinh </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"]}],"source":["print('Decode:', tokenizer.decode(tokenized_datasets['train'][1484]['input_ids']))\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:50.700151Z","iopub.status.busy":"2024-05-26T10:55:50.699840Z","iopub.status.idle":"2024-05-26T10:55:50.733720Z","shell.execute_reply":"2024-05-26T10:55:50.732515Z","shell.execute_reply.started":"2024-05-26T10:55:50.700126Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['Sentence', 'FACILITY', 'LECTURER', 'OTHERS', 'TRAINING_PROGRAM', 'input_ids', 'token_type_ids', 'attention_mask'],\n","    num_rows: 11426\n","})"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets['train']"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:50.735709Z","iopub.status.busy":"2024-05-26T10:55:50.735349Z","iopub.status.idle":"2024-05-26T10:55:51.916942Z","shell.execute_reply":"2024-05-26T10:55:51.916099Z","shell.execute_reply.started":"2024-05-26T10:55:50.735677Z"},"trusted":true},"outputs":[],"source":["train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"], batch_size=BATCH_SIZE, shuffle=True\n",")\n","\n","validation_dataset = tokenized_datasets[\"val\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"], batch_size=BATCH_SIZE, shuffle=True\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:51.918412Z","iopub.status.busy":"2024-05-26T10:55:51.918151Z","iopub.status.idle":"2024-05-26T10:55:51.923329Z","shell.execute_reply":"2024-05-26T10:55:51.922092Z","shell.execute_reply.started":"2024-05-26T10:55:51.918387Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from transformers import TFAutoModel, AutoTokenizer\n","from tensorflow.keras import layers, Model"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:51.925086Z","iopub.status.busy":"2024-05-26T10:55:51.924765Z","iopub.status.idle":"2024-05-26T10:55:51.937262Z","shell.execute_reply":"2024-05-26T10:55:51.936284Z","shell.execute_reply.started":"2024-05-26T10:55:51.925060Z"},"trusted":true},"outputs":[],"source":["class BertLayer(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(BertLayer, self).__init__(**kwargs)\n","        self.bert = TFAutoModel.from_pretrained(PRETRAINED_MODEL)\n","\n","    def call(self, inputs):\n","        input_ids, attention_mask, token_type_ids = inputs\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","        return outputs[0]\n","\n","def create_model(optimizer):\n","    # Build your model on top of BERT\n","    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n","    attention_mask = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n","    token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='token_type_ids')\n","\n","    bert_layer = BertLayer()([input_ids, attention_mask, token_type_ids])\n","    pooled_output = bert_layer[:, 0, :]  # Take only the first token\n","    #dense = layers.Dense(16, activation='softmax')(pooled_output)\n","    dense = concatenate([\n","        Dense(\n","            units = 4, \n","            activation = 'softmax',\n","            name = label,\n","        )(pooled_output) for label in train_df.columns[1:]\n","    ], axis = -1)\n","    # Define your model\n","    model = Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=dense)\n","\n","    # Compile your model\n","    #model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","    model.compile(optimizer=optimizer, loss='binary_crossentropy')\n","    \n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:51.938719Z","iopub.status.busy":"2024-05-26T10:55:51.938396Z","iopub.status.idle":"2024-05-26T10:55:51.961706Z","shell.execute_reply":"2024-05-26T10:55:51.960639Z","shell.execute_reply.started":"2024-05-26T10:55:51.938693Z"},"trusted":true},"outputs":[{"data":{"text/plain":["keras.src.optimizers.adam.Adam"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow.keras.optimizers import Adam\n","optimizer = Adam(learning_rate=1e-5)\n","type(optimizer)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:55:51.963378Z","iopub.status.busy":"2024-05-26T10:55:51.962989Z","iopub.status.idle":"2024-05-26T10:56:23.023072Z","shell.execute_reply":"2024-05-26T10:56:23.022109Z","shell.execute_reply.started":"2024-05-26T10:55:51.963343Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eab0f46a6d7049f088ad99181e729ee9","version_major":2,"version_minor":0},"text/plain":["tf_model.h5:   0%|          | 0.00/740M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ token_type_ids      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ bert_layer          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │                   │            │ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ FACILITY (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ LECTURER (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ OTHERS (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ TRAINING_PROGRAM    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ FACILITY[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ LECTURER[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│                     │                   │            │ OTHERS[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n","│                     │                   │            │ TRAINING_PROGRAM… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ token_type_ids      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ bert_layer          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│ (\u001b[38;5;33mBertLayer\u001b[0m)         │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n","│                     │                   │            │ token_type_ids[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ get_item (\u001b[38;5;33mGetItem\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bert_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ FACILITY (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ LECTURER (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ OTHERS (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ TRAINING_PROGRAM    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ FACILITY[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ LECTURER[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│                     │                   │            │ OTHERS[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n","│                     │                   │            │ TRAINING_PROGRAM… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["model = create_model(optimizer)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:23.029847Z","iopub.status.busy":"2024-05-26T10:56:23.029551Z","iopub.status.idle":"2024-05-26T10:56:23.035361Z","shell.execute_reply":"2024-05-26T10:56:23.034450Z","shell.execute_reply.started":"2024-05-26T10:56:23.029821Z"},"trusted":true},"outputs":[],"source":["# Create TensorFlow Datasets\n","def encode_data(tokenized_datasets, labels):\n","    return (\n","        {\n","            'input_ids': tf.constant(tokenized_datasets['input_ids']),\n","            'attention_mask': tf.constant(tokenized_datasets['attention_mask']),\n","            'token_type_ids': tf.constant(tokenized_datasets['token_type_ids'])\n","        },\n","        tf.constant(labels)\n","    )"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:23.036739Z","iopub.status.busy":"2024-05-26T10:56:23.036479Z","iopub.status.idle":"2024-05-26T10:56:30.462063Z","shell.execute_reply":"2024-05-26T10:56:30.461054Z","shell.execute_reply.started":"2024-05-26T10:56:23.036716Z"},"trusted":true},"outputs":[],"source":["train_data = encode_data(tokenized_datasets['train'], np.reshape(y_train, (-1, 16)))\n","val_data = encode_data(tokenized_datasets['val'], np.reshape(y_val, (-1, 16)))\n"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:30.463788Z","iopub.status.busy":"2024-05-26T10:56:30.463398Z","iopub.status.idle":"2024-05-26T10:56:30.473912Z","shell.execute_reply":"2024-05-26T10:56:30.472850Z","shell.execute_reply.started":"2024-05-26T10:56:30.463757Z"},"trusted":true},"outputs":[{"data":{"text/plain":["({'input_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n","  array([[    0, 48090,  4368, ...,     1,     1,     1],\n","         [    0,  2515,   939, ...,     1,     1,     1],\n","         [    0,    57,   222, ...,     1,     1,     1],\n","         ...,\n","         [    0,   574,   387, ...,     1,     1,     1],\n","         [    0,  4368,  1430, ...,     1,     1,     1],\n","         [    0,  1685,  2953, ...,     1,     1,     1]], dtype=int32)>,\n","  'attention_mask': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n","  array([[1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0],\n","         ...,\n","         [1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n","  'token_type_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n","  array([[0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>},\n"," <tf.Tensor: shape=(11426, 16), dtype=uint8, numpy=\n"," array([[1, 0, 0, ..., 1, 0, 0],\n","        [1, 0, 0, ..., 0, 0, 0],\n","        [1, 0, 0, ..., 0, 1, 0],\n","        ...,\n","        [1, 0, 0, ..., 0, 0, 0],\n","        [1, 0, 0, ..., 0, 0, 0],\n","        [1, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["train_data"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:30.475561Z","iopub.status.busy":"2024-05-26T10:56:30.475219Z","iopub.status.idle":"2024-05-26T10:56:31.279483Z","shell.execute_reply":"2024-05-26T10:56:31.278307Z","shell.execute_reply.started":"2024-05-26T10:56:30.475534Z"},"trusted":true},"outputs":[],"source":["# Create TensorFlow datasets\n","train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(BATCH_SIZE)\n","val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:31.281743Z","iopub.status.busy":"2024-05-26T10:56:31.281112Z","iopub.status.idle":"2024-05-26T10:56:31.291567Z","shell.execute_reply":"2024-05-26T10:56:31.290414Z","shell.execute_reply.started":"2024-05-26T10:56:31.281706Z"},"trusted":true},"outputs":[{"data":{"text/plain":["({'input_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n","  array([[    0, 48090,  4368, ...,     1,     1,     1],\n","         [    0,  2515,   939, ...,     1,     1,     1],\n","         [    0,    57,   222, ...,     1,     1,     1],\n","         ...,\n","         [    0,   574,   387, ...,     1,     1,     1],\n","         [    0,  4368,  1430, ...,     1,     1,     1],\n","         [    0,  1685,  2953, ...,     1,     1,     1]], dtype=int32)>,\n","  'attention_mask': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n","  array([[1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0],\n","         ...,\n","         [1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0],\n","         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n","  'token_type_ids': <tf.Tensor: shape=(11426, 256), dtype=int32, numpy=\n","  array([[0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0],\n","         ...,\n","         [0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0],\n","         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>},\n"," <tf.Tensor: shape=(11426, 16), dtype=uint8, numpy=\n"," array([[1, 0, 0, ..., 1, 0, 0],\n","        [1, 0, 0, ..., 0, 0, 0],\n","        [1, 0, 0, ..., 0, 1, 0],\n","        ...,\n","        [1, 0, 0, ..., 0, 0, 0],\n","        [1, 0, 0, ..., 0, 0, 0],\n","        [1, 0, 0, ..., 0, 0, 0]], dtype=uint8)>)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["train_data"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:31.295439Z","iopub.status.busy":"2024-05-26T10:56:31.293041Z","iopub.status.idle":"2024-05-26T10:56:31.303270Z","shell.execute_reply":"2024-05-26T10:56:31.302165Z","shell.execute_reply.started":"2024-05-26T10:56:31.295411Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'# Huấn luyện mô hình\\ndef train_model(model, train_dataset, val_dataset, epochs=3):\\n\\n    for epoch in range(epochs):\\n        history = model.fit(\\n                        train_dataset,\\n                        epochs=epochs,\\n                        validation_data=val_dataset\\n                    )\\n\\n        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {history.history[\\'loss\\'][0]:.4f}, Train Accuracy: {history.history[\\'accuracy\\'][0]:.4f}\")\\n        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {history.history[\\'val_loss\\'][0]:.4f}, Val Accuracy: {history.history[\\'val_accuracy\\'][0]:.4f}\")\\n    \\n    model.save(\\'sentiment_model.h5\\')'"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"# Huấn luyện mô hình\n","def train_model(model, train_dataset, val_dataset, epochs=3):\n","\n","    for epoch in range(epochs):\n","        history = model.fit(\n","                        train_dataset,\n","                        epochs=epochs,\n","                        validation_data=val_dataset\n","                    )\n","\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {history.history['loss'][0]:.4f}, Train Accuracy: {history.history['accuracy'][0]:.4f}\")\n","        print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {history.history['val_loss'][0]:.4f}, Val Accuracy: {history.history['val_accuracy'][0]:.4f}\")\n","    \n","    model.save('sentiment_model.h5')\"\"\""]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:31.304888Z","iopub.status.busy":"2024-05-26T10:56:31.304532Z","iopub.status.idle":"2024-05-26T10:56:49.582556Z","shell.execute_reply":"2024-05-26T10:56:49.581441Z","shell.execute_reply.started":"2024-05-26T10:56:31.304844Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\n","Collecting wandb\n","  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.16.1)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\n","Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (4.2.0)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\n","Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\n","Requirement already satisfied: tensorboard<2.17,>=2.16 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n","Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.2.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.0)\n","Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.3)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n","Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: wandb\n","  Attempting uninstall: wandb\n","    Found existing installation: wandb 0.16.6\n","    Uninstalling wandb-0.16.6:\n","      Successfully uninstalled wandb-0.16.6\n","Successfully installed wandb-0.17.0\n"]}],"source":["!pip install --upgrade wandb tensorflow\n"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:49.584390Z","iopub.status.busy":"2024-05-26T10:56:49.584074Z","iopub.status.idle":"2024-05-26T10:56:50.018960Z","shell.execute_reply":"2024-05-26T10:56:50.018066Z","shell.execute_reply.started":"2024-05-26T10:56:49.584358Z"},"trusted":true},"outputs":[],"source":["import os\n","import wandb\n","import tensorflow as tf\n","import numpy as np\n","import warnings\n","import logging\n","# Set TensorFlow logging level to ERROR to suppress warnings and informational messages\n","tf.get_logger().setLevel('ERROR')\n","logging.getLogger('tensorflow').setLevel(logging.ERROR)\n","\n","# Additional method to suppress specific warnings\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings and errors: 0 = all logs, 1 = filter out INFO, 2 = filter out WARNING, 3 = filter out ERROR\n","\n"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T10:56:50.020699Z","iopub.status.busy":"2024-05-26T10:56:50.020297Z","iopub.status.idle":"2024-05-26T10:56:53.938510Z","shell.execute_reply":"2024-05-26T10:56:53.937557Z","shell.execute_reply.started":"2024-05-26T10:56:50.020656Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlavibuu\u001b[0m (\u001b[33mlavibu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.0"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240526_105652-9n6h5k0i</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Resuming run <strong><a href='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i' target=\"_blank\">azure-valley-25</a></strong> to <a href='https://wandb.ai/lavibu/absa-vietnamese' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/lavibu/absa-vietnamese' target=\"_blank\">https://wandb.ai/lavibu/absa-vietnamese</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i' target=\"_blank\">https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lavibu/absa-vietnamese/runs/9n6h5k0i?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fdc70164280>"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","import os\n","import tensorflow as tf\n","\n","# Define the checkpoint path\n","checkpoint_path = \"sentiment_model_checkpoint.weights.h5\"\n","# Set your WandB API key\n","wandb.login(key=\"b9575849263a9312a73f76d71d270c8751628e10\")\n","\n","# Initialize WandB with a unique run ID based on current timestamp\n","run_id = \"9n6h5k0i\"\n","wandb.init(project=\"absa-vietnamese\", id=run_id, resume=\"allow\")"]},{"cell_type":"code","execution_count":76,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:00:36.404735Z","iopub.status.busy":"2024-05-26T13:00:36.404367Z","iopub.status.idle":"2024-05-26T13:29:11.280915Z","shell.execute_reply":"2024-05-26T13:29:11.279732Z","shell.execute_reply.started":"2024-05-26T13:00:36.404706Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Checkpoint loaded.\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1145\n","Epoch 1: val_loss improved from inf to 0.12247, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1145 - val_loss: 0.1225\n","Epoch 1/10\n","train_loss: 0.1155 - val_loss: 0.1225\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1144\n","Epoch 1: val_loss improved from 0.12247 to 0.12241, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1144 - val_loss: 0.1224\n","Epoch 2/10\n","train_loss: 0.1154 - val_loss: 0.1224\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1144\n","Epoch 1: val_loss improved from 0.12241 to 0.12236, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1144 - val_loss: 0.1224\n","Epoch 3/10\n","train_loss: 0.1153 - val_loss: 0.1224\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1143\n","Epoch 1: val_loss improved from 0.12236 to 0.12230, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1143 - val_loss: 0.1223\n","Epoch 4/10\n","train_loss: 0.1152 - val_loss: 0.1223\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1142\n","Epoch 1: val_loss improved from 0.12230 to 0.12225, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1142 - val_loss: 0.1222\n","Epoch 5/10\n","train_loss: 0.1151 - val_loss: 0.1222\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1141\n","Epoch 1: val_loss improved from 0.12225 to 0.12220, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1141 - val_loss: 0.1222\n","Epoch 6/10\n","train_loss: 0.1150 - val_loss: 0.1222\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1140\n","Epoch 1: val_loss improved from 0.12220 to 0.12214, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1140 - val_loss: 0.1221\n","Epoch 7/10\n","train_loss: 0.1149 - val_loss: 0.1221\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1139\n","Epoch 1: val_loss improved from 0.12214 to 0.12209, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1139 - val_loss: 0.1221\n","Epoch 8/10\n","train_loss: 0.1149 - val_loss: 0.1221\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1138\n","Epoch 1: val_loss improved from 0.12209 to 0.12204, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 240ms/step - loss: 0.1138 - val_loss: 0.1220\n","Epoch 9/10\n","train_loss: 0.1148 - val_loss: 0.1220\n","\u001b[1m714/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - loss: 0.1137\n","Epoch 1: val_loss improved from 0.12204 to 0.12199, saving model to sentiment_model_checkpoint.weights.h5\n","\u001b[1m715/715\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 240ms/step - loss: 0.1137 - val_loss: 0.1220\n","Epoch 10/10\n","train_loss: 0.1147 - val_loss: 0.1220\n"]}],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Định nghĩa early_stop_callback\n","early_stop_callback = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',  \n","    patience=5,  \n","    restore_best_weights=True  \n",")\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_weights_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","\n","# Thiết lập mức độ log của TensorFlow để chỉ hiển thị các lỗi nghiêm trọng\n","tf.get_logger().setLevel('ERROR')\n","\n","# Check if checkpoint exists and load weights\n","if os.path.exists(checkpoint_path):\n","    model.load_weights(checkpoint_path)\n","    print(\"Checkpoint loaded.\")\n","\n","# Define the training function\n","def train_model(model, train_dataset, val_dataset, epochs=3):\n","    for epoch in range(epochs):\n","        history = model.fit(\n","            train_dataset,\n","            epochs=1,  # Train for 1 epoch at a time\n","            validation_data=val_dataset,\n","            callbacks=[\n","                early_stop_callback,\n","                model_checkpoint_callback,\n","            ],\n","            verbose=1\n","        )\n","        # Calculate average train and validation loss\n","        avg_train_loss = np.mean(history.history['loss'])\n","        avg_val_loss = np.mean(history.history['val_loss'])\n","\n","        print(f\"Epoch {epoch+1}/{epochs}\")\n","        print(f\"train_loss: {avg_train_loss:.4f} - val_loss: {avg_val_loss:.4f}\")\n","\n","        # Log metrics to WandB\n","        wandb.log({\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n","# Train the model\n","train_model(model, train_dataset, val_dataset, epochs=10)\n"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T11:26:07.690876Z","iopub.status.busy":"2024-05-26T11:26:07.690481Z","iopub.status.idle":"2024-05-26T11:26:09.454992Z","shell.execute_reply":"2024-05-26T11:26:09.453779Z","shell.execute_reply.started":"2024-05-26T11:26:07.690835Z"},"trusted":true},"outputs":[],"source":["test_data = encode_data(tokenized_datasets['test'], np.reshape(y_test, (-1, 16)))\n","test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE)"]},{"cell_type":"code","execution_count":77,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:42:50.895086Z","iopub.status.busy":"2024-05-26T13:42:50.893744Z","iopub.status.idle":"2024-05-26T13:44:12.812338Z","shell.execute_reply":"2024-05-26T13:44:12.811229Z","shell.execute_reply.started":"2024-05-26T13:42:50.895044Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 213ms/step - loss: 0.1278\n","Test Loss: 0.1264\n"]}],"source":["# Evaluate the model on the test dataset\n","test_loss= model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Reload the model"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:12.814624Z","iopub.status.busy":"2024-05-26T13:44:12.814295Z","iopub.status.idle":"2024-05-26T13:44:25.637384Z","shell.execute_reply":"2024-05-26T13:44:25.636113Z","shell.execute_reply.started":"2024-05-26T13:44:12.814593Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n","- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"functional_5\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ token_type_ids      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ bert_layer_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│                     │                   │            │ token_type_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ get_item_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ FACILITY (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ LECTURER (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ OTHERS (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ TRAINING_PROGRAM    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,076</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ FACILITY[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ LECTURER[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n","│                     │                   │            │ OTHERS[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n","│                     │                   │            │ TRAINING_PROGRAM… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ token_type_ids      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ bert_layer_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│ (\u001b[38;5;33mBertLayer\u001b[0m)         │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n","│                     │                   │            │ token_type_ids[\u001b[38;5;34m0\u001b[0m… │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ get_item_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bert_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n","│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ FACILITY (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ LECTURER (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ OTHERS (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ TRAINING_PROGRAM    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m3,076\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n","│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ FACILITY[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ LECTURER[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n","│                     │                   │            │ OTHERS[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n","│                     │                   │            │ TRAINING_PROGRAM… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,914</span> (144.20 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,914\u001b[0m (144.20 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,304</span> (48.06 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,304\u001b[0m (48.06 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,610</span> (96.14 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m24,610\u001b[0m (96.14 KB)\n"]},"metadata":{},"output_type":"display_data"}],"source":["reloaded_model = create_model(optimizer)\n","reloaded_model.load_weights('/kaggle/working/sentiment_model_checkpoint.weights.h5')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:25.639151Z","iopub.status.busy":"2024-05-26T13:44:25.638790Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.utils import plot_model\n","plot_model(reloaded_model, to_file='architecture.png', rankdir='LR', dpi=52, show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","# Tiến hành dự đoán\n","def predict_sentiment(text, loaded_model):\n","    inputs = tokenizer(text, max_length=MAX_SEQUENCE_LENGTH, padding='max_length', truncation=True, return_tensors='tf')\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","    token_type_ids = inputs['token_type_ids']\n","    predictions = loaded_model.predict([input_ids, attention_mask, token_type_ids])\n","    return predictions\n","\n","def print_acsa_pred(predictions):\n","    # Giải thích kết quả\n","    topics = [\"FACILITY\", \"LECTURER\", \"OTHERS\", \"TRAINING_PROGRAMS\"]\n","    sentiments = [\"None\", \"Positive\", \"Negative\", \"Neutral\"] \n","    y_pred = predictions.reshape(len(predictions), -1, 4)\n","    for i in range(4):\n","        sentiment_index = np.argmax(y_pred[0][i])\n","        topic = topics[i]\n","        sentiment = sentiments[sentiment_index]\n","        if sentiment != \"None\":\n","            print(f\"{topic}: {sentiment}\")"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:25.748662Z","iopub.status.idle":"2024-05-26T13:44:25.756617Z","shell.execute_reply":"2024-05-26T13:44:25.755563Z","shell.execute_reply.started":"2024-05-26T13:44:25.748627Z"},"trusted":true},"outputs":[],"source":["def report(predictions):\n","    sentiment_indices = []\n","\n","    # Reshape predictions to match the expected format\n","    y_pred = predictions.reshape(len(predictions), -1, 4)\n","\n","    # Iterate over each category\n","    for i in range(4):\n","        # Get the sentiment index for the current category\n","        sentiment_index = np.argmax(y_pred[0][i])\n","\n","        # Append the sentiment index to the list\n","        sentiment_indices.append(sentiment_index)\n","\n","    # Return the list of sentiment indices for each category\n","    return sentiment_indices\n"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:25.758768Z","iopub.status.busy":"2024-05-26T13:44:25.757913Z","iopub.status.idle":"2024-05-26T13:44:29.264203Z","shell.execute_reply":"2024-05-26T13:44:29.262939Z","shell.execute_reply.started":"2024-05-26T13:44:25.758740Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["W0000 00:00:1716731067.690088     158 assert_op.cc:38] Ignoring Assert operator functional_5_1/bert_layer_2_1/tf_roberta_model_2/roberta/embeddings/assert_less/Assert/Assert\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n","[[9.9983847e-01 2.8307688e-05 1.2431138e-04 8.7628086e-06 4.6013118e-04\n","  9.7507465e-01 2.4146359e-02 3.1887632e-04 9.9894303e-01 6.7140500e-04\n","  2.9901156e-04 8.6607928e-05 9.9920678e-01 3.2926607e-04 3.7895472e-04\n","  8.5075684e-05]]\n","[[9.9983847e-01 2.8307688e-05 1.2431138e-04 8.7628086e-06]\n"," [4.6013118e-04 9.7507465e-01 2.4146359e-02 3.1887632e-04]\n"," [9.9894303e-01 6.7140500e-04 2.9901156e-04 8.6607928e-05]\n"," [9.9920678e-01 3.2926607e-04 3.7895472e-04 8.5075684e-05]]\n","LECTURER: Positive\n","[0, 1, 0, 0]\n"]}],"source":["# Ví dụ sử dụng\n","text = \"giảng viên rất nhiệt tình và dễ hiểu\"\n","predictions = predict_sentiment(text, reloaded_model)\n","print(predictions)\n","print(predictions.reshape(y_test[1].shape))\n","print_acsa_pred(predictions)\n","print(report(predictions))"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:29.267540Z","iopub.status.busy":"2024-05-26T13:44:29.265495Z","iopub.status.idle":"2024-05-26T13:44:29.382356Z","shell.execute_reply":"2024-05-26T13:44:29.381041Z","shell.execute_reply.started":"2024-05-26T13:44:29.267495Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n","[[0.99337995 0.00157052 0.00389787 0.00115165 0.05584941 0.21189219\n","  0.7064535  0.02580487 0.9671745  0.0112598  0.01326553 0.00830019\n","  0.96330786 0.00729772 0.02645061 0.00294381]]\n","[[0.99337995 0.00157052 0.00389787 0.00115165]\n"," [0.05584941 0.21189219 0.7064535  0.02580487]\n"," [0.9671745  0.0112598  0.01326553 0.00830019]\n"," [0.96330786 0.00729772 0.02645061 0.00294381]]\n","LECTURER: Negative\n","[0, 2, 0, 0]\n"]}],"source":["# Ví dụ sử dụng\n","text = \"dạy chán, không quan tâm học sinh\"\n","predictions = predict_sentiment(text, reloaded_model)\n","print(predictions)\n","print(predictions.reshape(y_test[1].shape))\n","print_acsa_pred(predictions)\n","print(report(predictions))"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:29.383993Z","iopub.status.busy":"2024-05-26T13:44:29.383627Z","iopub.status.idle":"2024-05-26T13:44:29.488725Z","shell.execute_reply":"2024-05-26T13:44:29.487614Z","shell.execute_reply.started":"2024-05-26T13:44:29.383959Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n"]}],"source":["# Ví dụ sử dụng\n","text = \"chương trình học khô khan, khó hiểu\"\n","predictions = predict_sentiment(text, reloaded_model)\n","print_acsa_pred(predictions)\n"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:29.493225Z","iopub.status.busy":"2024-05-26T13:44:29.491052Z","iopub.status.idle":"2024-05-26T13:44:29.604059Z","shell.execute_reply":"2024-05-26T13:44:29.602730Z","shell.execute_reply.started":"2024-05-26T13:44:29.493194Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n","[[0.09857271 0.01481084 0.88182676 0.00478971 0.6561089  0.13309951\n","  0.19698283 0.01380874 0.94935995 0.01791392 0.02512837 0.00759772\n","  0.91343355 0.04684234 0.03586247 0.00386167]]\n","[[0.09857271 0.01481084 0.88182676 0.00478971]\n"," [0.6561089  0.13309951 0.19698283 0.01380874]\n"," [0.94935995 0.01791392 0.02512837 0.00759772]\n"," [0.91343355 0.04684234 0.03586247 0.00386167]]\n","FACILITY: Negative\n","[2, 0, 0, 0]\n"]}],"source":["# Ví dụ sử dụng\n","text = \"phòng học dơ bẩn, ẩm mốc, và cực kỳ tối\"\n","predictions = predict_sentiment(text, reloaded_model)\n","print(predictions)\n","print(predictions.reshape(y_test[1].shape))\n","print_acsa_pred(predictions)\n","print(report(predictions))"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:44:29.609473Z","iopub.status.busy":"2024-05-26T13:44:29.609058Z","iopub.status.idle":"2024-05-26T13:44:29.722344Z","shell.execute_reply":"2024-05-26T13:44:29.721415Z","shell.execute_reply.started":"2024-05-26T13:44:29.609421Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","[[0.9393128  0.00159395 0.05603147 0.00306187 0.49649248 0.1425621\n","  0.2945748  0.06637062 0.75500363 0.06792098 0.07609788 0.10097753\n","  0.8580513  0.07133412 0.03397258 0.03664197]]\n","[[0.9393128  0.00159395 0.05603147 0.00306187]\n"," [0.49649248 0.1425621  0.2945748  0.06637062]\n"," [0.75500363 0.06792098 0.07609788 0.10097753]\n"," [0.8580513  0.07133412 0.03397258 0.03664197]]\n","[0, 0, 0, 0]\n"]}],"source":["# Ví dụ sử dụng\n","text = \"sợ ma\"\n","predictions = predict_sentiment(text, reloaded_model)\n","print(predictions)\n","print(predictions.reshape(y_test[1].shape))\n","print_acsa_pred(predictions)\n","print(report(predictions))"]},{"cell_type":"markdown","metadata":{},"source":["# Prediction"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T13:45:23.951668Z","iopub.status.busy":"2024-05-26T13:45:23.951260Z","iopub.status.idle":"2024-05-26T13:45:23.962395Z","shell.execute_reply":"2024-05-26T13:45:23.961034Z","shell.execute_reply.started":"2024-05-26T13:45:23.951637Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[0, 1, 0, 0],\n","       [0, 1, 0, 0],\n","       [0, 1, 0, 0],\n","       ...,\n","       [0, 1, 0, 0],\n","       [0, 2, 0, 0],\n","       [0, 0, 0, 3]])"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["y_test_argmax = np.argmax(y_test, axis=-1)\n","y_test_argmax"]},{"cell_type":"markdown","metadata":{},"source":["## Predict on test data"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:00:02.379732Z","iopub.status.busy":"2024-05-26T14:00:02.379323Z","iopub.status.idle":"2024-05-26T14:00:02.387422Z","shell.execute_reply":"2024-05-26T14:00:02.386193Z","shell.execute_reply.started":"2024-05-26T14:00:02.379697Z"},"trusted":true},"outputs":[],"source":["def predict(model, inputs, batch_size=1, verbose=0):\n","    y_pred = model.predict(inputs, batch_size=batch_size, verbose=verbose)\n","    y_pred = y_pred.reshape(len(y_pred), -1, 4)\n","    return np.argmax(y_pred, axis=-1) # sentiment values (position that have max value)"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:00:05.648171Z","iopub.status.busy":"2024-05-26T14:00:05.647478Z","iopub.status.idle":"2024-05-26T14:00:05.654949Z","shell.execute_reply":"2024-05-26T14:00:05.653694Z","shell.execute_reply.started":"2024-05-26T14:00:05.648135Z"},"trusted":true},"outputs":[],"source":["def print_acsa_pred(replacements, categories, sentence_pred):\n","    sentiments = map(lambda x: replacements[x], sentence_pred)\n","    for category, sentiment in zip(categories, sentiments): \n","        if sentiment: print(f'=> {category},{sentiment}')"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:01:54.844735Z","iopub.status.busy":"2024-05-26T15:01:54.844285Z","iopub.status.idle":"2024-05-26T15:03:18.271021Z","shell.execute_reply":"2024-05-26T15:03:18.269964Z","shell.execute_reply.started":"2024-05-26T15:01:54.844701Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 212ms/step\n","\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 210ms/step - loss: 0.1278\n"]},{"data":{"text/plain":["0.12644782662391663"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":["y_pred = predict(reloaded_model, test_dataset, BATCH_SIZE, verbose=1)\n","reloaded_model.evaluate(test_dataset, batch_size=BATCH_SIZE, verbose=1)"]},{"cell_type":"code","execution_count":123,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:03:18.273345Z","iopub.status.busy":"2024-05-26T15:03:18.273033Z","iopub.status.idle":"2024-05-26T15:03:18.281965Z","shell.execute_reply":"2024-05-26T15:03:18.280445Z","shell.execute_reply.started":"2024-05-26T15:03:18.273318Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[1, 0, 0, 0],\n","       [0, 1, 0, 0],\n","       [1, 0, 0, 0],\n","       [1, 0, 0, 0]], dtype=uint8)"]},"execution_count":123,"metadata":{},"output_type":"execute_result"}],"source":["y_test[1]"]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:04:22.707506Z","iopub.status.busy":"2024-05-26T15:04:22.706657Z","iopub.status.idle":"2024-05-26T15:04:22.715583Z","shell.execute_reply":"2024-05-26T15:04:22.714320Z","shell.execute_reply.started":"2024-05-26T15:04:22.707464Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 1, 0, 0])"]},"execution_count":125,"metadata":{},"output_type":"execute_result"}],"source":["y_test_argmax[1]"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:03:18.283968Z","iopub.status.busy":"2024-05-26T15:03:18.283509Z","iopub.status.idle":"2024-05-26T15:03:18.294356Z","shell.execute_reply":"2024-05-26T15:03:18.293288Z","shell.execute_reply.started":"2024-05-26T15:03:18.283924Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([0, 0, 0, 0])"]},"execution_count":124,"metadata":{},"output_type":"execute_result"}],"source":["y_pred[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:03:40.065592Z","iopub.status.busy":"2024-05-26T14:03:40.065202Z","iopub.status.idle":"2024-05-26T14:03:40.098122Z","shell.execute_reply":"2024-05-26T14:03:40.096927Z","shell.execute_reply.started":"2024-05-26T14:03:40.065561Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Example: nói tiếng anh lưu loát \n","Example: giáo viên rất vui tính \n","=> LECTURER,positive\n","Example: cô max có tâm \n","=> LECTURER,positive\n","Example: giảng bài thu hút  dí dỏm \n","=> LECTURER,negative\n","Example: giáo viên không giảng dạy kiến thức  hướng dẫn thực hành trong quá trình học \n","=> LECTURER,negative\n","Example: thầy dạy nhiệt tình và tâm huyết \n","=> LECTURER,positive\n","Example: tính điểm thi đua các nhóm \n","=> TRAINING_PROGRAM,negative\n","Example: thầy nhiệt tình giảng lại cho học sinh \n","=> LECTURER,positive\n","Example: có đôi lúc nói hơi nhanh làm sinh viên không theo kịp \n","=> LECTURER,negative\n","Example: giảng dạy nhiệt tình  liên hệ thực tế khá nhiều  tương tác với sinh viên tương đối tốt \n","=> LECTURER,positive\n","Example: giảng viên nhiệt tình trong công tác giảng dạy \n","=> LECTURER,positive\n","Example: cô rất nhiệt tình  dễ thương  dạy dễ hiểu \n","=> LECTURER,positive\n","Example: trong trường macbook thầy số hai thì không có máy nào số một \n","=> LECTURER,negative\n","Example: sinh viên không tiếp thu kịp cũng như không hiểu gì \n","=> LECTURER,negative\n","Example: thầy nhiệt tình giúp đỡ sinh viên trong quá trình thực hành \n","=> LECTURER,positive\n","Example: thầy rất tận tình và giúp đỡ sinh viên rất nhiều \n","=> LECTURER,positive\n","Example: giảng viên giải thích kỹ và chi tiết \n","=> LECTURER,positive\n","Example: thầy dạy rất chi tiết  lý thuyết đầy đủ \n","=> LECTURER,positive\n","Example: còn những phần tìm bao đóng  chứng minh dạng chuẩn chưa làm rõ \n","=> TRAINING_PROGRAM,negative\n","Example: cung cấp kiến thức bổ ích \n","=> LECTURER,positive\n","Example: bắt đầu buổi học đúng giờ \n","Example: giảng dạy dễ hiểu  tận tâm \n","=> LECTURER,positive\n","Example: giảng viên nhiệt tình  giải đáp thắc mắc đầy đủ \n","=> LECTURER,positive\n","Example: nên đưa ra một vài phương pháp học lập trình hay cho sinh viên \n","=> TRAINING_PROGRAM,negative\n","Example: giảng viên dạy sôi nổi \n","=> LECTURER,positive\n","Example: em học bên chất lượng cao mà phòng máy không cung cấp đủ máy  vì máy hư hoặc không cài chương trình  \n","=> FACILITY,negative\n","Example: chưa giỏi chuyên môn cho lắm \n","Example: cách mà cô tiếp cận với sinh viên \n","=> LECTURER,positive\n","Example: trợ giàng nhiệt tình  quan tâm và theo sát sinh viên \n","=> LECTURER,positive\n","Example: giảng viên tận tâm  gần gũi với học sinh \n","=> LECTURER,positive\n","Example: thầy dạy rất nhiệt tình và giảng kỹ bài không cần cải thiện gì cả \n","=> LECTURER,positive\n","Example: phòng học thoáng mát  trang thiết bị đầy đủ \n","=> FACILITY,negative\n","Example: thầy rất nhiệt tình thân thiện và vui tính cách giảng dạy rất dễ hiểu \n","=> LECTURER,positive\n","Example: không nhiệt tình chỉ dẫn và luôn gây khó khăn cho sinh viên \n","=> LECTURER,negative\n","Example: giữa lý thuyết từ vựng với trò chơi để dễ tiếp thu \n","Example: môn học này giúp chúng em hiểu ra những vấn đề cơ bản \n","=> TRAINING_PROGRAM,negative\n","Example: giáo trình chưa có hợp lý \n","Example: mong thầy xem xét lại việc nộp bài \n","=> LECTURER,negative\n","Example: cô wzjwz234  dạy rất nhiệt tính  tận tâm tận lực  giải thích từng câu code giúp học sinh hiểu rõ vấn đề  rất thích cô dạy môn hướng đối tượng này \n","=> LECTURER,positive\n","Example: cung cấp bài tập đa dạng \n","Example: hay gõ micro vào bảng hoặc bàn \n","Example: ổn \n","Example: tốc độ dạy của giảng viên nhanh  nên có một số nội dung không cung cấp đầy đủ kiến thức \n","=> LECTURER,negative\n","Example: cô vui tính  học không áp lực \n","=> LECTURER,positive\n","Example: cô dạy rất tốt và nhiệt tình \n","=> LECTURER,positive\n","Example: phần lớn chỉ là lý thuyết và bài tập \n","=> TRAINING_PROGRAM,negative\n","Example: thầy nhiệt tình trả lời thắc mắc của sinh viên \n","=> LECTURER,positive\n","Example: thiết bị phòng học như quạt máy cần phải đầu tư thêm \n","=> FACILITY,negative\n","Example: cô nhiệt tình  giảng bài hiệu quả \n","=> LECTURER,positive\n","Example: khi nghỉ học  cần thông báo kịp thời trên web của khoa wzjwz148 \n"]}],"source":["replacements = {0: None, 1: 'positive', 2: 'negative', 3: 'neutral'}\n","categories = test_df.columns[1:]\n","for i in range(50):\n","    print('Example:', test_df['Sentence'][i])\n","    print_acsa_pred(replacements, categories, y_pred[i])"]},{"cell_type":"markdown","metadata":{},"source":["# Report"]},{"cell_type":"code","execution_count":108,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:04:44.901375Z","iopub.status.busy":"2024-05-26T14:04:44.900404Z","iopub.status.idle":"2024-05-26T14:04:44.963981Z","shell.execute_reply":"2024-05-26T14:04:44.962858Z","shell.execute_reply.started":"2024-05-26T14:04:44.901338Z"},"trusted":true},"outputs":[],"source":["aspect_test = []\n","aspect_pred = []\n","\n","for row_test, row_pred in zip(y_test_argmax, y_pred):\n","    for index, (col_test, col_pred) in enumerate(zip(row_test, row_pred)):\n","        aspect_test.append(bool(col_test) * categories[index])\n","        aspect_pred.append(bool(col_pred) * categories[index])"]},{"cell_type":"code","execution_count":109,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:04:58.504649Z","iopub.status.busy":"2024-05-26T14:04:58.503417Z","iopub.status.idle":"2024-05-26T14:04:58.995939Z","shell.execute_reply":"2024-05-26T14:04:58.994869Z","shell.execute_reply.started":"2024-05-26T14:04:58.504603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                  precision    recall  f1-score   support\n","\n","                     0.9275    0.9707    0.9486      9498\n","        FACILITY     0.9619    0.6966    0.8080       145\n","        LECTURER     0.9260    0.8856    0.9054      2290\n","          OTHERS     0.6800    0.1069    0.1848       159\n","TRAINING_PROGRAM     0.7419    0.5227    0.6133       572\n","\n","        accuracy                         0.9211     12664\n","       macro avg     0.8475    0.6365    0.6920     12664\n","    weighted avg     0.9161    0.9211    0.9144     12664\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","aspect_report = classification_report(aspect_test, aspect_pred, digits=4, zero_division=1, output_dict=True)\n","print(classification_report(aspect_test, aspect_pred, digits=4, zero_division=1))"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:05:30.522713Z","iopub.status.busy":"2024-05-26T14:05:30.522266Z","iopub.status.idle":"2024-05-26T14:05:30.530159Z","shell.execute_reply":"2024-05-26T14:05:30.528741Z","shell.execute_reply.started":"2024-05-26T14:05:30.522679Z"},"trusted":true},"outputs":[],"source":["y_test_flat = y_test_argmax.flatten()\n","y_pred_flat = y_pred.flatten()\n","target_names = list(map(str, replacements.values()))"]},{"cell_type":"code","execution_count":111,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:05:32.785082Z","iopub.status.busy":"2024-05-26T14:05:32.784389Z","iopub.status.idle":"2024-05-26T14:05:32.859834Z","shell.execute_reply":"2024-05-26T14:05:32.858606Z","shell.execute_reply.started":"2024-05-26T14:05:32.785045Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        None     0.9275    0.9707    0.9486      9498\n","    positive     0.8517    0.8088    0.8297      1590\n","    negative     0.7481    0.6366    0.6879      1409\n","     neutral     0.5714    0.0479    0.0884       167\n","\n","    accuracy                         0.9011     12664\n","   macro avg     0.7747    0.6160    0.6386     12664\n","weighted avg     0.8933    0.9011    0.8933     12664\n","\n"]}],"source":["polarity_report = classification_report(y_test_flat, y_pred_flat, digits=4, output_dict=True)\n","print(classification_report(y_test_flat, y_pred_flat, target_names=target_names, digits=4))"]},{"cell_type":"code","execution_count":112,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:06:01.963962Z","iopub.status.busy":"2024-05-26T14:06:01.963074Z","iopub.status.idle":"2024-05-26T14:06:02.040019Z","shell.execute_reply":"2024-05-26T14:06:02.038689Z","shell.execute_reply.started":"2024-05-26T14:06:01.963921Z"},"trusted":true},"outputs":[],"source":["aspect_polarity_test = []\n","aspect_polarity_pred = []\n","\n","for row_test, row_pred in zip(y_test_argmax, y_pred):\n","    for index, (col_test, col_pred) in enumerate(zip(row_test, row_pred)):\n","        aspect_polarity_test.append(f'{categories[index]},{replacements[col_test]}')\n","        aspect_polarity_pred.append(f'{categories[index]},{replacements[col_pred]}')"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:06:04.446055Z","iopub.status.busy":"2024-05-26T14:06:04.445490Z","iopub.status.idle":"2024-05-26T14:06:04.990759Z","shell.execute_reply":"2024-05-26T14:06:04.989592Z","shell.execute_reply.started":"2024-05-26T14:06:04.446009Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["                           precision    recall  f1-score   support\n","\n","            FACILITY,None     0.9856    0.9987    0.9921      3021\n","        FACILITY,negative     0.9429    0.7174    0.8148       138\n","         FACILITY,neutral     1.0000    0.0000    0.0000         2\n","        FACILITY,positive     1.0000    0.0000    0.0000         5\n","            LECTURER,None     0.7316    0.8151    0.7711       876\n","        LECTURER,negative     0.7636    0.6941    0.7272       791\n","         LECTURER,neutral     1.0000    0.0135    0.0267        74\n","        LECTURER,positive     0.8619    0.8891    0.8753      1425\n","              OTHERS,None     0.9548    0.9973    0.9756      3007\n","          OTHERS,negative     0.4000    0.0312    0.0580        64\n","           OTHERS,neutral     0.5385    0.1591    0.2456        44\n","          OTHERS,positive     0.1429    0.0196    0.0345        51\n","    TRAINING_PROGRAM,None     0.9012    0.9599    0.9296      2594\n","TRAINING_PROGRAM,negative     0.6676    0.5938    0.6285       416\n"," TRAINING_PROGRAM,neutral     1.0000    0.0000    0.0000        47\n","TRAINING_PROGRAM,positive     0.5455    0.1651    0.2535       109\n","\n","                 accuracy                         0.9011     12664\n","                macro avg     0.7772    0.4409    0.4583     12664\n","             weighted avg     0.8932    0.9011    0.8891     12664\n","\n"]}],"source":["aspect_polarity_report = classification_report(aspect_polarity_test, aspect_polarity_pred, digits=4, zero_division=1, output_dict=True)\n","print(classification_report(aspect_polarity_test, aspect_polarity_pred, digits=4, zero_division=1))"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:06:20.219107Z","iopub.status.busy":"2024-05-26T14:06:20.218230Z","iopub.status.idle":"2024-05-26T14:06:20.227089Z","shell.execute_reply":"2024-05-26T14:06:20.225873Z","shell.execute_reply.started":"2024-05-26T14:06:20.219059Z"},"trusted":true},"outputs":[],"source":["aspect_dict = aspect_report['macro avg']\n","aspect_dict['accuracy'] = aspect_report['accuracy']\n","\n","polarity_dict  = polarity_report['macro avg']\n","polarity_dict['accuracy'] = polarity_report['accuracy']\n","\n","aspect_polarity_dict = aspect_polarity_report['macro avg']\n","aspect_polarity_dict['accuracy'] = aspect_polarity_report['accuracy']"]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:06:30.427238Z","iopub.status.busy":"2024-05-26T14:06:30.426845Z","iopub.status.idle":"2024-05-26T14:06:30.453822Z","shell.execute_reply":"2024-05-26T14:06:30.452868Z","shell.execute_reply.started":"2024-05-26T14:06:30.427206Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1-score</th>\n","      <th>accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Aspect Detection</th>\n","      <td>0.847468</td>\n","      <td>0.636503</td>\n","      <td>0.692016</td>\n","      <td>0.921115</td>\n","    </tr>\n","    <tr>\n","      <th>Polarity Detection</th>\n","      <td>0.774670</td>\n","      <td>0.616015</td>\n","      <td>0.638642</td>\n","      <td>0.901058</td>\n","    </tr>\n","    <tr>\n","      <th>Aspect + Polarity</th>\n","      <td>0.777240</td>\n","      <td>0.440870</td>\n","      <td>0.458276</td>\n","      <td>0.901058</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    precision    recall  f1-score  accuracy\n","Aspect Detection     0.847468  0.636503  0.692016  0.921115\n","Polarity Detection   0.774670  0.616015  0.638642  0.901058\n","Aspect + Polarity    0.777240  0.440870  0.458276  0.901058"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["df_report = pd.DataFrame.from_dict([aspect_dict, polarity_dict, aspect_polarity_dict])\n","df_report.index = ['Aspect Detection', 'Polarity Detection', 'Aspect + Polarity']\n","df_report.drop('support', axis=1)"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:57:46.020827Z","iopub.status.busy":"2024-05-26T14:57:46.020371Z","iopub.status.idle":"2024-05-26T14:57:52.266145Z","shell.execute_reply":"2024-05-26T14:57:52.265043Z","shell.execute_reply.started":"2024-05-26T14:57:46.020792Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your sentence:  sợ ma quá\n"]}],"source":["example_input = text_preprocess(input('Enter your sentence: '))\n","tokenized_input = tokenizer(example_input, padding='max_length', truncation=True)\n","features = {x: [[tokenized_input[x]]] for x in tokenizer.model_input_names}\n","\n","pred = predict(reloaded_model, tf.data.Dataset.from_tensor_slices(features))\n","print_acsa_pred(replacements, categories, pred[0])"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5056692,"sourceId":8478679,"sourceType":"datasetVersion"},{"datasetId":5071077,"sourceId":8498142,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
